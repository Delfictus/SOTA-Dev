//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-37061995
// Cuda compilation tools, release 13.1, V13.1.115
// Based on NVVM 21.0.0
//

.version 9.1
.target sm_120
.address_size 64

	// .globl	mega_fused_vasil_accuracy
.const .align 4 .b8 c_tmax[20] = {0, 0, 96, 65, 0, 0, 140, 65, 0, 0, 168, 65, 0, 0, 196, 65, 0, 0, 224, 65};
.const .align 4 .b8 c_thalf[60] = {0, 0, 200, 65, 184, 30, 225, 65, 236, 81, 250, 65, 82, 184, 9, 66, 174, 71, 22, 66, 10, 215, 34, 66, 164, 112, 47, 66, 0, 0, 60, 66, 92, 143, 72, 66, 246, 40, 85, 66, 82, 184, 97, 66, 174, 71, 110, 66, 10, 215, 122, 66, 82, 184, 131, 66, 0, 0, 138, 66};
.const .align 4 .b8 c_default_ic50[44] = {154, 153, 89, 63, 41, 92, 143, 63, 123, 20, 110, 63, 102, 102, 134, 63, 72, 225, 122, 63, 72, 225, 154, 63, 10, 215, 99, 63, 113, 61, 138, 63, 51, 51, 115, 63, 10, 215, 131, 63, 0, 0, 128, 63};
// _ZZ25mega_fused_vasil_accuracyE9smem_ic50 has been demoted
// _ZZ25mega_fused_vasil_accuracyE13smem_escape_y has been demoted
// _ZZ25mega_fused_vasil_accuracyE15smem_immunity_y has been demoted
// _ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg has been demoted
// _ZZ25mega_fused_vasil_accuracyE14smem_local_min has been demoted
// _ZZ25mega_fused_vasil_accuracyE14smem_local_max has been demoted
.global .align 1 .b8 _ZN48_INTERNAL_10bc5e62_19_mega_fused_vasil_cu_c_tmax4cuda3std3__45__cpo9iter_swapE[1];
.global .align 1 .b8 _ZN48_INTERNAL_10bc5e62_19_mega_fused_vasil_cu_c_tmax4cuda3std3__45__cpo5beginE[1];
.global .align 1 .b8 _ZN48_INTERNAL_10bc5e62_19_mega_fused_vasil_cu_c_tmax4cuda3std3__45__cpo3endE[1];
.global .align 1 .b8 _ZN48_INTERNAL_10bc5e62_19_mega_fused_vasil_cu_c_tmax4cuda3std3__45__cpo6cbeginE[1];
.global .align 1 .b8 _ZN48_INTERNAL_10bc5e62_19_mega_fused_vasil_cu_c_tmax4cuda3std3__45__cpo4cendE[1];
.global .align 1 .b8 _ZN48_INTERNAL_10bc5e62_19_mega_fused_vasil_cu_c_tmax4cuda3std3__450_GLOBAL__N__10bc5e62_19_mega_fused_vasil_cu_c_tmax6ignoreE[1];
.global .align 1 .b8 _ZN48_INTERNAL_10bc5e62_19_mega_fused_vasil_cu_c_tmax4cuda3std3__419piecewise_constructE[1];
.global .align 1 .b8 _ZN48_INTERNAL_10bc5e62_19_mega_fused_vasil_cu_c_tmax4cuda3std3__48in_placeE[1];
.global .align 1 .b8 _ZN48_INTERNAL_10bc5e62_19_mega_fused_vasil_cu_c_tmax4cuda3std6ranges3__45__cpo4swapE[1];
.global .align 1 .b8 _ZN48_INTERNAL_10bc5e62_19_mega_fused_vasil_cu_c_tmax4cuda3std6ranges3__45__cpo9iter_moveE[1];
.global .align 1 .b8 _ZN48_INTERNAL_10bc5e62_19_mega_fused_vasil_cu_c_tmax4cuda3std6ranges3__45__cpo7advanceE[1];

.visible .entry mega_fused_vasil_accuracy(
	.param .u64 .ptr .align 1 mega_fused_vasil_accuracy_param_0,
	.param .u64 .ptr .align 1 mega_fused_vasil_accuracy_param_1,
	.param .u64 .ptr .align 1 mega_fused_vasil_accuracy_param_2,
	.param .u64 .ptr .align 1 mega_fused_vasil_accuracy_param_3,
	.param .u64 .ptr .align 1 mega_fused_vasil_accuracy_param_4,
	.param .u64 .ptr .align 1 mega_fused_vasil_accuracy_param_5,
	.param .u64 .ptr .align 1 mega_fused_vasil_accuracy_param_6,
	.param .u64 .ptr .align 1 mega_fused_vasil_accuracy_param_7,
	.param .f64 mega_fused_vasil_accuracy_param_8,
	.param .u32 mega_fused_vasil_accuracy_param_9,
	.param .u32 mega_fused_vasil_accuracy_param_10,
	.param .u32 mega_fused_vasil_accuracy_param_11,
	.param .u32 mega_fused_vasil_accuracy_param_12
)
{
	.reg .pred 	%p<131>;
	.reg .b16 	%rs<12>;
	.reg .b32 	%r<610>;
	.reg .b64 	%rd<210>;
	// demoted variable
	.shared .align 4 .b8 _ZZ25mega_fused_vasil_accuracyE9smem_ic50[44];
	// demoted variable
	.shared .align 4 .b8 _ZZ25mega_fused_vasil_accuracyE13smem_escape_y[44];
	// demoted variable
	.shared .align 8 .b8 _ZZ25mega_fused_vasil_accuracyE15smem_immunity_y[600];
	// demoted variable
	.shared .align 8 .b8 _ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg[600];
	// demoted variable
	.shared .align 8 .b8 _ZZ25mega_fused_vasil_accuracyE14smem_local_min[2048];
	// demoted variable
	.shared .align 8 .b8 _ZZ25mega_fused_vasil_accuracyE14smem_local_max[2048];
	ld.param.b64 	%rd51, [mega_fused_vasil_accuracy_param_0];
	ld.param.b64 	%rd52, [mega_fused_vasil_accuracy_param_1];
	ld.param.b64 	%rd53, [mega_fused_vasil_accuracy_param_2];
	ld.param.b64 	%rd45, [mega_fused_vasil_accuracy_param_3];
	ld.param.b64 	%rd46, [mega_fused_vasil_accuracy_param_4];
	ld.param.b64 	%rd47, [mega_fused_vasil_accuracy_param_5];
	ld.param.b64 	%rd48, [mega_fused_vasil_accuracy_param_6];
	ld.param.b64 	%rd49, [mega_fused_vasil_accuracy_param_7];
	ld.param.b64 	%rd50, [mega_fused_vasil_accuracy_param_8];
	ld.param.b32 	%r254, [mega_fused_vasil_accuracy_param_9];
	ld.param.b32 	%r255, [mega_fused_vasil_accuracy_param_10];
	ld.param.b32 	%r256, [mega_fused_vasil_accuracy_param_11];
	ld.param.b32 	%r257, [mega_fused_vasil_accuracy_param_12];
	cvta.to.global.u64 	%rd1, %rd53;
	cvta.to.global.u64 	%rd2, %rd51;
	cvta.to.global.u64 	%rd3, %rd52;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ctaid.y;
	setp.ge.s32 	%p2, %r1, %r254;
	setp.ge.s32 	%p3, %r2, %r255;
	or.pred 	%p4, %p2, %p3;
	@%p4 bra 	$L__BB0_164;
	add.s32 	%r3, %r257, %r2;
	setp.ge.s32 	%p5, %r3, %r256;
	setp.lt.s32 	%p6, %r3, 1;
	or.pred 	%p7, %p5, %p6;
	@%p7 bra 	$L__BB0_164;
	mad.lo.s32 	%r258, %r255, %r1, %r2;
	cvt.u64.u32 	%rd54, %r258;
	cvta.to.global.u64 	%rd55, %rd46;
	add.s64 	%rd56, %rd55, %rd54;
	ld.global.nc.b8 	%rs1, [%rd56];
	cvta.to.global.u64 	%rd57, %rd47;
	mul.wide.u32 	%rd58, %r258, 4;
	add.s64 	%rd4, %rd57, %rd58;
	setp.eq.s16 	%p8, %rs1, 0;
	@%p8 bra 	$L__BB0_164;
	ld.global.nc.b32 	%r259, [%rd4];
	abs.ftz.f32 	%r260, %r259;
	setp.lt.ftz.f32 	%p9, %r260, 0f3D4CCCCD;
	@%p9 bra 	$L__BB0_164;
	mad.lo.s32 	%r261, %r256, %r1, %r3;
	mul.wide.s32 	%rd59, %r261, 4;
	add.s64 	%rd60, %rd3, %rd59;
	ld.global.nc.b32 	%r262, [%rd60];
	setp.lt.ftz.f32 	%p10, %r262, 0f3CF5C28F;
	@%p10 bra 	$L__BB0_164;
	mov.u32 	%r4, %tid.x;
	setp.gt.u32 	%p11, %r4, 10;
	@%p11 bra 	$L__BB0_10;
	setp.eq.s64 	%p12, %rd45, 0;
	@%p12 bra 	$L__BB0_8;
	cvta.to.global.u64 	%rd61, %rd45;
	mul.wide.u32 	%rd62, %r4, 4;
	add.s64 	%rd63, %rd61, %rd62;
	ld.global.nc.b32 	%r558, [%rd63];
	bra.uni 	$L__BB0_9;
$L__BB0_8:
	mul.wide.u32 	%rd64, %r4, 4;
	mov.b64 	%rd65, c_default_ic50;
	add.s64 	%rd66, %rd65, %rd64;
	ld.const.b32 	%r558, [%rd66];
$L__BB0_9:
	shl.b32 	%r263, %r4, 2;
	mov.b32 	%r264, _ZZ25mega_fused_vasil_accuracyE9smem_ic50;
	add.s32 	%r265, %r264, %r263;
	st.shared.b32 	[%r265], %r558;
	mad.lo.s32 	%r266, %r1, 11, %r4;
	mul.wide.u32 	%rd67, %r266, 4;
	add.s64 	%rd68, %rd2, %rd67;
	ld.global.nc.b32 	%r267, [%rd68];
	mov.b32 	%r268, _ZZ25mega_fused_vasil_accuracyE13smem_escape_y;
	add.s32 	%r269, %r268, %r263;
	st.shared.b32 	[%r269], %r267;
$L__BB0_10:
	barrier.sync 	0;
	shr.u32 	%r559, %r4, 5;
	mov.u32 	%r270, %tid.y;
	mov.u32 	%r271, %tid.z;
	mov.u32 	%r272, %ntid.x;
	mov.u32 	%r273, %ntid.y;
	mad.lo.s32 	%r274, %r271, %r273, %r270;
	mad.lo.s32 	%r275, %r274, %r272, %r4;
	and.b32 	%r9, %r275, 31;
	mul.lo.s32 	%r10, %r3, %r254;
$L__BB0_11:
	setp.ge.u32 	%p13, %r9, %r10;
	mov.b64 	%rd192, 0d0000000000000000;
	@%p13 bra 	$L__BB0_69;
	cvt.u16.u32 	%rs3, %r559;
	and.b16 	%rs4, %rs3, 255;
	mul.lo.s16 	%rs5, %rs4, 137;
	shr.u16 	%rs6, %rs5, 11;
	cvt.u32.u16 	%r276, %rs6;
	mul.wide.u32 	%rd71, %r276, 4;
	mov.b64 	%rd72, c_tmax;
	add.s64 	%rd5, %rd72, %rd71;
	mul.lo.s16 	%rs7, %rs6, 15;
	sub.s16 	%rs8, %rs3, %rs7;
	cvt.u32.u16 	%r277, %rs8;
	and.b32 	%r278, %r277, 255;
	mul.wide.u32 	%rd73, %r278, 4;
	mov.b64 	%rd74, c_thalf;
	add.s64 	%rd6, %rd74, %rd73;
	ld.shared.b32 	%r279, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y];
	add.ftz.f32 	%r12, %r279, 0f3F800000;
	ld.shared.b32 	%r13, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50];
	ld.shared.b32 	%r280, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+4];
	add.ftz.f32 	%r14, %r280, 0f3F800000;
	ld.shared.b32 	%r15, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+4];
	ld.shared.b32 	%r281, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+8];
	add.ftz.f32 	%r16, %r281, 0f3F800000;
	ld.shared.b32 	%r17, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+8];
	ld.shared.b32 	%r282, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+12];
	add.ftz.f32 	%r18, %r282, 0f3F800000;
	ld.shared.b32 	%r19, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+12];
	ld.shared.b32 	%r283, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+16];
	add.ftz.f32 	%r20, %r283, 0f3F800000;
	ld.shared.b32 	%r21, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+16];
	ld.shared.b32 	%r284, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+20];
	add.ftz.f32 	%r22, %r284, 0f3F800000;
	ld.shared.b32 	%r23, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+20];
	ld.shared.b32 	%r285, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+24];
	add.ftz.f32 	%r24, %r285, 0f3F800000;
	ld.shared.b32 	%r25, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+24];
	ld.shared.b32 	%r286, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+28];
	add.ftz.f32 	%r26, %r286, 0f3F800000;
	ld.shared.b32 	%r27, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+28];
	ld.shared.b32 	%r287, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+32];
	add.ftz.f32 	%r28, %r287, 0f3F800000;
	ld.shared.b32 	%r29, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+32];
	ld.shared.b32 	%r288, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+36];
	add.ftz.f32 	%r30, %r288, 0f3F800000;
	ld.shared.b32 	%r31, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+36];
	ld.shared.b32 	%r289, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+40];
	add.ftz.f32 	%r32, %r289, 0f3F800000;
	mov.b64 	%rd192, 0d0000000000000000;
	ld.shared.b32 	%r33, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+40];
	mov.b32 	%r560, %r9;
$L__BB0_13:
	div.u32 	%r35, %r560, %r3;
	mul.lo.s32 	%r290, %r35, %r3;
	sub.s32 	%r36, %r560, %r290;
	setp.ge.s32 	%p14, %r36, %r256;
	@%p14 bra 	$L__BB0_68;
	sub.s32 	%r37, %r3, %r36;
	add.s32 	%r291, %r37, -1500;
	setp.lt.u32 	%p15, %r291, -1499;
	@%p15 bra 	$L__BB0_68;
	mad.lo.s32 	%r292, %r35, %r256, %r36;
	mul.wide.s32 	%rd75, %r292, 4;
	add.s64 	%rd76, %rd3, %rd75;
	ld.global.nc.b32 	%r38, [%rd76];
	setp.lt.ftz.f32 	%p16, %r38, 0f3A83126F;
	@%p16 bra 	$L__BB0_68;
	mul.wide.u32 	%rd77, %r36, 8;
	add.s64 	%rd78, %rd1, %rd77;
	ld.global.nc.b64 	%rd8, [%rd78];
	setp.lt.f64 	%p17, %rd8, 0d3FF0000000000000;
	@%p17 bra 	$L__BB0_68;
	mul.lo.s32 	%r293, %r35, 11;
	mul.wide.u32 	%rd79, %r293, 4;
	add.s64 	%rd80, %rd2, %rd79;
	ld.global.nc.b32 	%r39, [%rd80];
	ld.global.nc.b32 	%r40, [%rd80+4];
	ld.global.nc.b32 	%r41, [%rd80+8];
	ld.global.nc.b32 	%r42, [%rd80+12];
	ld.global.nc.b32 	%r43, [%rd80+16];
	ld.global.nc.b32 	%r44, [%rd80+20];
	ld.global.nc.b32 	%r45, [%rd80+24];
	ld.global.nc.b32 	%r46, [%rd80+28];
	ld.global.nc.b32 	%r47, [%rd80+32];
	ld.global.nc.b32 	%r48, [%rd80+36];
	ld.global.nc.b32 	%r49, [%rd80+40];
	ld.const.b32 	%r50, [%rd5];
	ld.const.b32 	%r294, [%rd6];
	mov.b32 	%r295, 0f40000000;
	lg2.approx.ftz.f32 	%r296, %r295;
	mul.ftz.f32 	%r51, %r296, 0f3F317218;
	div.approx.ftz.f32 	%r52, %r51, %r294;
	mul.ftz.f32 	%r53, %r50, %r52;
	add.ftz.f32 	%r297, %r51, 0f3C23D70A;
	setp.leu.ftz.f32 	%p18, %r53, %r297;
	@%p18 bra 	$L__BB0_19;
	sub.ftz.f32 	%r298, %r53, %r51;
	div.approx.ftz.f32 	%r299, %r53, %r298;
	lg2.approx.ftz.f32 	%r300, %r299;
	mul.ftz.f32 	%r561, %r300, 0f3F317218;
	bra.uni 	$L__BB0_20;
$L__BB0_19:
	add.ftz.f32 	%r561, %r52, %r52;
$L__BB0_20:
	neg.ftz.f32 	%r57, %r52;
	mul.ftz.f32 	%r301, %r50, %r57;
	mul.ftz.f32 	%r302, %r301, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%r303, %r302;
	neg.ftz.f32 	%r58, %r561;
	mul.ftz.f32 	%r304, %r50, %r58;
	mul.ftz.f32 	%r305, %r304, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%r306, %r305;
	sub.ftz.f32 	%r59, %r303, %r306;
	abs.ftz.f32 	%r307, %r59;
	setp.lt.ftz.f32 	%p19, %r307, 0f2EDBE6FF;
	@%p19 bra 	$L__BB0_68;
	cvt.rn.f32.u32 	%r308, %r37;
	mul.ftz.f32 	%r309, %r308, %r57;
	mul.ftz.f32 	%r310, %r309, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%r311, %r310;
	mul.ftz.f32 	%r312, %r308, %r58;
	mul.ftz.f32 	%r313, %r312, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%r314, %r313;
	sub.ftz.f32 	%r315, %r311, %r314;
	div.approx.ftz.f32 	%r316, %r315, %r59;
	max.ftz.f32 	%r60, %r316, 0f00000000;
	setp.lt.ftz.f32 	%p20, %r60, 0f322BCC77;
	@%p20 bra 	$L__BB0_68;
	setp.leu.ftz.f32 	%p21, %r39, 0f3C23D70A;
	mov.b32 	%r562, %r12;
	@%p21 bra 	$L__BB0_24;
	add.ftz.f32 	%r317, %r39, 0f3F800000;
	div.approx.ftz.f32 	%r562, %r12, %r317;
$L__BB0_24:
	min.ftz.f32 	%r319, %r562, 0f42C80000;
	max.ftz.f32 	%r320, %r319, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r63, %r13, %r320, %r60;
	setp.leu.ftz.f32 	%p22, %r63, 0f2EDBE6FF;
	mov.b32 	%r563, 0f00000000;
	@%p22 bra 	$L__BB0_26;
	div.approx.ftz.f32 	%r563, %r60, %r63;
$L__BB0_26:
	setp.leu.ftz.f32 	%p23, %r40, 0f3C23D70A;
	mov.b32 	%r564, %r14;
	@%p23 bra 	$L__BB0_28;
	add.ftz.f32 	%r321, %r40, 0f3F800000;
	div.approx.ftz.f32 	%r564, %r14, %r321;
$L__BB0_28:
	min.ftz.f32 	%r323, %r564, 0f42C80000;
	max.ftz.f32 	%r324, %r323, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r68, %r15, %r324, %r60;
	setp.leu.ftz.f32 	%p24, %r68, 0f2EDBE6FF;
	mov.b32 	%r565, 0f00000000;
	@%p24 bra 	$L__BB0_30;
	div.approx.ftz.f32 	%r565, %r60, %r68;
$L__BB0_30:
	mov.b32 	%r325, 0f3F800000;
	sub.ftz.f32 	%r326, %r325, %r565;
	sub.ftz.f32 	%r327, %r325, %r563;
	mul.ftz.f32 	%r71, %r327, %r326;
	setp.leu.ftz.f32 	%p25, %r41, 0f3C23D70A;
	mov.b32 	%r566, %r16;
	@%p25 bra 	$L__BB0_32;
	add.ftz.f32 	%r328, %r41, 0f3F800000;
	div.approx.ftz.f32 	%r566, %r16, %r328;
$L__BB0_32:
	min.ftz.f32 	%r330, %r566, 0f42C80000;
	max.ftz.f32 	%r331, %r330, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r74, %r17, %r331, %r60;
	setp.leu.ftz.f32 	%p26, %r74, 0f2EDBE6FF;
	mov.b32 	%r567, 0f00000000;
	@%p26 bra 	$L__BB0_34;
	div.approx.ftz.f32 	%r567, %r60, %r74;
$L__BB0_34:
	mov.b32 	%r332, 0f3F800000;
	sub.ftz.f32 	%r333, %r332, %r567;
	mul.ftz.f32 	%r77, %r71, %r333;
	setp.leu.ftz.f32 	%p27, %r42, 0f3C23D70A;
	mov.b32 	%r568, %r18;
	@%p27 bra 	$L__BB0_36;
	add.ftz.f32 	%r334, %r42, 0f3F800000;
	div.approx.ftz.f32 	%r568, %r18, %r334;
$L__BB0_36:
	min.ftz.f32 	%r336, %r568, 0f42C80000;
	max.ftz.f32 	%r337, %r336, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r80, %r19, %r337, %r60;
	setp.leu.ftz.f32 	%p28, %r80, 0f2EDBE6FF;
	mov.b32 	%r569, 0f00000000;
	@%p28 bra 	$L__BB0_38;
	div.approx.ftz.f32 	%r569, %r60, %r80;
$L__BB0_38:
	mov.b32 	%r338, 0f3F800000;
	sub.ftz.f32 	%r339, %r338, %r569;
	mul.ftz.f32 	%r83, %r77, %r339;
	setp.leu.ftz.f32 	%p29, %r43, 0f3C23D70A;
	mov.b32 	%r570, %r20;
	@%p29 bra 	$L__BB0_40;
	add.ftz.f32 	%r340, %r43, 0f3F800000;
	div.approx.ftz.f32 	%r570, %r20, %r340;
$L__BB0_40:
	min.ftz.f32 	%r342, %r570, 0f42C80000;
	max.ftz.f32 	%r343, %r342, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r86, %r21, %r343, %r60;
	setp.leu.ftz.f32 	%p30, %r86, 0f2EDBE6FF;
	mov.b32 	%r571, 0f00000000;
	@%p30 bra 	$L__BB0_42;
	div.approx.ftz.f32 	%r571, %r60, %r86;
$L__BB0_42:
	mov.b32 	%r344, 0f3F800000;
	sub.ftz.f32 	%r345, %r344, %r571;
	mul.ftz.f32 	%r89, %r83, %r345;
	setp.leu.ftz.f32 	%p31, %r44, 0f3C23D70A;
	mov.b32 	%r572, %r22;
	@%p31 bra 	$L__BB0_44;
	add.ftz.f32 	%r346, %r44, 0f3F800000;
	div.approx.ftz.f32 	%r572, %r22, %r346;
$L__BB0_44:
	min.ftz.f32 	%r348, %r572, 0f42C80000;
	max.ftz.f32 	%r349, %r348, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r92, %r23, %r349, %r60;
	setp.leu.ftz.f32 	%p32, %r92, 0f2EDBE6FF;
	mov.b32 	%r573, 0f00000000;
	@%p32 bra 	$L__BB0_46;
	div.approx.ftz.f32 	%r573, %r60, %r92;
$L__BB0_46:
	mov.b32 	%r350, 0f3F800000;
	sub.ftz.f32 	%r351, %r350, %r573;
	mul.ftz.f32 	%r95, %r89, %r351;
	setp.leu.ftz.f32 	%p33, %r45, 0f3C23D70A;
	mov.b32 	%r574, %r24;
	@%p33 bra 	$L__BB0_48;
	add.ftz.f32 	%r352, %r45, 0f3F800000;
	div.approx.ftz.f32 	%r574, %r24, %r352;
$L__BB0_48:
	min.ftz.f32 	%r354, %r574, 0f42C80000;
	max.ftz.f32 	%r355, %r354, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r98, %r25, %r355, %r60;
	setp.leu.ftz.f32 	%p34, %r98, 0f2EDBE6FF;
	mov.b32 	%r575, 0f00000000;
	@%p34 bra 	$L__BB0_50;
	div.approx.ftz.f32 	%r575, %r60, %r98;
$L__BB0_50:
	mov.b32 	%r356, 0f3F800000;
	sub.ftz.f32 	%r357, %r356, %r575;
	mul.ftz.f32 	%r101, %r95, %r357;
	setp.leu.ftz.f32 	%p35, %r46, 0f3C23D70A;
	mov.b32 	%r576, %r26;
	@%p35 bra 	$L__BB0_52;
	add.ftz.f32 	%r358, %r46, 0f3F800000;
	div.approx.ftz.f32 	%r576, %r26, %r358;
$L__BB0_52:
	min.ftz.f32 	%r360, %r576, 0f42C80000;
	max.ftz.f32 	%r361, %r360, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r104, %r27, %r361, %r60;
	setp.leu.ftz.f32 	%p36, %r104, 0f2EDBE6FF;
	mov.b32 	%r577, 0f00000000;
	@%p36 bra 	$L__BB0_54;
	div.approx.ftz.f32 	%r577, %r60, %r104;
$L__BB0_54:
	mov.b32 	%r362, 0f3F800000;
	sub.ftz.f32 	%r363, %r362, %r577;
	mul.ftz.f32 	%r107, %r101, %r363;
	setp.leu.ftz.f32 	%p37, %r47, 0f3C23D70A;
	mov.b32 	%r578, %r28;
	@%p37 bra 	$L__BB0_56;
	add.ftz.f32 	%r364, %r47, 0f3F800000;
	div.approx.ftz.f32 	%r578, %r28, %r364;
$L__BB0_56:
	min.ftz.f32 	%r366, %r578, 0f42C80000;
	max.ftz.f32 	%r367, %r366, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r110, %r29, %r367, %r60;
	setp.leu.ftz.f32 	%p38, %r110, 0f2EDBE6FF;
	mov.b32 	%r579, 0f00000000;
	@%p38 bra 	$L__BB0_58;
	div.approx.ftz.f32 	%r579, %r60, %r110;
$L__BB0_58:
	mov.b32 	%r368, 0f3F800000;
	sub.ftz.f32 	%r369, %r368, %r579;
	mul.ftz.f32 	%r113, %r107, %r369;
	setp.leu.ftz.f32 	%p39, %r48, 0f3C23D70A;
	mov.b32 	%r580, %r30;
	@%p39 bra 	$L__BB0_60;
	add.ftz.f32 	%r370, %r48, 0f3F800000;
	div.approx.ftz.f32 	%r580, %r30, %r370;
$L__BB0_60:
	min.ftz.f32 	%r372, %r580, 0f42C80000;
	max.ftz.f32 	%r373, %r372, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r116, %r31, %r373, %r60;
	setp.leu.ftz.f32 	%p40, %r116, 0f2EDBE6FF;
	mov.b32 	%r581, 0f00000000;
	@%p40 bra 	$L__BB0_62;
	div.approx.ftz.f32 	%r581, %r60, %r116;
$L__BB0_62:
	mov.b32 	%r374, 0f3F800000;
	sub.ftz.f32 	%r375, %r374, %r581;
	mul.ftz.f32 	%r119, %r113, %r375;
	setp.leu.ftz.f32 	%p41, %r49, 0f3C23D70A;
	mov.b32 	%r582, %r32;
	@%p41 bra 	$L__BB0_64;
	add.ftz.f32 	%r376, %r49, 0f3F800000;
	div.approx.ftz.f32 	%r582, %r32, %r376;
$L__BB0_64:
	min.ftz.f32 	%r378, %r582, 0f42C80000;
	max.ftz.f32 	%r379, %r378, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r122, %r33, %r379, %r60;
	setp.leu.ftz.f32 	%p42, %r122, 0f2EDBE6FF;
	mov.b32 	%r583, 0f00000000;
	@%p42 bra 	$L__BB0_66;
	div.approx.ftz.f32 	%r583, %r60, %r122;
$L__BB0_66:
	mov.b32 	%r380, 0f3F800000;
	sub.ftz.f32 	%r381, %r380, %r583;
	mul.ftz.f32 	%r382, %r119, %r381;
	sub.ftz.f32 	%r125, %r380, %r382;
	setp.leu.ftz.f32 	%p43, %r125, 0f322BCC77;
	@%p43 bra 	$L__BB0_68;
	cvt.ftz.f64.f32 	%rd81, %r38;
	mul.f64 	%rd82, %rd8, %rd81;
	cvt.ftz.f64.f32 	%rd83, %r125;
	fma.rn.f64 	%rd192, %rd82, %rd83, %rd192;
$L__BB0_68:
	add.s32 	%r560, %r560, 32;
	setp.lt.s32 	%p44, %r560, %r10;
	@%p44 bra 	$L__BB0_13;
$L__BB0_69:
	setp.ne.s32 	%p45, %r9, 0;
	// begin inline asm
	mov.b64 {%r383,%r384}, %rd192;
	// end inline asm
	shfl.sync.down.b32 	%r386|%p46, %r384, 16, 31, -1;
	shfl.sync.down.b32 	%r385|%p47, %r383, 16, 31, -1;
	// begin inline asm
	mov.b64 %rd85, {%r385,%r386};
	// end inline asm
	add.f64 	%rd86, %rd192, %rd85;
	// begin inline asm
	mov.b64 {%r387,%r388}, %rd86;
	// end inline asm
	shfl.sync.down.b32 	%r390|%p48, %r388, 8, 31, -1;
	shfl.sync.down.b32 	%r389|%p49, %r387, 8, 31, -1;
	// begin inline asm
	mov.b64 %rd87, {%r389,%r390};
	// end inline asm
	add.f64 	%rd88, %rd86, %rd87;
	// begin inline asm
	mov.b64 {%r391,%r392}, %rd88;
	// end inline asm
	shfl.sync.down.b32 	%r394|%p50, %r392, 4, 31, -1;
	shfl.sync.down.b32 	%r393|%p51, %r391, 4, 31, -1;
	// begin inline asm
	mov.b64 %rd89, {%r393,%r394};
	// end inline asm
	add.f64 	%rd90, %rd88, %rd89;
	// begin inline asm
	mov.b64 {%r395,%r396}, %rd90;
	// end inline asm
	shfl.sync.down.b32 	%r398|%p52, %r396, 2, 31, -1;
	shfl.sync.down.b32 	%r397|%p53, %r395, 2, 31, -1;
	// begin inline asm
	mov.b64 %rd91, {%r397,%r398};
	// end inline asm
	add.f64 	%rd92, %rd90, %rd91;
	// begin inline asm
	mov.b64 {%r399,%r400}, %rd92;
	// end inline asm
	shfl.sync.down.b32 	%r402|%p54, %r400, 1, 31, -1;
	shfl.sync.down.b32 	%r401|%p55, %r399, 1, 31, -1;
	// begin inline asm
	mov.b64 %rd93, {%r401,%r402};
	// end inline asm
	add.f64 	%rd12, %rd92, %rd93;
	@%p45 bra 	$L__BB0_71;
	shl.b32 	%r403, %r559, 3;
	mov.b32 	%r404, _ZZ25mega_fused_vasil_accuracyE15smem_immunity_y;
	add.s32 	%r405, %r404, %r403;
	st.shared.b64 	[%r405], %rd12;
$L__BB0_71:
	add.s32 	%r127, %r559, 8;
	setp.lt.u32 	%p56, %r559, 67;
	mov.b32 	%r559, %r127;
	@%p56 bra 	$L__BB0_11;
	barrier.sync 	0;
	setp.gt.u32 	%p57, %r4, 31;
	@%p57 bra 	$L__BB0_140;
	setp.ge.s32 	%p58, %r9, %r254;
	mov.b64 	%rd202, 0d0000000000000000;
	mov.b64 	%rd203, %rd202;
	@%p58 bra 	$L__BB0_135;
	min.s32 	%r406, %r3, %r256;
	setp.gt.s32 	%p1, %r406, 0;
	ld.const.b32 	%r128, [c_tmax+8];
	ld.const.b32 	%r129, [c_thalf+28];
	max.s32 	%r407, %r406, 1;
	ld.shared.b32 	%r408, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y];
	add.ftz.f32 	%r130, %r408, 0f3F800000;
	ld.shared.b32 	%r131, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50];
	ld.shared.b32 	%r409, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+4];
	add.ftz.f32 	%r132, %r409, 0f3F800000;
	ld.shared.b32 	%r133, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+4];
	ld.shared.b32 	%r410, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+8];
	add.ftz.f32 	%r134, %r410, 0f3F800000;
	ld.shared.b32 	%r135, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+8];
	ld.shared.b32 	%r411, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+12];
	add.ftz.f32 	%r136, %r411, 0f3F800000;
	ld.shared.b32 	%r137, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+12];
	ld.shared.b32 	%r412, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+16];
	add.ftz.f32 	%r138, %r412, 0f3F800000;
	ld.shared.b32 	%r139, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+16];
	ld.shared.b32 	%r413, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+20];
	add.ftz.f32 	%r140, %r413, 0f3F800000;
	ld.shared.b32 	%r141, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+20];
	ld.shared.b32 	%r414, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+24];
	add.ftz.f32 	%r142, %r414, 0f3F800000;
	ld.shared.b32 	%r143, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+24];
	ld.shared.b32 	%r415, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+28];
	add.ftz.f32 	%r144, %r415, 0f3F800000;
	ld.shared.b32 	%r145, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+28];
	ld.shared.b32 	%r416, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+32];
	add.ftz.f32 	%r146, %r416, 0f3F800000;
	ld.shared.b32 	%r147, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+32];
	ld.shared.b32 	%r417, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+36];
	add.ftz.f32 	%r148, %r417, 0f3F800000;
	ld.shared.b32 	%r149, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+36];
	ld.shared.b32 	%r418, [_ZZ25mega_fused_vasil_accuracyE13smem_escape_y+40];
	add.ftz.f32 	%r150, %r418, 0f3F800000;
	ld.shared.b32 	%r151, [_ZZ25mega_fused_vasil_accuracyE9smem_ic50+40];
	neg.s32 	%r152, %r407;
	mov.b64 	%rd203, 0d0000000000000000;
	not.pred 	%p60, %p1;
	mov.b32 	%r584, %r9;
	mov.b64 	%rd202, %rd203;
$L__BB0_75:
	mul.lo.s32 	%r154, %r584, %r256;
	add.s32 	%r419, %r154, %r3;
	mul.wide.s32 	%rd96, %r419, 4;
	add.s64 	%rd97, %rd3, %rd96;
	ld.global.nc.b32 	%r155, [%rd97];
	setp.ltu.ftz.f32 	%p59, %r155, 0f3A83126F;
	@%p59 bra 	$L__BB0_134;
	mov.b64 	%rd200, 0d0000000000000000;
	@%p60 bra 	$L__BB0_133;
	add.s32 	%r586, %r3, -1500;
	mul.lo.s32 	%r420, %r584, 11;
	mul.wide.u32 	%rd100, %r420, 4;
	add.s64 	%rd101, %rd2, %rd100;
	ld.global.nc.b32 	%r157, [%rd101+40];
	ld.global.nc.b32 	%r158, [%rd101+36];
	ld.global.nc.b32 	%r159, [%rd101+32];
	ld.global.nc.b32 	%r160, [%rd101+28];
	ld.global.nc.b32 	%r161, [%rd101+24];
	ld.global.nc.b32 	%r162, [%rd101+20];
	ld.global.nc.b32 	%r163, [%rd101+16];
	ld.global.nc.b32 	%r164, [%rd101+12];
	ld.global.nc.b32 	%r165, [%rd101+8];
	ld.global.nc.b32 	%r166, [%rd101+4];
	ld.global.nc.b32 	%r167, [%rd101];
	add.ftz.f32 	%r168, %r166, 0f3F800000;
	add.ftz.f32 	%r169, %r159, 0f3F800000;
	add.ftz.f32 	%r170, %r158, 0f3F800000;
	add.ftz.f32 	%r171, %r157, 0f3F800000;
	mul.wide.u32 	%rd102, %r154, 4;
	add.s64 	%rd196, %rd3, %rd102;
	mov.b64 	%rd200, 0d0000000000000000;
	div.approx.ftz.f32 	%r190, %r132, %r168;
	div.approx.ftz.f32 	%r232, %r146, %r169;
	div.approx.ftz.f32 	%r238, %r148, %r170;
	div.approx.ftz.f32 	%r244, %r150, %r171;
	mov.b32 	%r585, %r152;
	mov.b64 	%rd197, %rd1;
$L__BB0_78:
	ld.global.nc.b32 	%r174, [%rd196];
	setp.lt.ftz.f32 	%p61, %r174, 0f3A83126F;
	@%p61 bra 	$L__BB0_132;
	ld.global.nc.b64 	%rd19, [%rd197];
	setp.lt.f64 	%p62, %rd19, 0d3FF0000000000000;
	@%p62 bra 	$L__BB0_132;
	setp.lt.u32 	%p63, %r586, -1499;
	@%p63 bra 	$L__BB0_132;
	mov.b32 	%r421, 0f40000000;
	lg2.approx.ftz.f32 	%r422, %r421;
	mul.ftz.f32 	%r175, %r422, 0f3F317218;
	div.approx.ftz.f32 	%r176, %r175, %r129;
	mul.ftz.f32 	%r177, %r128, %r176;
	add.ftz.f32 	%r423, %r175, 0f3C23D70A;
	setp.leu.ftz.f32 	%p64, %r177, %r423;
	@%p64 bra 	$L__BB0_83;
	sub.ftz.f32 	%r424, %r177, %r175;
	div.approx.ftz.f32 	%r425, %r177, %r424;
	lg2.approx.ftz.f32 	%r426, %r425;
	mul.ftz.f32 	%r587, %r426, 0f3F317218;
	bra.uni 	$L__BB0_84;
$L__BB0_83:
	add.ftz.f32 	%r587, %r176, %r176;
$L__BB0_84:
	neg.ftz.f32 	%r181, %r176;
	mul.ftz.f32 	%r427, %r128, %r181;
	mul.ftz.f32 	%r428, %r427, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%r429, %r428;
	neg.ftz.f32 	%r182, %r587;
	mul.ftz.f32 	%r430, %r128, %r182;
	mul.ftz.f32 	%r431, %r430, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%r432, %r431;
	sub.ftz.f32 	%r183, %r429, %r432;
	abs.ftz.f32 	%r433, %r183;
	setp.lt.ftz.f32 	%p65, %r433, 0f2EDBE6FF;
	mov.b64 	%rd199, 0d0000000000000000;
	@%p65 bra 	$L__BB0_131;
	add.s32 	%r434, %r586, 1500;
	cvt.rn.f32.u32 	%r435, %r434;
	mul.ftz.f32 	%r436, %r435, %r181;
	mul.ftz.f32 	%r437, %r436, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%r438, %r437;
	mul.ftz.f32 	%r439, %r435, %r182;
	mul.ftz.f32 	%r440, %r439, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%r441, %r440;
	sub.ftz.f32 	%r442, %r438, %r441;
	div.approx.ftz.f32 	%r443, %r442, %r183;
	max.ftz.f32 	%r184, %r443, 0f00000000;
	setp.lt.ftz.f32 	%p66, %r184, 0f322BCC77;
	@%p66 bra 	$L__BB0_131;
	setp.leu.ftz.f32 	%p67, %r167, 0f3C23D70A;
	mov.b32 	%r588, %r130;
	@%p67 bra 	$L__BB0_88;
	add.ftz.f32 	%r444, %r167, 0f3F800000;
	div.approx.ftz.f32 	%r588, %r130, %r444;
$L__BB0_88:
	min.ftz.f32 	%r446, %r588, 0f42C80000;
	max.ftz.f32 	%r447, %r446, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r187, %r131, %r447, %r184;
	setp.leu.ftz.f32 	%p68, %r187, 0f2EDBE6FF;
	mov.b32 	%r589, 0f00000000;
	@%p68 bra 	$L__BB0_90;
	div.approx.ftz.f32 	%r589, %r184, %r187;
$L__BB0_90:
	setp.leu.ftz.f32 	%p69, %r166, 0f3C23D70A;
	mov.b32 	%r590, %r132;
	@%p69 bra 	$L__BB0_92;
	mov.b32 	%r590, %r190;
$L__BB0_92:
	min.ftz.f32 	%r449, %r590, 0f42C80000;
	max.ftz.f32 	%r450, %r449, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r192, %r133, %r450, %r184;
	setp.leu.ftz.f32 	%p70, %r192, 0f2EDBE6FF;
	mov.b32 	%r591, 0f00000000;
	@%p70 bra 	$L__BB0_94;
	div.approx.ftz.f32 	%r591, %r184, %r192;
$L__BB0_94:
	setp.leu.ftz.f32 	%p71, %r165, 0f3C23D70A;
	mov.b32 	%r451, 0f3F800000;
	sub.ftz.f32 	%r452, %r451, %r591;
	sub.ftz.f32 	%r453, %r451, %r589;
	mul.ftz.f32 	%r195, %r453, %r452;
	mov.b32 	%r592, %r134;
	@%p71 bra 	$L__BB0_96;
	add.ftz.f32 	%r454, %r165, 0f3F800000;
	div.approx.ftz.f32 	%r592, %r134, %r454;
$L__BB0_96:
	min.ftz.f32 	%r456, %r592, 0f42C80000;
	max.ftz.f32 	%r457, %r456, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r198, %r135, %r457, %r184;
	setp.leu.ftz.f32 	%p72, %r198, 0f2EDBE6FF;
	mov.b32 	%r593, 0f00000000;
	@%p72 bra 	$L__BB0_98;
	div.approx.ftz.f32 	%r593, %r184, %r198;
$L__BB0_98:
	setp.leu.ftz.f32 	%p73, %r164, 0f3C23D70A;
	mov.b32 	%r458, 0f3F800000;
	sub.ftz.f32 	%r459, %r458, %r593;
	mul.ftz.f32 	%r201, %r195, %r459;
	mov.b32 	%r594, %r136;
	@%p73 bra 	$L__BB0_100;
	add.ftz.f32 	%r460, %r164, 0f3F800000;
	div.approx.ftz.f32 	%r594, %r136, %r460;
$L__BB0_100:
	min.ftz.f32 	%r462, %r594, 0f42C80000;
	max.ftz.f32 	%r463, %r462, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r204, %r137, %r463, %r184;
	setp.leu.ftz.f32 	%p74, %r204, 0f2EDBE6FF;
	mov.b32 	%r595, 0f00000000;
	@%p74 bra 	$L__BB0_102;
	div.approx.ftz.f32 	%r595, %r184, %r204;
$L__BB0_102:
	setp.leu.ftz.f32 	%p75, %r163, 0f3C23D70A;
	mov.b32 	%r464, 0f3F800000;
	sub.ftz.f32 	%r465, %r464, %r595;
	mul.ftz.f32 	%r207, %r201, %r465;
	mov.b32 	%r596, %r138;
	@%p75 bra 	$L__BB0_104;
	add.ftz.f32 	%r466, %r163, 0f3F800000;
	div.approx.ftz.f32 	%r596, %r138, %r466;
$L__BB0_104:
	min.ftz.f32 	%r468, %r596, 0f42C80000;
	max.ftz.f32 	%r469, %r468, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r210, %r139, %r469, %r184;
	setp.leu.ftz.f32 	%p76, %r210, 0f2EDBE6FF;
	mov.b32 	%r597, 0f00000000;
	@%p76 bra 	$L__BB0_106;
	div.approx.ftz.f32 	%r597, %r184, %r210;
$L__BB0_106:
	setp.leu.ftz.f32 	%p77, %r162, 0f3C23D70A;
	mov.b32 	%r470, 0f3F800000;
	sub.ftz.f32 	%r471, %r470, %r597;
	mul.ftz.f32 	%r213, %r207, %r471;
	mov.b32 	%r598, %r140;
	@%p77 bra 	$L__BB0_108;
	add.ftz.f32 	%r472, %r162, 0f3F800000;
	div.approx.ftz.f32 	%r598, %r140, %r472;
$L__BB0_108:
	min.ftz.f32 	%r474, %r598, 0f42C80000;
	max.ftz.f32 	%r475, %r474, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r216, %r141, %r475, %r184;
	setp.leu.ftz.f32 	%p78, %r216, 0f2EDBE6FF;
	mov.b32 	%r599, 0f00000000;
	@%p78 bra 	$L__BB0_110;
	div.approx.ftz.f32 	%r599, %r184, %r216;
$L__BB0_110:
	setp.leu.ftz.f32 	%p79, %r161, 0f3C23D70A;
	mov.b32 	%r476, 0f3F800000;
	sub.ftz.f32 	%r477, %r476, %r599;
	mul.ftz.f32 	%r219, %r213, %r477;
	mov.b32 	%r600, %r142;
	@%p79 bra 	$L__BB0_112;
	add.ftz.f32 	%r478, %r161, 0f3F800000;
	div.approx.ftz.f32 	%r600, %r142, %r478;
$L__BB0_112:
	min.ftz.f32 	%r480, %r600, 0f42C80000;
	max.ftz.f32 	%r481, %r480, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r222, %r143, %r481, %r184;
	setp.leu.ftz.f32 	%p80, %r222, 0f2EDBE6FF;
	mov.b32 	%r601, 0f00000000;
	@%p80 bra 	$L__BB0_114;
	div.approx.ftz.f32 	%r601, %r184, %r222;
$L__BB0_114:
	setp.leu.ftz.f32 	%p81, %r160, 0f3C23D70A;
	mov.b32 	%r482, 0f3F800000;
	sub.ftz.f32 	%r483, %r482, %r601;
	mul.ftz.f32 	%r225, %r219, %r483;
	mov.b32 	%r602, %r144;
	@%p81 bra 	$L__BB0_116;
	add.ftz.f32 	%r484, %r160, 0f3F800000;
	div.approx.ftz.f32 	%r602, %r144, %r484;
$L__BB0_116:
	min.ftz.f32 	%r486, %r602, 0f42C80000;
	max.ftz.f32 	%r487, %r486, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r228, %r145, %r487, %r184;
	setp.leu.ftz.f32 	%p82, %r228, 0f2EDBE6FF;
	mov.b32 	%r603, 0f00000000;
	@%p82 bra 	$L__BB0_118;
	div.approx.ftz.f32 	%r603, %r184, %r228;
$L__BB0_118:
	setp.leu.ftz.f32 	%p83, %r159, 0f3C23D70A;
	mov.b32 	%r488, 0f3F800000;
	sub.ftz.f32 	%r489, %r488, %r603;
	mul.ftz.f32 	%r231, %r225, %r489;
	mov.b32 	%r604, %r146;
	@%p83 bra 	$L__BB0_120;
	mov.b32 	%r604, %r232;
$L__BB0_120:
	min.ftz.f32 	%r491, %r604, 0f42C80000;
	max.ftz.f32 	%r492, %r491, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r234, %r147, %r492, %r184;
	setp.leu.ftz.f32 	%p84, %r234, 0f2EDBE6FF;
	mov.b32 	%r605, 0f00000000;
	@%p84 bra 	$L__BB0_122;
	div.approx.ftz.f32 	%r605, %r184, %r234;
$L__BB0_122:
	setp.leu.ftz.f32 	%p85, %r158, 0f3C23D70A;
	mov.b32 	%r493, 0f3F800000;
	sub.ftz.f32 	%r494, %r493, %r605;
	mul.ftz.f32 	%r237, %r231, %r494;
	mov.b32 	%r606, %r148;
	@%p85 bra 	$L__BB0_124;
	mov.b32 	%r606, %r238;
$L__BB0_124:
	min.ftz.f32 	%r496, %r606, 0f42C80000;
	max.ftz.f32 	%r497, %r496, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r240, %r149, %r497, %r184;
	setp.leu.ftz.f32 	%p86, %r240, 0f2EDBE6FF;
	mov.b32 	%r607, 0f00000000;
	@%p86 bra 	$L__BB0_126;
	div.approx.ftz.f32 	%r607, %r184, %r240;
$L__BB0_126:
	setp.leu.ftz.f32 	%p87, %r157, 0f3C23D70A;
	mov.b32 	%r498, 0f3F800000;
	sub.ftz.f32 	%r499, %r498, %r607;
	mul.ftz.f32 	%r243, %r237, %r499;
	mov.b32 	%r608, %r150;
	@%p87 bra 	$L__BB0_128;
	mov.b32 	%r608, %r244;
$L__BB0_128:
	min.ftz.f32 	%r501, %r608, 0f42C80000;
	max.ftz.f32 	%r502, %r501, 0f3DCCCCCD;
	fma.rn.ftz.f32 	%r246, %r151, %r502, %r184;
	setp.leu.ftz.f32 	%p88, %r246, 0f2EDBE6FF;
	mov.b32 	%r609, 0f00000000;
	@%p88 bra 	$L__BB0_130;
	div.approx.ftz.f32 	%r609, %r184, %r246;
$L__BB0_130:
	mov.b32 	%r503, 0f3F800000;
	sub.ftz.f32 	%r504, %r503, %r609;
	mul.ftz.f32 	%r505, %r243, %r504;
	sub.ftz.f32 	%r506, %r503, %r505;
	cvt.ftz.f64.f32 	%rd199, %r506;
$L__BB0_131:
	cvt.ftz.f64.f32 	%rd105, %r174;
	mul.f64 	%rd106, %rd19, %rd105;
	fma.rn.f64 	%rd200, %rd106, %rd199, %rd200;
$L__BB0_132:
	add.s32 	%r586, %r586, -1;
	add.s64 	%rd197, %rd197, 8;
	add.s32 	%r585, %r585, 1;
	setp.ne.s32 	%p89, %r585, 0;
	add.s64 	%rd196, %rd196, 4;
	@%p89 bra 	$L__BB0_78;
$L__BB0_133:
	sub.f64 	%rd107, %rd50, %rd200;
	max.f64 	%rd108, %rd107, 0d0000000000000000;
	cvt.ftz.f64.f32 	%rd109, %r155;
	fma.rn.f64 	%rd202, %rd108, %rd109, %rd202;
	add.f64 	%rd203, %rd203, %rd109;
$L__BB0_134:
	add.s32 	%r584, %r584, 32;
	setp.lt.s32 	%p90, %r584, %r254;
	@%p90 bra 	$L__BB0_75;
$L__BB0_135:
	setp.ne.s32 	%p91, %r9, 0;
	// begin inline asm
	mov.b64 {%r507,%r508}, %rd202;
	// end inline asm
	shfl.sync.down.b32 	%r510|%p92, %r508, 16, 31, -1;
	shfl.sync.down.b32 	%r509|%p93, %r507, 16, 31, -1;
	// begin inline asm
	mov.b64 %rd111, {%r509,%r510};
	// end inline asm
	add.f64 	%rd114, %rd202, %rd111;
	// begin inline asm
	mov.b64 {%r511,%r512}, %rd203;
	// end inline asm
	shfl.sync.down.b32 	%r514|%p94, %r512, 16, 31, -1;
	shfl.sync.down.b32 	%r513|%p95, %r511, 16, 31, -1;
	// begin inline asm
	mov.b64 %rd113, {%r513,%r514};
	// end inline asm
	add.f64 	%rd116, %rd203, %rd113;
	// begin inline asm
	mov.b64 {%r515,%r516}, %rd114;
	// end inline asm
	shfl.sync.down.b32 	%r518|%p96, %r516, 8, 31, -1;
	shfl.sync.down.b32 	%r517|%p97, %r515, 8, 31, -1;
	// begin inline asm
	mov.b64 %rd115, {%r517,%r518};
	// end inline asm
	add.f64 	%rd118, %rd114, %rd115;
	// begin inline asm
	mov.b64 {%r519,%r520}, %rd116;
	// end inline asm
	shfl.sync.down.b32 	%r522|%p98, %r520, 8, 31, -1;
	shfl.sync.down.b32 	%r521|%p99, %r519, 8, 31, -1;
	// begin inline asm
	mov.b64 %rd117, {%r521,%r522};
	// end inline asm
	add.f64 	%rd120, %rd116, %rd117;
	// begin inline asm
	mov.b64 {%r523,%r524}, %rd118;
	// end inline asm
	shfl.sync.down.b32 	%r526|%p100, %r524, 4, 31, -1;
	shfl.sync.down.b32 	%r525|%p101, %r523, 4, 31, -1;
	// begin inline asm
	mov.b64 %rd119, {%r525,%r526};
	// end inline asm
	add.f64 	%rd122, %rd118, %rd119;
	// begin inline asm
	mov.b64 {%r527,%r528}, %rd120;
	// end inline asm
	shfl.sync.down.b32 	%r530|%p102, %r528, 4, 31, -1;
	shfl.sync.down.b32 	%r529|%p103, %r527, 4, 31, -1;
	// begin inline asm
	mov.b64 %rd121, {%r529,%r530};
	// end inline asm
	add.f64 	%rd124, %rd120, %rd121;
	// begin inline asm
	mov.b64 {%r531,%r532}, %rd122;
	// end inline asm
	shfl.sync.down.b32 	%r534|%p104, %r532, 2, 31, -1;
	shfl.sync.down.b32 	%r533|%p105, %r531, 2, 31, -1;
	// begin inline asm
	mov.b64 %rd123, {%r533,%r534};
	// end inline asm
	add.f64 	%rd126, %rd122, %rd123;
	// begin inline asm
	mov.b64 {%r535,%r536}, %rd124;
	// end inline asm
	shfl.sync.down.b32 	%r538|%p106, %r536, 2, 31, -1;
	shfl.sync.down.b32 	%r537|%p107, %r535, 2, 31, -1;
	// begin inline asm
	mov.b64 %rd125, {%r537,%r538};
	// end inline asm
	add.f64 	%rd128, %rd124, %rd125;
	// begin inline asm
	mov.b64 {%r539,%r540}, %rd126;
	// end inline asm
	shfl.sync.down.b32 	%r542|%p108, %r540, 1, 31, -1;
	shfl.sync.down.b32 	%r541|%p109, %r539, 1, 31, -1;
	// begin inline asm
	mov.b64 %rd127, {%r541,%r542};
	// end inline asm
	add.f64 	%rd33, %rd126, %rd127;
	// begin inline asm
	mov.b64 {%r543,%r544}, %rd128;
	// end inline asm
	shfl.sync.down.b32 	%r546|%p110, %r544, 1, 31, -1;
	shfl.sync.down.b32 	%r545|%p111, %r543, 1, 31, -1;
	// begin inline asm
	mov.b64 %rd129, {%r545,%r546};
	// end inline asm
	add.f64 	%rd34, %rd128, %rd129;
	@%p91 bra 	$L__BB0_140;
	setp.leu.f64 	%p112, %rd34, 0d0000000000000000;
	@%p112 bra 	$L__BB0_138;
	div.rn.f64 	%rd206, %rd33, %rd34;
	bra.uni 	$L__BB0_139;
$L__BB0_138:
	mul.f64 	%rd206, %rd50, 0d3FE0000000000000;
$L__BB0_139:
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+8], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+16], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+24], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+32], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+40], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+48], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+56], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+64], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+72], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+80], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+88], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+96], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+104], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+112], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+120], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+128], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+136], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+144], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+152], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+160], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+168], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+176], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+184], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+192], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+200], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+208], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+216], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+224], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+232], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+240], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+248], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+256], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+264], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+272], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+280], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+288], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+296], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+304], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+312], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+320], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+328], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+336], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+344], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+352], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+360], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+368], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+376], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+384], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+392], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+400], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+408], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+416], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+424], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+432], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+440], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+448], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+456], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+464], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+472], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+480], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+488], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+496], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+504], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+512], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+520], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+528], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+536], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+544], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+552], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+560], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+568], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+576], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+584], %rd206;
	st.shared.b64 	[_ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg+592], %rd206;
$L__BB0_140:
	barrier.sync 	0;
	barrier.sync 	0;
	setp.gt.u32 	%p113, %r4, 74;
	mov.b64 	%rd209, 0dC202A05F20000000;
	mov.b64 	%rd208, 0d4202A05F20000000;
	@%p113 bra 	$L__BB0_144;
	shl.b32 	%r547, %r4, 3;
	mov.b32 	%r548, _ZZ25mega_fused_vasil_accuracyE17smem_weighted_avg;
	add.s32 	%r549, %r548, %r547;
	ld.shared.b64 	%rd38, [%r549];
	setp.leu.f64 	%p114, %rd38, 0d3FB999999999999A;
	mov.b64 	%rd207, 0d0000000000000000;
	@%p114 bra 	$L__BB0_143;
	mov.b32 	%r551, _ZZ25mega_fused_vasil_accuracyE15smem_immunity_y;
	add.s32 	%r552, %r551, %r547;
	ld.shared.b64 	%rd133, [%r552];
	sub.f64 	%rd134, %rd50, %rd133;
	max.f64 	%rd135, %rd134, 0d0000000000000000;
	div.rn.f64 	%rd136, %rd135, %rd38;
	add.f64 	%rd207, %rd136, 0dBFF0000000000000;
$L__BB0_143:
	max.f64 	%rd209, %rd207, 0dC202A05F20000000;
	min.f64 	%rd208, %rd207, 0d4202A05F20000000;
$L__BB0_144:
	shl.b32 	%r553, %r4, 3;
	mov.b32 	%r554, _ZZ25mega_fused_vasil_accuracyE14smem_local_min;
	add.s32 	%r252, %r554, %r553;
	st.shared.b64 	[%r252], %rd208;
	mov.b32 	%r555, _ZZ25mega_fused_vasil_accuracyE14smem_local_max;
	add.s32 	%r253, %r555, %r553;
	st.shared.b64 	[%r253], %rd209;
	barrier.sync 	0;
	setp.gt.u32 	%p115, %r4, 127;
	@%p115 bra 	$L__BB0_146;
	ld.shared.b64 	%rd137, [%r252];
	ld.shared.b64 	%rd138, [%r252+1024];
	min.f64 	%rd139, %rd137, %rd138;
	st.shared.b64 	[%r252], %rd139;
	ld.shared.b64 	%rd140, [%r253];
	ld.shared.b64 	%rd141, [%r253+1024];
	max.f64 	%rd142, %rd140, %rd141;
	st.shared.b64 	[%r253], %rd142;
$L__BB0_146:
	barrier.sync 	0;
	setp.gt.u32 	%p116, %r4, 63;
	@%p116 bra 	$L__BB0_148;
	ld.shared.b64 	%rd143, [%r252];
	ld.shared.b64 	%rd144, [%r252+512];
	min.f64 	%rd145, %rd143, %rd144;
	st.shared.b64 	[%r252], %rd145;
	ld.shared.b64 	%rd146, [%r253];
	ld.shared.b64 	%rd147, [%r253+512];
	max.f64 	%rd148, %rd146, %rd147;
	st.shared.b64 	[%r253], %rd148;
$L__BB0_148:
	setp.gt.u32 	%p117, %r4, 31;
	barrier.sync 	0;
	@%p117 bra 	$L__BB0_150;
	ld.shared.b64 	%rd149, [%r252];
	ld.shared.b64 	%rd150, [%r252+256];
	min.f64 	%rd151, %rd149, %rd150;
	st.shared.b64 	[%r252], %rd151;
	ld.shared.b64 	%rd152, [%r253];
	ld.shared.b64 	%rd153, [%r253+256];
	max.f64 	%rd154, %rd152, %rd153;
	st.shared.b64 	[%r253], %rd154;
$L__BB0_150:
	barrier.sync 	0;
	setp.gt.u32 	%p118, %r4, 15;
	@%p118 bra 	$L__BB0_152;
	ld.shared.b64 	%rd155, [%r252];
	ld.shared.b64 	%rd156, [%r252+128];
	min.f64 	%rd157, %rd155, %rd156;
	st.shared.b64 	[%r252], %rd157;
	ld.shared.b64 	%rd158, [%r253];
	ld.shared.b64 	%rd159, [%r253+128];
	max.f64 	%rd160, %rd158, %rd159;
	st.shared.b64 	[%r253], %rd160;
$L__BB0_152:
	barrier.sync 	0;
	setp.gt.u32 	%p119, %r4, 7;
	@%p119 bra 	$L__BB0_154;
	ld.shared.b64 	%rd161, [%r252];
	ld.shared.b64 	%rd162, [%r252+64];
	min.f64 	%rd163, %rd161, %rd162;
	st.shared.b64 	[%r252], %rd163;
	ld.shared.b64 	%rd164, [%r253];
	ld.shared.b64 	%rd165, [%r253+64];
	max.f64 	%rd166, %rd164, %rd165;
	st.shared.b64 	[%r253], %rd166;
$L__BB0_154:
	barrier.sync 	0;
	setp.gt.u32 	%p120, %r4, 3;
	@%p120 bra 	$L__BB0_156;
	ld.shared.b64 	%rd167, [%r252];
	ld.shared.b64 	%rd168, [%r252+32];
	min.f64 	%rd169, %rd167, %rd168;
	st.shared.b64 	[%r252], %rd169;
	ld.shared.b64 	%rd170, [%r253];
	ld.shared.b64 	%rd171, [%r253+32];
	max.f64 	%rd172, %rd170, %rd171;
	st.shared.b64 	[%r253], %rd172;
$L__BB0_156:
	barrier.sync 	0;
	setp.gt.u32 	%p121, %r4, 1;
	@%p121 bra 	$L__BB0_158;
	ld.shared.b64 	%rd173, [%r252];
	ld.shared.b64 	%rd174, [%r252+16];
	min.f64 	%rd175, %rd173, %rd174;
	st.shared.b64 	[%r252], %rd175;
	ld.shared.b64 	%rd176, [%r253];
	ld.shared.b64 	%rd177, [%r253+16];
	max.f64 	%rd178, %rd176, %rd177;
	st.shared.b64 	[%r253], %rd178;
$L__BB0_158:
	setp.ne.s32 	%p122, %r4, 0;
	barrier.sync 	0;
	@%p122 bra 	$L__BB0_160;
	ld.shared.b64 	%rd179, [%r252];
	ld.shared.b64 	%rd180, [_ZZ25mega_fused_vasil_accuracyE14smem_local_min+8];
	min.f64 	%rd181, %rd179, %rd180;
	st.shared.b64 	[%r252], %rd181;
	ld.shared.b64 	%rd182, [%r253];
	ld.shared.b64 	%rd183, [_ZZ25mega_fused_vasil_accuracyE14smem_local_max+8];
	max.f64 	%rd184, %rd182, %rd183;
	st.shared.b64 	[%r253], %rd184;
$L__BB0_160:
	setp.ne.s32 	%p123, %r4, 0;
	barrier.sync 	0;
	@%p123 bra 	$L__BB0_164;
	ld.shared.b64 	%rd185, [_ZZ25mega_fused_vasil_accuracyE14smem_local_min];
	ld.shared.b64 	%rd186, [_ZZ25mega_fused_vasil_accuracyE14smem_local_max];
	setp.gt.f64 	%p124, %rd185, 0d0000000000000000;
	setp.gt.f64 	%p125, %rd186, 0d0000000000000000;
	setp.lt.f64 	%p126, %rd185, 0d0000000000000000;
	setp.lt.f64 	%p127, %rd186, 0d0000000000000000;
	and.pred 	%p128, %p126, %p127;
	selp.b16 	%rs9, -1, 0, %p128;
	selp.b16 	%rs10, 1, %rs9, %p125;
	selp.b16 	%rs2, %rs10, %rs9, %p124;
	setp.eq.s16 	%p129, %rs2, 0;
	@%p129 bra 	$L__BB0_164;
	cvta.to.global.u64 	%rd188, %rd49;
	atom.global.add.u32 	%r556, [%rd188], 1;
	and.b16 	%rs11, %rs2, 255;
	setp.ne.s16 	%p130, %rs11, %rs1;
	@%p130 bra 	$L__BB0_164;
	cvta.to.global.u64 	%rd189, %rd48;
	atom.global.add.u32 	%r557, [%rd189], 1;
$L__BB0_164:
	ret;

}
	// .globl	reset_accuracy_counters
.visible .entry reset_accuracy_counters(
	.param .u64 .ptr .align 1 reset_accuracy_counters_param_0,
	.param .u64 .ptr .align 1 reset_accuracy_counters_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<5>;

	ld.param.b64 	%rd1, [reset_accuracy_counters_param_0];
	ld.param.b64 	%rd2, [reset_accuracy_counters_param_1];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	or.b32 	%r3, %r1, %r2;
	setp.ne.s32 	%p1, %r3, 0;
	@%p1 bra 	$L__BB1_2;
	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	st.global.b32 	[%rd3], 0;
	st.global.b32 	[%rd4], 0;
$L__BB1_2:
	ret;

}
	// .globl	compute_final_accuracy
.visible .entry compute_final_accuracy(
	.param .u64 .ptr .align 1 compute_final_accuracy_param_0,
	.param .u64 .ptr .align 1 compute_final_accuracy_param_1,
	.param .u64 .ptr .align 1 compute_final_accuracy_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<7>;

	ld.param.b64 	%rd1, [compute_final_accuracy_param_0];
	ld.param.b64 	%rd2, [compute_final_accuracy_param_1];
	ld.param.b64 	%rd3, [compute_final_accuracy_param_2];
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r5, %ctaid.x;
	or.b32 	%r6, %r4, %r5;
	setp.ne.s32 	%p1, %r6, 0;
	@%p1 bra 	$L__BB2_4;
	cvta.to.global.u64 	%rd4, %rd2;
	ld.global.b32 	%r1, [%rd4];
	setp.eq.s32 	%p2, %r1, 0;
	mov.b32 	%r11, 0f00000000;
	@%p2 bra 	$L__BB2_3;
	cvta.to.global.u64 	%rd5, %rd1;
	ld.global.b32 	%r8, [%rd5];
	cvt.rn.f32.u32 	%r9, %r8;
	cvt.rn.f32.u32 	%r10, %r1;
	div.approx.ftz.f32 	%r11, %r9, %r10;
$L__BB2_3:
	cvta.to.global.u64 	%rd6, %rd3;
	st.global.b32 	[%rd6], %r11;
$L__BB2_4:
	ret;

}
