//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-37061995
// Cuda compilation tools, release 13.1, V13.1.115
// Based on NVVM 21.0.0
//

.version 9.1
.target sm_120
.address_size 64

	// .globl	hessian_matvec
// _ZZ14hessian_matvecE9s_partial has been demoted
// _ZZ10vector_dotE9s_partial has been demoted
// _ZZ8find_maxE5s_max has been demoted
// _ZZ17rayleigh_quotientE5s_vAv has been demoted
// _ZZ17rayleigh_quotientE4s_vv has been demoted

.visible .entry hessian_matvec(
	.param .u64 .ptr .align 1 hessian_matvec_param_0,
	.param .u64 .ptr .align 1 hessian_matvec_param_1,
	.param .u64 .ptr .align 1 hessian_matvec_param_2,
	.param .u32 hessian_matvec_param_3,
	.param .f32 hessian_matvec_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<13>;
	// demoted variable
	.shared .align 4 .b8 _ZZ14hessian_matvecE9s_partial[1024];
	ld.param.b64 	%rd3, [hessian_matvec_param_0];
	ld.param.b64 	%rd4, [hessian_matvec_param_1];
	ld.param.b64 	%rd5, [hessian_matvec_param_2];
	ld.param.b32 	%r14, [hessian_matvec_param_3];
	ld.param.b32 	%r15, [hessian_matvec_param_4];
	mov.u32 	%r1, %ctaid.x;
	setp.ge.s32 	%p1, %r1, %r14;
	@%p1 bra 	$L__BB0_10;
	mov.u32 	%r2, %tid.x;
	setp.ge.u32 	%p2, %r2, %r14;
	mov.b32 	%r33, 0f00000000;
	@%p2 bra 	$L__BB0_4;
	mul.lo.s32 	%r3, %r14, %r1;
	mov.u32 	%r4, %ntid.x;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
	mov.b32 	%r33, 0f00000000;
	mov.b32 	%r31, %r2;
$L__BB0_3:
	add.s32 	%r18, %r31, %r3;
	mul.wide.s32 	%rd6, %r18, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.nc.b32 	%r19, [%rd7];
	setp.eq.s32 	%p3, %r1, %r31;
	add.ftz.f32 	%r20, %r15, %r19;
	selp.f32 	%r21, %r20, %r19, %p3;
	mul.wide.s32 	%rd8, %r31, 4;
	add.s64 	%rd9, %rd2, %rd8;
	ld.global.nc.b32 	%r22, [%rd9];
	fma.rn.ftz.f32 	%r33, %r22, %r21, %r33;
	add.s32 	%r31, %r31, %r4;
	setp.lt.s32 	%p4, %r31, %r14;
	@%p4 bra 	$L__BB0_3;
$L__BB0_4:
	shl.b32 	%r23, %r2, 2;
	mov.b32 	%r24, _ZZ14hessian_matvecE9s_partial;
	add.s32 	%r10, %r24, %r23;
	st.shared.b32 	[%r10], %r33;
	bar.sync 	0;
	mov.u32 	%r34, %ntid.x;
	setp.lt.u32 	%p5, %r34, 2;
	@%p5 bra 	$L__BB0_8;
$L__BB0_5:
	shr.u32 	%r13, %r34, 1;
	setp.ge.u32 	%p6, %r2, %r13;
	@%p6 bra 	$L__BB0_7;
	shl.b32 	%r25, %r13, 2;
	add.s32 	%r26, %r10, %r25;
	ld.shared.b32 	%r27, [%r26];
	ld.shared.b32 	%r28, [%r10];
	add.ftz.f32 	%r29, %r27, %r28;
	st.shared.b32 	[%r10], %r29;
$L__BB0_7:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r34, 3;
	mov.b32 	%r34, %r13;
	@%p7 bra 	$L__BB0_5;
$L__BB0_8:
	setp.ne.s32 	%p8, %r2, 0;
	@%p8 bra 	$L__BB0_10;
	ld.shared.b32 	%r30, [_ZZ14hessian_matvecE9s_partial];
	cvta.to.global.u64 	%rd10, %rd5;
	mul.wide.u32 	%rd11, %r1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	st.global.b32 	[%rd12], %r30;
$L__BB0_10:
	ret;

}
	// .globl	vector_dot
.visible .entry vector_dot(
	.param .u64 .ptr .align 1 vector_dot_param_0,
	.param .u64 .ptr .align 1 vector_dot_param_1,
	.param .u64 .ptr .align 1 vector_dot_param_2,
	.param .u32 vector_dot_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<33>;
	.reg .b64 	%rd<10>;
	// demoted variable
	.shared .align 4 .b8 _ZZ10vector_dotE9s_partial[1024];
	ld.param.b64 	%rd3, [vector_dot_param_0];
	ld.param.b64 	%rd4, [vector_dot_param_1];
	ld.param.b64 	%rd5, [vector_dot_param_2];
	ld.param.b32 	%r13, [vector_dot_param_3];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r32, %ntid.x;
	mad.lo.s32 	%r29, %r15, %r32, %r1;
	setp.ge.s32 	%p1, %r29, %r13;
	mov.b32 	%r31, 0f00000000;
	@%p1 bra 	$L__BB1_3;
	mov.u32 	%r17, %nctaid.x;
	mul.lo.s32 	%r4, %r32, %r17;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
	mov.b32 	%r31, 0f00000000;
$L__BB1_2:
	mul.wide.s32 	%rd6, %r29, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.nc.b32 	%r18, [%rd7];
	add.s64 	%rd8, %rd2, %rd6;
	ld.global.nc.b32 	%r19, [%rd8];
	fma.rn.ftz.f32 	%r31, %r18, %r19, %r31;
	add.s32 	%r29, %r29, %r4;
	setp.lt.s32 	%p2, %r29, %r13;
	@%p2 bra 	$L__BB1_2;
$L__BB1_3:
	shl.b32 	%r20, %r1, 2;
	mov.b32 	%r21, _ZZ10vector_dotE9s_partial;
	add.s32 	%r10, %r21, %r20;
	st.shared.b32 	[%r10], %r31;
	bar.sync 	0;
	setp.lt.u32 	%p3, %r32, 2;
	@%p3 bra 	$L__BB1_7;
$L__BB1_4:
	shr.u32 	%r12, %r32, 1;
	setp.ge.u32 	%p4, %r1, %r12;
	@%p4 bra 	$L__BB1_6;
	shl.b32 	%r22, %r12, 2;
	add.s32 	%r23, %r10, %r22;
	ld.shared.b32 	%r24, [%r23];
	ld.shared.b32 	%r25, [%r10];
	add.ftz.f32 	%r26, %r24, %r25;
	st.shared.b32 	[%r10], %r26;
$L__BB1_6:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r32, 3;
	mov.b32 	%r32, %r12;
	@%p5 bra 	$L__BB1_4;
$L__BB1_7:
	setp.ne.s32 	%p6, %r1, 0;
	@%p6 bra 	$L__BB1_9;
	ld.shared.b32 	%r27, [_ZZ10vector_dotE9s_partial];
	cvta.to.global.u64 	%rd9, %rd5;
	atom.global.add.f32 	%r28, [%rd9], %r27;
$L__BB1_9:
	ret;

}
	// .globl	vector_normalize
.visible .entry vector_normalize(
	.param .u64 .ptr .align 1 vector_normalize_param_0,
	.param .u64 .ptr .align 1 vector_normalize_param_1,
	.param .u32 vector_normalize_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<7>;

	ld.param.b64 	%rd1, [vector_normalize_param_0];
	ld.param.b64 	%rd2, [vector_normalize_param_1];
	ld.param.b32 	%r3, [vector_normalize_param_2];
	cvta.to.global.u64 	%rd3, %rd2;
	ld.global.nc.b32 	%r4, [%rd3];
	sqrt.approx.ftz.f32 	%r1, %r4;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r7;
	setp.ge.s32 	%p1, %r2, %r3;
	@%p1 bra 	$L__BB2_2;
	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r2, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.b32 	%r8, [%rd6];
	setp.lt.ftz.f32 	%p2, %r1, 0f2EDBE6FF;
	selp.f32 	%r9, 0f3F800000, %r1, %p2;
	div.approx.ftz.f32 	%r10, %r8, %r9;
	st.global.b32 	[%rd6], %r10;
$L__BB2_2:
	ret;

}
	// .globl	vector_axpy
.visible .entry vector_axpy(
	.param .f32 vector_axpy_param_0,
	.param .u64 .ptr .align 1 vector_axpy_param_1,
	.param .u64 .ptr .align 1 vector_axpy_param_2,
	.param .u32 vector_axpy_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<8>;

	ld.param.b32 	%r2, [vector_axpy_param_0];
	ld.param.b64 	%rd1, [vector_axpy_param_1];
	ld.param.b64 	%rd2, [vector_axpy_param_2];
	ld.param.b32 	%r3, [vector_axpy_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB3_2;
	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd3, %rd5;
	ld.global.nc.b32 	%r7, [%rd6];
	add.s64 	%rd7, %rd4, %rd5;
	ld.global.b32 	%r8, [%rd7];
	fma.rn.ftz.f32 	%r9, %r2, %r7, %r8;
	st.global.b32 	[%rd7], %r9;
$L__BB3_2:
	ret;

}
	// .globl	deflate_matrix
.visible .entry deflate_matrix(
	.param .u64 .ptr .align 1 deflate_matrix_param_0,
	.param .u64 .ptr .align 1 deflate_matrix_param_1,
	.param .f32 deflate_matrix_param_2,
	.param .u32 deflate_matrix_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<11>;

	ld.param.b64 	%rd1, [deflate_matrix_param_0];
	ld.param.b64 	%rd2, [deflate_matrix_param_1];
	ld.param.b32 	%r3, [deflate_matrix_param_2];
	ld.param.b32 	%r5, [deflate_matrix_param_3];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %ntid.x;
	mad.lo.s32 	%r9, %r7, %r8, %r6;
	max.s32 	%r10, %r1, %r9;
	setp.ge.s32 	%p1, %r10, %r5;
	@%p1 bra 	$L__BB4_2;
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.u32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd3, %rd5;
	ld.global.nc.b32 	%r11, [%rd6];
	mul.ftz.f32 	%r12, %r3, %r11;
	mul.wide.u32 	%rd7, %r9, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.nc.b32 	%r13, [%rd8];
	mad.lo.s32 	%r14, %r5, %r1, %r9;
	mul.wide.u32 	%rd9, %r14, 4;
	add.s64 	%rd10, %rd4, %rd9;
	neg.ftz.f32 	%r15, %r13;
	mul.ftz.f32 	%r16, %r12, %r15;
	atom.global.add.f32 	%r17, [%rd10], %r16;
$L__BB4_2:
	ret;

}
	// .globl	compute_residue_mobility
.visible .entry compute_residue_mobility(
	.param .u64 .ptr .align 1 compute_residue_mobility_param_0,
	.param .u64 .ptr .align 1 compute_residue_mobility_param_1,
	.param .u64 .ptr .align 1 compute_residue_mobility_param_2,
	.param .u32 compute_residue_mobility_param_3,
	.param .u32 compute_residue_mobility_param_4,
	.param .u32 compute_residue_mobility_param_5
)
{
	.reg .pred 	%p<26>;
	.reg .b32 	%r<253>;
	.reg .b64 	%rd<79>;

	ld.param.b64 	%rd9, [compute_residue_mobility_param_0];
	ld.param.b64 	%rd10, [compute_residue_mobility_param_1];
	ld.param.b64 	%rd8, [compute_residue_mobility_param_2];
	ld.param.b32 	%r93, [compute_residue_mobility_param_3];
	ld.param.b32 	%r91, [compute_residue_mobility_param_4];
	ld.param.b32 	%r92, [compute_residue_mobility_param_5];
	cvta.to.global.u64 	%rd1, %rd9;
	cvta.to.global.u64 	%rd2, %rd10;
	mov.u32 	%r94, %ctaid.x;
	mov.u32 	%r95, %ntid.x;
	mov.u32 	%r96, %tid.x;
	mad.lo.s32 	%r1, %r94, %r95, %r96;
	setp.ge.s32 	%p1, %r1, %r93;
	@%p1 bra 	$L__BB5_43;
	setp.lt.s32 	%p2, %r91, 1;
	mov.b32 	%r229, 0f00000000;
	@%p2 bra 	$L__BB5_42;
	mul.lo.s32 	%r2, %r1, 3;
	and.b32 	%r3, %r91, 7;
	setp.lt.u32 	%p3, %r91, 8;
	mov.b32 	%r229, 0f00000000;
	mov.b32 	%r245, 0;
	@%p3 bra 	$L__BB5_21;
	and.b32 	%r245, %r91, 2147483640;
	shl.b32 	%r227, %r92, 1;
	shl.b32 	%r6, %r92, 3;
	mul.lo.s32 	%r226, %r92, 3;
	shl.b32 	%r225, %r92, 2;
	mul.lo.s32 	%r224, %r92, 5;
	mul.lo.s32 	%r223, %r92, 6;
	mul.lo.s32 	%r222, %r92, 7;
	neg.s32 	%r221, %r245;
	add.s64 	%rd78, %rd2, 16;
	mov.b32 	%r229, 0f00000000;
	mov.b32 	%r219, 0;
	mul.wide.s32 	%rd13, %r2, 4;
	mov.b32 	%r220, %r92;
$L__BB5_4:
	.pragma "nounroll";
	ld.global.nc.b32 	%r23, [%rd78+-16];
	setp.le.ftz.f32 	%p4, %r23, 0f00000000;
	@%p4 bra 	$L__BB5_6;
	mul.wide.s32 	%rd11, %r219, 4;
	add.s64 	%rd12, %rd1, %rd11;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.nc.b32 	%r103, [%rd14];
	ld.global.nc.b32 	%r104, [%rd14+4];
	ld.global.nc.b32 	%r105, [%rd14+8];
	mul.ftz.f32 	%r106, %r104, %r104;
	fma.rn.ftz.f32 	%r107, %r103, %r103, %r106;
	fma.rn.ftz.f32 	%r108, %r105, %r105, %r107;
	div.approx.ftz.f32 	%r109, %r108, %r23;
	add.ftz.f32 	%r229, %r229, %r109;
$L__BB5_6:
	ld.global.nc.b32 	%r26, [%rd78+-12];
	setp.le.ftz.f32 	%p5, %r26, 0f00000000;
	@%p5 bra 	$L__BB5_8;
	mul.wide.s32 	%rd15, %r220, 4;
	add.s64 	%rd16, %rd1, %rd15;
	add.s64 	%rd18, %rd16, %rd13;
	ld.global.nc.b32 	%r110, [%rd18];
	ld.global.nc.b32 	%r111, [%rd18+4];
	ld.global.nc.b32 	%r112, [%rd18+8];
	mul.ftz.f32 	%r113, %r111, %r111;
	fma.rn.ftz.f32 	%r114, %r110, %r110, %r113;
	fma.rn.ftz.f32 	%r115, %r112, %r112, %r114;
	div.approx.ftz.f32 	%r116, %r115, %r26;
	add.ftz.f32 	%r229, %r229, %r116;
$L__BB5_8:
	ld.global.nc.b32 	%r29, [%rd78+-8];
	setp.le.ftz.f32 	%p6, %r29, 0f00000000;
	@%p6 bra 	$L__BB5_10;
	mul.wide.s32 	%rd19, %r227, 4;
	add.s64 	%rd20, %rd1, %rd19;
	add.s64 	%rd22, %rd20, %rd13;
	ld.global.nc.b32 	%r117, [%rd22];
	ld.global.nc.b32 	%r118, [%rd22+4];
	ld.global.nc.b32 	%r119, [%rd22+8];
	mul.ftz.f32 	%r120, %r118, %r118;
	fma.rn.ftz.f32 	%r121, %r117, %r117, %r120;
	fma.rn.ftz.f32 	%r122, %r119, %r119, %r121;
	div.approx.ftz.f32 	%r123, %r122, %r29;
	add.ftz.f32 	%r229, %r229, %r123;
$L__BB5_10:
	ld.global.nc.b32 	%r32, [%rd78+-4];
	setp.le.ftz.f32 	%p7, %r32, 0f00000000;
	@%p7 bra 	$L__BB5_12;
	mul.wide.s32 	%rd23, %r226, 4;
	add.s64 	%rd24, %rd1, %rd23;
	mul.wide.s32 	%rd25, %r2, 4;
	add.s64 	%rd26, %rd24, %rd25;
	ld.global.nc.b32 	%r124, [%rd26];
	ld.global.nc.b32 	%r125, [%rd26+4];
	ld.global.nc.b32 	%r126, [%rd26+8];
	mul.ftz.f32 	%r127, %r125, %r125;
	fma.rn.ftz.f32 	%r128, %r124, %r124, %r127;
	fma.rn.ftz.f32 	%r129, %r126, %r126, %r128;
	div.approx.ftz.f32 	%r130, %r129, %r32;
	add.ftz.f32 	%r229, %r229, %r130;
$L__BB5_12:
	ld.global.nc.b32 	%r35, [%rd78];
	setp.le.ftz.f32 	%p8, %r35, 0f00000000;
	@%p8 bra 	$L__BB5_14;
	mul.wide.s32 	%rd27, %r225, 4;
	add.s64 	%rd28, %rd1, %rd27;
	mul.wide.s32 	%rd29, %r2, 4;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.b32 	%r131, [%rd30];
	ld.global.nc.b32 	%r132, [%rd30+4];
	ld.global.nc.b32 	%r133, [%rd30+8];
	mul.ftz.f32 	%r134, %r132, %r132;
	fma.rn.ftz.f32 	%r135, %r131, %r131, %r134;
	fma.rn.ftz.f32 	%r136, %r133, %r133, %r135;
	div.approx.ftz.f32 	%r137, %r136, %r35;
	add.ftz.f32 	%r229, %r229, %r137;
$L__BB5_14:
	ld.global.nc.b32 	%r38, [%rd78+4];
	setp.le.ftz.f32 	%p9, %r38, 0f00000000;
	@%p9 bra 	$L__BB5_16;
	mul.wide.s32 	%rd31, %r224, 4;
	add.s64 	%rd32, %rd1, %rd31;
	mul.wide.s32 	%rd33, %r2, 4;
	add.s64 	%rd34, %rd32, %rd33;
	ld.global.nc.b32 	%r138, [%rd34];
	ld.global.nc.b32 	%r139, [%rd34+4];
	ld.global.nc.b32 	%r140, [%rd34+8];
	mul.ftz.f32 	%r141, %r139, %r139;
	fma.rn.ftz.f32 	%r142, %r138, %r138, %r141;
	fma.rn.ftz.f32 	%r143, %r140, %r140, %r142;
	div.approx.ftz.f32 	%r144, %r143, %r38;
	add.ftz.f32 	%r229, %r229, %r144;
$L__BB5_16:
	ld.global.nc.b32 	%r41, [%rd78+8];
	setp.le.ftz.f32 	%p10, %r41, 0f00000000;
	@%p10 bra 	$L__BB5_18;
	mul.wide.s32 	%rd35, %r223, 4;
	add.s64 	%rd36, %rd1, %rd35;
	mul.wide.s32 	%rd37, %r2, 4;
	add.s64 	%rd38, %rd36, %rd37;
	ld.global.nc.b32 	%r145, [%rd38];
	ld.global.nc.b32 	%r146, [%rd38+4];
	ld.global.nc.b32 	%r147, [%rd38+8];
	mul.ftz.f32 	%r148, %r146, %r146;
	fma.rn.ftz.f32 	%r149, %r145, %r145, %r148;
	fma.rn.ftz.f32 	%r150, %r147, %r147, %r149;
	div.approx.ftz.f32 	%r151, %r150, %r41;
	add.ftz.f32 	%r229, %r229, %r151;
$L__BB5_18:
	ld.global.nc.b32 	%r44, [%rd78+12];
	setp.le.ftz.f32 	%p11, %r44, 0f00000000;
	@%p11 bra 	$L__BB5_20;
	mul.wide.s32 	%rd39, %r222, 4;
	add.s64 	%rd40, %rd1, %rd39;
	mul.wide.s32 	%rd41, %r2, 4;
	add.s64 	%rd42, %rd40, %rd41;
	ld.global.nc.b32 	%r152, [%rd42];
	ld.global.nc.b32 	%r153, [%rd42+4];
	ld.global.nc.b32 	%r154, [%rd42+8];
	mul.ftz.f32 	%r155, %r153, %r153;
	fma.rn.ftz.f32 	%r156, %r152, %r152, %r155;
	fma.rn.ftz.f32 	%r157, %r154, %r154, %r156;
	div.approx.ftz.f32 	%r158, %r157, %r44;
	add.ftz.f32 	%r229, %r229, %r158;
$L__BB5_20:
	add.s32 	%r227, %r227, %r6;
	add.s32 	%r226, %r226, %r6;
	add.s32 	%r225, %r225, %r6;
	add.s32 	%r224, %r224, %r6;
	add.s32 	%r223, %r223, %r6;
	add.s32 	%r222, %r222, %r6;
	add.s32 	%r221, %r221, 8;
	add.s32 	%r220, %r220, %r6;
	add.s32 	%r219, %r219, %r6;
	add.s64 	%rd78, %rd78, 32;
	setp.ne.s32 	%p12, %r221, 0;
	@%p12 bra 	$L__BB5_4;
$L__BB5_21:
	setp.eq.s32 	%p13, %r3, 0;
	@%p13 bra 	$L__BB5_42;
	and.b32 	%r59, %r91, 3;
	setp.lt.u32 	%p14, %r3, 4;
	@%p14 bra 	$L__BB5_32;
	mul.wide.u32 	%rd43, %r245, 4;
	add.s64 	%rd6, %rd2, %rd43;
	ld.global.nc.b32 	%r60, [%rd6];
	setp.le.ftz.f32 	%p15, %r60, 0f00000000;
	@%p15 bra 	$L__BB5_25;
	mul.lo.s32 	%r160, %r245, %r92;
	mul.wide.s32 	%rd44, %r160, 4;
	add.s64 	%rd45, %rd1, %rd44;
	mul.wide.s32 	%rd46, %r2, 4;
	add.s64 	%rd47, %rd45, %rd46;
	ld.global.nc.b32 	%r161, [%rd47];
	ld.global.nc.b32 	%r162, [%rd47+4];
	ld.global.nc.b32 	%r163, [%rd47+8];
	mul.ftz.f32 	%r164, %r162, %r162;
	fma.rn.ftz.f32 	%r165, %r161, %r161, %r164;
	fma.rn.ftz.f32 	%r166, %r163, %r163, %r165;
	div.approx.ftz.f32 	%r167, %r166, %r60;
	add.ftz.f32 	%r229, %r229, %r167;
$L__BB5_25:
	add.s32 	%r63, %r245, 1;
	ld.global.nc.b32 	%r64, [%rd6+4];
	setp.le.ftz.f32 	%p16, %r64, 0f00000000;
	@%p16 bra 	$L__BB5_27;
	mul.lo.s32 	%r168, %r63, %r92;
	mul.wide.s32 	%rd48, %r168, 4;
	add.s64 	%rd49, %rd1, %rd48;
	mul.wide.s32 	%rd50, %r2, 4;
	add.s64 	%rd51, %rd49, %rd50;
	ld.global.nc.b32 	%r169, [%rd51];
	ld.global.nc.b32 	%r170, [%rd51+4];
	ld.global.nc.b32 	%r171, [%rd51+8];
	mul.ftz.f32 	%r172, %r170, %r170;
	fma.rn.ftz.f32 	%r173, %r169, %r169, %r172;
	fma.rn.ftz.f32 	%r174, %r171, %r171, %r173;
	div.approx.ftz.f32 	%r175, %r174, %r64;
	add.ftz.f32 	%r229, %r229, %r175;
$L__BB5_27:
	ld.global.nc.b32 	%r67, [%rd6+8];
	setp.le.ftz.f32 	%p17, %r67, 0f00000000;
	@%p17 bra 	$L__BB5_29;
	mad.lo.s32 	%r176, %r92, %r63, %r92;
	mul.wide.s32 	%rd52, %r176, 4;
	add.s64 	%rd53, %rd1, %rd52;
	mul.wide.s32 	%rd54, %r2, 4;
	add.s64 	%rd55, %rd53, %rd54;
	ld.global.nc.b32 	%r177, [%rd55];
	ld.global.nc.b32 	%r178, [%rd55+4];
	ld.global.nc.b32 	%r179, [%rd55+8];
	mul.ftz.f32 	%r180, %r178, %r178;
	fma.rn.ftz.f32 	%r181, %r177, %r177, %r180;
	fma.rn.ftz.f32 	%r182, %r179, %r179, %r181;
	div.approx.ftz.f32 	%r183, %r182, %r67;
	add.ftz.f32 	%r229, %r229, %r183;
$L__BB5_29:
	ld.global.nc.b32 	%r70, [%rd6+12];
	setp.le.ftz.f32 	%p18, %r70, 0f00000000;
	@%p18 bra 	$L__BB5_31;
	add.s32 	%r184, %r63, 2;
	mul.lo.s32 	%r185, %r184, %r92;
	mul.wide.s32 	%rd56, %r185, 4;
	add.s64 	%rd57, %rd1, %rd56;
	mul.wide.s32 	%rd58, %r2, 4;
	add.s64 	%rd59, %rd57, %rd58;
	ld.global.nc.b32 	%r186, [%rd59];
	ld.global.nc.b32 	%r187, [%rd59+4];
	ld.global.nc.b32 	%r188, [%rd59+8];
	mul.ftz.f32 	%r189, %r187, %r187;
	fma.rn.ftz.f32 	%r190, %r186, %r186, %r189;
	fma.rn.ftz.f32 	%r191, %r188, %r188, %r190;
	div.approx.ftz.f32 	%r192, %r191, %r70;
	add.ftz.f32 	%r229, %r229, %r192;
$L__BB5_31:
	add.s32 	%r245, %r63, 3;
$L__BB5_32:
	setp.eq.s32 	%p19, %r59, 0;
	@%p19 bra 	$L__BB5_42;
	setp.eq.s32 	%p20, %r59, 1;
	@%p20 bra 	$L__BB5_39;
	mul.wide.u32 	%rd60, %r245, 4;
	add.s64 	%rd7, %rd2, %rd60;
	ld.global.nc.b32 	%r77, [%rd7];
	setp.le.ftz.f32 	%p21, %r77, 0f00000000;
	@%p21 bra 	$L__BB5_36;
	mul.lo.s32 	%r194, %r245, %r92;
	mul.wide.s32 	%rd61, %r194, 4;
	add.s64 	%rd62, %rd1, %rd61;
	mul.wide.s32 	%rd63, %r2, 4;
	add.s64 	%rd64, %rd62, %rd63;
	ld.global.nc.b32 	%r195, [%rd64];
	ld.global.nc.b32 	%r196, [%rd64+4];
	ld.global.nc.b32 	%r197, [%rd64+8];
	mul.ftz.f32 	%r198, %r196, %r196;
	fma.rn.ftz.f32 	%r199, %r195, %r195, %r198;
	fma.rn.ftz.f32 	%r200, %r197, %r197, %r199;
	div.approx.ftz.f32 	%r201, %r200, %r77;
	add.ftz.f32 	%r229, %r229, %r201;
$L__BB5_36:
	add.s32 	%r80, %r245, 1;
	ld.global.nc.b32 	%r81, [%rd7+4];
	setp.le.ftz.f32 	%p22, %r81, 0f00000000;
	@%p22 bra 	$L__BB5_38;
	mul.lo.s32 	%r202, %r80, %r92;
	mul.wide.s32 	%rd65, %r202, 4;
	add.s64 	%rd66, %rd1, %rd65;
	mul.wide.s32 	%rd67, %r2, 4;
	add.s64 	%rd68, %rd66, %rd67;
	ld.global.nc.b32 	%r203, [%rd68];
	ld.global.nc.b32 	%r204, [%rd68+4];
	ld.global.nc.b32 	%r205, [%rd68+8];
	mul.ftz.f32 	%r206, %r204, %r204;
	fma.rn.ftz.f32 	%r207, %r203, %r203, %r206;
	fma.rn.ftz.f32 	%r208, %r205, %r205, %r207;
	div.approx.ftz.f32 	%r209, %r208, %r81;
	add.ftz.f32 	%r229, %r229, %r209;
$L__BB5_38:
	add.s32 	%r245, %r80, 1;
$L__BB5_39:
	and.b32 	%r210, %r91, 1;
	setp.ne.b32 	%p23, %r210, 0;
	not.pred 	%p24, %p23;
	@%p24 bra 	$L__BB5_42;
	mul.wide.u32 	%rd69, %r245, 4;
	add.s64 	%rd70, %rd2, %rd69;
	ld.global.nc.b32 	%r88, [%rd70];
	setp.le.ftz.f32 	%p25, %r88, 0f00000000;
	@%p25 bra 	$L__BB5_42;
	mul.lo.s32 	%r211, %r245, %r92;
	mul.wide.s32 	%rd71, %r211, 4;
	add.s64 	%rd72, %rd1, %rd71;
	mul.wide.s32 	%rd73, %r2, 4;
	add.s64 	%rd74, %rd72, %rd73;
	ld.global.nc.b32 	%r212, [%rd74];
	ld.global.nc.b32 	%r213, [%rd74+4];
	ld.global.nc.b32 	%r214, [%rd74+8];
	mul.ftz.f32 	%r215, %r213, %r213;
	fma.rn.ftz.f32 	%r216, %r212, %r212, %r215;
	fma.rn.ftz.f32 	%r217, %r214, %r214, %r216;
	div.approx.ftz.f32 	%r218, %r217, %r88;
	add.ftz.f32 	%r229, %r229, %r218;
$L__BB5_42:
	cvta.to.global.u64 	%rd75, %rd8;
	mul.wide.s32 	%rd76, %r1, 4;
	add.s64 	%rd77, %rd75, %rd76;
	st.global.b32 	[%rd77], %r229;
$L__BB5_43:
	ret;

}
	// .globl	normalize_mobility
.visible .entry normalize_mobility(
	.param .u64 .ptr .align 1 normalize_mobility_param_0,
	.param .u64 .ptr .align 1 normalize_mobility_param_1,
	.param .u32 normalize_mobility_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<7>;

	ld.param.b64 	%rd1, [normalize_mobility_param_0];
	ld.param.b64 	%rd2, [normalize_mobility_param_1];
	ld.param.b32 	%r3, [normalize_mobility_param_2];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB6_3;
	cvta.to.global.u64 	%rd3, %rd2;
	ld.global.nc.b32 	%r2, [%rd3];
	setp.leu.ftz.f32 	%p2, %r2, 0f00000000;
	@%p2 bra 	$L__BB6_3;
	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.b32 	%r7, [%rd6];
	div.approx.ftz.f32 	%r8, %r7, %r2;
	st.global.b32 	[%rd6], %r8;
$L__BB6_3:
	ret;

}
	// .globl	find_max
.visible .entry find_max(
	.param .u64 .ptr .align 1 find_max_param_0,
	.param .u64 .ptr .align 1 find_max_param_1,
	.param .u32 find_max_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<7>;
	// demoted variable
	.shared .align 4 .b8 _ZZ8find_maxE5s_max[1024];
	ld.param.b64 	%rd2, [find_max_param_0];
	ld.param.b64 	%rd3, [find_max_param_1];
	ld.param.b32 	%r13, [find_max_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r31, %ntid.x;
	mad.lo.s32 	%r28, %r15, %r31, %r1;
	setp.ge.s32 	%p1, %r28, %r13;
	mov.b32 	%r30, 0fF149F2CA;
	@%p1 bra 	$L__BB7_3;
	mov.u32 	%r17, %nctaid.x;
	mul.lo.s32 	%r4, %r31, %r17;
	cvta.to.global.u64 	%rd1, %rd2;
	mov.b32 	%r30, 0fF149F2CA;
$L__BB7_2:
	mul.wide.s32 	%rd4, %r28, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.nc.b32 	%r18, [%rd5];
	max.ftz.f32 	%r30, %r30, %r18;
	add.s32 	%r28, %r28, %r4;
	setp.lt.s32 	%p2, %r28, %r13;
	@%p2 bra 	$L__BB7_2;
$L__BB7_3:
	shl.b32 	%r19, %r1, 2;
	mov.b32 	%r20, _ZZ8find_maxE5s_max;
	add.s32 	%r10, %r20, %r19;
	st.shared.b32 	[%r10], %r30;
	bar.sync 	0;
	setp.lt.u32 	%p3, %r31, 2;
	@%p3 bra 	$L__BB7_7;
$L__BB7_4:
	shr.u32 	%r12, %r31, 1;
	setp.ge.u32 	%p4, %r1, %r12;
	@%p4 bra 	$L__BB7_6;
	ld.shared.b32 	%r21, [%r10];
	shl.b32 	%r22, %r12, 2;
	add.s32 	%r23, %r10, %r22;
	ld.shared.b32 	%r24, [%r23];
	max.ftz.f32 	%r25, %r21, %r24;
	st.shared.b32 	[%r10], %r25;
$L__BB7_6:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r31, 3;
	mov.b32 	%r31, %r12;
	@%p5 bra 	$L__BB7_4;
$L__BB7_7:
	setp.ne.s32 	%p6, %r1, 0;
	@%p6 bra 	$L__BB7_9;
	ld.shared.b32 	%r26, [_ZZ8find_maxE5s_max];
	cvta.to.global.u64 	%rd6, %rd3;
	atom.global.max.s32 	%r27, [%rd6], %r26;
$L__BB7_9:
	ret;

}
	// .globl	init_random_vector
.visible .entry init_random_vector(
	.param .u64 .ptr .align 1 init_random_vector_param_0,
	.param .u32 init_random_vector_param_1,
	.param .u32 init_random_vector_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<5>;

	ld.param.b64 	%rd1, [init_random_vector_param_0];
	ld.param.b32 	%r3, [init_random_vector_param_1];
	ld.param.b32 	%r2, [init_random_vector_param_2];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB8_2;
	cvta.to.global.u64 	%rd2, %rd1;
	mad.lo.s32 	%r7, %r1, 1103515245, %r2;
	mad.lo.s32 	%r8, %r7, 1103515245, 1406932606;
	and.b32 	%r9, %r8, 2147483647;
	cvt.rn.f32.u32 	%r10, %r9;
	mov.b32 	%r11, 0f4F000000;
	div.approx.ftz.f32 	%r12, %r10, %r11;
	add.ftz.f32 	%r13, %r12, 0fBF000000;
	mul.wide.s32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.b32 	[%rd4], %r13;
$L__BB8_2:
	ret;

}
	// .globl	power_iteration_step
.visible .entry power_iteration_step(
	.param .u64 .ptr .align 1 power_iteration_step_param_0,
	.param .u64 .ptr .align 1 power_iteration_step_param_1,
	.param .u64 .ptr .align 1 power_iteration_step_param_2,
	.param .u64 .ptr .align 1 power_iteration_step_param_3,
	.param .u64 .ptr .align 1 power_iteration_step_param_4,
	.param .u32 power_iteration_step_param_5,
	.param .f32 power_iteration_step_param_6,
	.param .f32 power_iteration_step_param_7
)
{


	ret;

}
	// .globl	rayleigh_quotient
.visible .entry rayleigh_quotient(
	.param .u64 .ptr .align 1 rayleigh_quotient_param_0,
	.param .u64 .ptr .align 1 rayleigh_quotient_param_1,
	.param .u64 .ptr .align 1 rayleigh_quotient_param_2,
	.param .u64 .ptr .align 1 rayleigh_quotient_param_3,
	.param .u32 rayleigh_quotient_param_4
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<46>;
	.reg .b64 	%rd<12>;
	// demoted variable
	.shared .align 4 .b8 _ZZ17rayleigh_quotientE5s_vAv[1024];
	// demoted variable
	.shared .align 4 .b8 _ZZ17rayleigh_quotientE4s_vv[1024];
	ld.param.b64 	%rd3, [rayleigh_quotient_param_0];
	ld.param.b64 	%rd4, [rayleigh_quotient_param_1];
	ld.param.b64 	%rd5, [rayleigh_quotient_param_2];
	ld.param.b64 	%rd6, [rayleigh_quotient_param_3];
	ld.param.b32 	%r17, [rayleigh_quotient_param_4];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r19, %ctaid.x;
	mov.u32 	%r45, %ntid.x;
	mad.lo.s32 	%r40, %r19, %r45, %r1;
	setp.ge.s32 	%p1, %r40, %r17;
	mov.b32 	%r43, 0f00000000;
	mov.b32 	%r44, %r43;
	@%p1 bra 	$L__BB10_3;
	mov.u32 	%r21, %nctaid.x;
	mul.lo.s32 	%r4, %r45, %r21;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
	mov.b32 	%r44, 0f00000000;
	mov.b32 	%r43, %r44;
$L__BB10_2:
	mul.wide.s32 	%rd7, %r40, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.b32 	%r22, [%rd8];
	add.s64 	%rd9, %rd2, %rd7;
	ld.global.nc.b32 	%r23, [%rd9];
	fma.rn.ftz.f32 	%r43, %r22, %r23, %r43;
	fma.rn.ftz.f32 	%r44, %r22, %r22, %r44;
	add.s32 	%r40, %r40, %r4;
	setp.lt.s32 	%p2, %r40, %r17;
	@%p2 bra 	$L__BB10_2;
$L__BB10_3:
	shl.b32 	%r24, %r1, 2;
	mov.b32 	%r25, _ZZ17rayleigh_quotientE5s_vAv;
	add.s32 	%r13, %r25, %r24;
	st.shared.b32 	[%r13], %r43;
	mov.b32 	%r26, _ZZ17rayleigh_quotientE4s_vv;
	add.s32 	%r14, %r26, %r24;
	st.shared.b32 	[%r14], %r44;
	bar.sync 	0;
	setp.lt.u32 	%p3, %r45, 2;
	@%p3 bra 	$L__BB10_7;
$L__BB10_4:
	shr.u32 	%r16, %r45, 1;
	setp.ge.u32 	%p4, %r1, %r16;
	@%p4 bra 	$L__BB10_6;
	shl.b32 	%r27, %r16, 2;
	add.s32 	%r28, %r13, %r27;
	ld.shared.b32 	%r29, [%r28];
	ld.shared.b32 	%r30, [%r13];
	add.ftz.f32 	%r31, %r29, %r30;
	st.shared.b32 	[%r13], %r31;
	add.s32 	%r32, %r14, %r27;
	ld.shared.b32 	%r33, [%r32];
	ld.shared.b32 	%r34, [%r14];
	add.ftz.f32 	%r35, %r33, %r34;
	st.shared.b32 	[%r14], %r35;
$L__BB10_6:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r45, 3;
	mov.b32 	%r45, %r16;
	@%p5 bra 	$L__BB10_4;
$L__BB10_7:
	setp.ne.s32 	%p6, %r1, 0;
	@%p6 bra 	$L__BB10_9;
	ld.shared.b32 	%r36, [_ZZ17rayleigh_quotientE5s_vAv];
	cvta.to.global.u64 	%rd10, %rd5;
	atom.global.add.f32 	%r37, [%rd10], %r36;
	ld.shared.b32 	%r38, [_ZZ17rayleigh_quotientE4s_vv];
	cvta.to.global.u64 	%rd11, %rd6;
	atom.global.add.f32 	%r39, [%rd11], %r38;
$L__BB10_9:
	ret;

}
	// .globl	copy_and_normalize
.visible .entry copy_and_normalize(
	.param .u64 .ptr .align 1 copy_and_normalize_param_0,
	.param .u64 .ptr .align 1 copy_and_normalize_param_1,
	.param .u64 .ptr .align 1 copy_and_normalize_param_2,
	.param .u32 copy_and_normalize_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<10>;

	ld.param.b64 	%rd1, [copy_and_normalize_param_0];
	ld.param.b64 	%rd2, [copy_and_normalize_param_1];
	ld.param.b64 	%rd3, [copy_and_normalize_param_2];
	ld.param.b32 	%r3, [copy_and_normalize_param_3];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.b32 	%r4, [%rd4];
	sqrt.approx.ftz.f32 	%r1, %r4;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r7;
	setp.ge.s32 	%p1, %r2, %r3;
	@%p1 bra 	$L__BB11_2;
	cvta.to.global.u64 	%rd5, %rd1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd5, %rd7;
	ld.global.nc.b32 	%r8, [%rd8];
	setp.lt.ftz.f32 	%p2, %r1, 0f2EDBE6FF;
	selp.f32 	%r9, 0f3F800000, %r1, %p2;
	div.approx.ftz.f32 	%r10, %r8, %r9;
	add.s64 	%rd9, %rd6, %rd7;
	st.global.b32 	[%rd9], %r10;
$L__BB11_2:
	ret;

}
