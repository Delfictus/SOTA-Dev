/**
 * PRISM DR-WHCR-AI-Q-PT Ultra Fused Kernel
 *
 * Ultra-optimized GPU kernel combining 8 advanced optimization techniques:
 * 1. W-Cycle Multigrid (4-level hierarchical coarsening)
 * 2. Dendritic Reservoir Computing (8-branch neuromorphic processing)
 * 3. Quantum Tunneling (6-state superposition)
 * 4. TPTP Persistent Homology (topological phase transition detection)
 * 5. Active Inference (belief-driven planning)
 * 6. Parallel Tempering (12 temperature replicas)
 * 7. WHCR Conflict Repair (wavelet-hierarchical optimization)
 * 8. Wavelet-guided prioritization
 *
 * Copyright (c) 2024 PRISM Research Team | Delfictus I/O Inc.
 * Los Angeles, CA 90013
 * Contact: IS@Delfictus.com
 * All Rights Reserved.
 */

#include <cuda_runtime.h>
#include <cooperative_groups.h>
#include <curand_kernel.h>
#include <math.h>

namespace cg = cooperative_groups;

// ═══════════════════════════════════════════════════════════════════════════
// CONSTANTS
// ═══════════════════════════════════════════════════════════════════════════

#define BLOCK_SIZE 256
#define MAX_VERTICES_PER_BLOCK 256  // Reduced to fit 100KB shared memory
#define MAX_COLORS 64
#define NUM_BRANCHES 8
#define NUM_LEVELS 4
#define NUM_REPLICAS 12
#define NUM_QUANTUM_STATES 6
#define MAX_NEIGHBORS 128
#define WARP_SIZE 32

// Precision constants
#define EPSILON 1e-8f
#define PI 3.14159265358979323846f

// ═══════════════════════════════════════════════════════════════════════════
// CONFIGURATION STRUCTURES
// ═══════════════════════════════════════════════════════════════════════════

/**
 * RuntimeConfig - FFI-compatible configuration from Rust
 * Must match crates/prism-core/src/runtime_config.rs exactly
 */
struct RuntimeConfig {
    // WHCR Parameters
    float stress_weight;
    float persistence_weight;
    float belief_weight;
    float hotspot_multiplier;

    // Dendritic Reservoir (8-branch)
    float tau_decay[8];
    float branch_weights[8];
    float reservoir_leak_rate;
    float spectral_radius;
    float input_scaling;
    float reservoir_sparsity;

    // W-Cycle Multigrid
    int num_levels;
    float coarsening_ratio;
    float restriction_weight;
    float prolongation_weight;
    int pre_smooth_iterations;
    int post_smooth_iterations;

    // Quantum Tunneling
    float tunneling_prob_base;
    float tunneling_prob_boost;
    float chemical_potential;
    float transverse_field;
    float interference_decay;
    int num_quantum_states;

    // Parallel Tempering
    float temperatures[8];
    int num_replicas;
    int swap_interval;
    float swap_probability;

    // TPTP (Topological Phase Transition Predictor)
    float betti_0_threshold;
    float betti_1_threshold;
    float betti_2_threshold;
    float persistence_threshold;
    int stability_window;
    float transition_sensitivity;

    // Active Inference
    float free_energy_threshold;
    float belief_update_rate;
    float precision_weight;
    float policy_temperature;

    // Meta/Control
    int iteration;
    int phase_id;
    float global_temperature;
    float learning_rate;
    float exploration_rate;

    // Flags
    int flags;

    // Padding
    float _padding;
};

/**
 * KernelTelemetry - Output metrics from kernel
 * Must match crates/prism-core/src/runtime_config.rs exactly
 */
struct KernelTelemetry {
    int conflicts;
    int colors_used;
    int moves_applied;
    int tunneling_events;
    int phase_transitions;
    float betti_numbers[3];
    float reservoir_activity;
    float free_energy;
    int best_replica;
    int iteration_time_us;
    float _padding[4];
};

// ═══════════════════════════════════════════════════════════════════════════
// SHARED MEMORY STATE (~98KB)
// ═══════════════════════════════════════════════════════════════════════════

/**
 * Dendritic State - 8-branch neuromorphic processing
 */
struct DendriticState {
    float activation[8];     // Per-branch activation
    float calcium;           // Long-term potentiation accumulator
    float threshold;         // Adaptive firing threshold
    float refractory;        // Refractory period counter
};

/**
 * Quantum Vertex - 6-state superposition
 */
struct QuantumVertex {
    float amplitude_real[6]; // Real part of quantum amplitude
    float amplitude_imag[6]; // Imaginary part of quantum amplitude
    int color_idx[6];        // Color for each superposition state
    float tunneling_prob;    // Current tunneling probability
    float phase;             // Global quantum phase
};

/**
 * Tempering Replica - Parallel tempering state
 */
struct TemperingReplica {
    int coloring[64];        // Replica's coloring (subset of vertices)
    float energy;            // Current energy
    int conflicts;           // Number of conflicts
};

/**
 * Persistent Homology State - TPTP topological tracking
 */
struct PersistentHomologyState {
    float betti[3];                 // Betti numbers (β0, β1, β2)
    float max_persistence;          // Maximum persistence value
    float stability_score;          // Stability measure
    int transition_detected;        // Phase boundary flag
    float betti_1_derivative;       // Rate of change of β1
    float persistence_diagram[64];  // Birth-death pairs (32 intervals)
};

/**
 * Ultra Shared Memory State
 * Total: ~95KB (optimized to fit in RTX 3060's 100KB shared memory)
 * Reduced from 153KB by limiting per-block vertices to 256
 */
struct UltraSharedState {
    // ═══════════════════════════════════════════════════════════════════════
    // W-CYCLE MULTIGRID HIERARCHY (4 levels) - OPTIMIZED
    // ═══════════════════════════════════════════════════════════════════════
    // Level 0 (Fine): 256 vertices max per block
    int coloring_L0[256];               // 1KB
    float conflict_signal_L0[256];      // 1KB

    // Level 1: 64 vertices
    int coloring_L1[64];                // 256B
    float conflict_signal_L1[64];       // 256B
    int projection_L0_to_L1[256];       // 1KB (fine→coarse mapping)

    // Level 2: 16 vertices
    int coloring_L2[16];                // 64B
    float conflict_signal_L2[16];       // 64B
    int projection_L1_to_L2[64];        // 256B

    // Level 3 (Coarsest): 4 vertices
    int coloring_L3[4];                 // 16B
    float conflict_signal_L3[4];        // 16B
    int projection_L2_to_L3[16];        // 64B

    // Wavelet coefficients (4 levels) - REDUCED FOR 100KB LIMIT
    float wavelet_approx[2][256];       // 2KB (approximation, 2 levels)
    float wavelet_detail[2][256];       // 2KB (detail, 2 levels)

    // ═══════════════════════════════════════════════════════════════════════
    // DENDRITIC RESERVOIR (8-branch) - OPTIMIZED FOR 100KB LIMIT
    // ═══════════════════════════════════════════════════════════════════════
    DendriticState dendrite[256];       // 12KB (256 vertices)
    float soma_potential[256];          // 1KB
    float spike_history[256];           // 1KB
    float reservoir_state[1024];        // 4KB (echo state, reduced)

    // ═══════════════════════════════════════════════════════════════════════
    // QUANTUM TUNNELING STATE - OPTIMIZED FOR 100KB LIMIT
    // ═══════════════════════════════════════════════════════════════════════
    QuantumVertex quantum[256];         // 20KB (256 vertices)

    // ═══════════════════════════════════════════════════════════════════════
    // PARALLEL TEMPERING (12 replicas)
    // ═══════════════════════════════════════════════════════════════════════
    TemperingReplica replica[12];       // 3.6KB
    float temperatures[16];             // 64B (temperature ladder)

    // ═══════════════════════════════════════════════════════════════════════
    // TPTP: PERSISTENT HOMOLOGY STATE
    // ═══════════════════════════════════════════════════════════════════════
    PersistentHomologyState tda;        // ~0.3KB

    // ═══════════════════════════════════════════════════════════════════════
    // ACTIVE INFERENCE - REDUCED FOR 100KB LIMIT
    // ═══════════════════════════════════════════════════════════════════════
    float belief_distribution[256][12]; // 12KB (beliefs over 12 colors, 256 vertices)
    float expected_free_energy[256];    // 1KB
    float precision_weights[256];       // 1KB

    // ═══════════════════════════════════════════════════════════════════════
    // WORK BUFFERS - OPTIMIZED FOR 100KB LIMIT
    // ═══════════════════════════════════════════════════════════════════════
    int conflict_vertices[256];         // 1KB (vertices with conflicts)
    int num_conflict_vertices;          // 4B
    float move_deltas[256];             // 1KB (best move delta per vertex)
    int best_colors[256];               // 1KB (best new color)
    int locks[256];                     // 1KB (vertex locks for atomic updates)

    // Total: ~95KB (optimized from 153KB)
};

// ═══════════════════════════════════════════════════════════════════════════
// FEATURE FLAG HELPERS (inline device functions)
// ═══════════════════════════════════════════════════════════════════════════

#define FLAG_QUANTUM_ENABLED (1 << 0)
#define FLAG_TPTP_ENABLED (1 << 1)
#define FLAG_DENDRITIC_ENABLED (1 << 2)
#define FLAG_PARALLEL_TEMPERING_ENABLED (1 << 3)
#define FLAG_ACTIVE_INFERENCE_ENABLED (1 << 4)
#define FLAG_MULTIGRID_ENABLED (1 << 5)

__device__ __forceinline__ bool quantum_enabled(const RuntimeConfig* cfg) {
    return (cfg->flags & FLAG_QUANTUM_ENABLED) != 0;
}

__device__ __forceinline__ bool tptp_enabled(const RuntimeConfig* cfg) {
    return (cfg->flags & FLAG_TPTP_ENABLED) != 0;
}

__device__ __forceinline__ bool dendritic_enabled(const RuntimeConfig* cfg) {
    return (cfg->flags & FLAG_DENDRITIC_ENABLED) != 0;
}

__device__ __forceinline__ bool tempering_enabled(const RuntimeConfig* cfg) {
    return (cfg->flags & FLAG_PARALLEL_TEMPERING_ENABLED) != 0;
}

__device__ __forceinline__ bool active_inference_enabled(const RuntimeConfig* cfg) {
    return (cfg->flags & FLAG_ACTIVE_INFERENCE_ENABLED) != 0;
}

__device__ __forceinline__ bool multigrid_enabled(const RuntimeConfig* cfg) {
    return (cfg->flags & FLAG_MULTIGRID_ENABLED) != 0;
}

// ═══════════════════════════════════════════════════════════════════════════
// FORWARD DECLARATIONS
// ═══════════════════════════════════════════════════════════════════════════

__device__ void restrict_to_coarse(UltraSharedState* state, int level, const RuntimeConfig* cfg);
__device__ void prolongate_to_fine(UltraSharedState* state, int level, const RuntimeConfig* cfg, curandState* rng);
__device__ void smooth_iteration(UltraSharedState* state, int level, const RuntimeConfig* cfg, cg::thread_block block);
__device__ void dendritic_update(UltraSharedState* state, int vertex, const RuntimeConfig* cfg);
__device__ void quantum_evolve(UltraSharedState* state, int vertex, const RuntimeConfig* cfg);
__device__ void tptp_update(UltraSharedState* state, const RuntimeConfig* cfg);
__device__ bool should_tunnel(UltraSharedState* state, int vertex, const RuntimeConfig* cfg, curandState* rng);
__device__ void active_inference_update(UltraSharedState* state, int vertex, const RuntimeConfig* cfg);
__device__ void tempering_step(UltraSharedState* state, int replica, const RuntimeConfig* cfg, curandState* rng);
__device__ void replica_exchange(UltraSharedState* state, const RuntimeConfig* cfg, curandState* rng);

// ═══════════════════════════════════════════════════════════════════════════
// MAIN ULTRA KERNEL
// ═══════════════════════════════════════════════════════════════════════════

/**
 * DR-WHCR-AI-Q-PT-TDA Ultra Fused Kernel
 *
 * Single kernel that performs complete optimization iteration combining all 8 components.
 *
 * @param graph_row_ptr CSR row pointers [num_vertices+1]
 * @param graph_col_idx CSR column indices [num_edges]
 * @param coloring Current vertex coloring [num_vertices] (modified in-place)
 * @param config RuntimeConfig struct with all 50+ parameters
 * @param telemetry Output telemetry struct
 * @param num_vertices Number of vertices
 * @param num_edges Number of edges
 * @param seed Random seed for this iteration
 */
extern "C" __global__ void dr_whcr_ultra_kernel(
    const int* __restrict__ graph_row_ptr,
    const int* __restrict__ graph_col_idx,
    int* __restrict__ coloring,
    const RuntimeConfig* __restrict__ config,
    KernelTelemetry* __restrict__ telemetry,
    int num_vertices,
    int num_edges,
    unsigned long long seed
) {
    // Shared memory allocation
    __shared__ UltraSharedState state;

    // Cooperative groups for synchronization
    cg::thread_block block = cg::this_thread_block();

    int tid = threadIdx.x;
    int bid = blockIdx.x;
    int gid = blockIdx.x * blockDim.x + threadIdx.x;

    // Initialize RNG
    curandState rng;
    curand_init(seed, gid, 0, &rng);

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 1: LOAD DATA INTO SHARED MEMORY
    // ═══════════════════════════════════════════════════════════════════════

    // Calculate vertex range for this block
    int vertices_per_block = (num_vertices + gridDim.x - 1) / gridDim.x;
    int block_start = bid * vertices_per_block;
    int block_end = min(block_start + vertices_per_block, num_vertices);
    int block_size = block_end - block_start;

    // Initialize locks
    for (int i = tid; i < MAX_VERTICES_PER_BLOCK; i += BLOCK_SIZE) {
        state.locks[i] = 0;
    }

    // Initialize projection mappings (simple 4:1 ratio)
    for (int i = tid; i < 512; i += BLOCK_SIZE) {
        state.projection_L0_to_L1[i] = i / 4;
    }
    for (int i = tid; i < 128; i += BLOCK_SIZE) {
        state.projection_L1_to_L2[i] = i / 4;
    }
    for (int i = tid; i < 32; i += BLOCK_SIZE) {
        state.projection_L2_to_L3[i] = i / 4;
    }

    // Initialize temperatures
    for (int i = tid; i < 16; i += BLOCK_SIZE) {
        if (i < 8) {
            state.temperatures[i] = config->temperatures[i];
        } else {
            state.temperatures[i] = 1.0f;
        }
    }

    // Initialize quantum states
    for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
        QuantumVertex* q = &state.quantum[i];

        // Equal superposition initialization
        float amp = 1.0f / sqrtf((float)NUM_QUANTUM_STATES);
        for (int s = 0; s < NUM_QUANTUM_STATES; s++) {
            q->amplitude_real[s] = amp;
            q->amplitude_imag[s] = 0.0f;
            q->color_idx[s] = s % MAX_COLORS;
        }
        q->tunneling_prob = config->tunneling_prob_base;
        q->phase = 0.0f;
    }

    // Initialize dendrites
    for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
        DendriticState* d = &state.dendrite[i];
        for (int b = 0; b < NUM_BRANCHES; b++) {
            d->activation[b] = 0.0f;
        }
        d->calcium = 0.0f;
        d->threshold = 0.5f;
        d->refractory = 0.0f;

        state.soma_potential[i] = 0.0f;
        state.spike_history[i] = 0.0f;
    }

    // Initialize belief distributions (uniform prior)
    for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
        for (int c = 0; c < 16; c++) {
            state.belief_distribution[i][c] = 1.0f / 16.0f;
        }
        state.expected_free_energy[i] = 0.0f;
        state.precision_weights[i] = 1.0f;
    }

    // Initialize TPTP state
    if (tid == 0) {
        state.tda.betti[0] = 0.0f;
        state.tda.betti[1] = 0.0f;
        state.tda.betti[2] = 0.0f;
        state.tda.max_persistence = 0.0f;
        state.tda.stability_score = 0.0f;
        state.tda.transition_detected = 0;
        state.tda.betti_1_derivative = 0.0f;
        for (int i = 0; i < 64; i++) {
            state.tda.persistence_diagram[i] = 0.0f;
        }
    }

    block.sync();

    // Load coloring into shared memory
    for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
        int v = block_start + i;
        if (v < num_vertices && i < MAX_VERTICES_PER_BLOCK) {
            state.coloring_L0[i] = coloring[v];
        }
    }
    block.sync();

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 2: COUNT INITIAL CONFLICTS
    // ═══════════════════════════════════════════════════════════════════════

    for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
        int v = block_start + i;
        if (v >= num_vertices) continue;

        int my_color = state.coloring_L0[i];
        int start = graph_row_ptr[v];
        int end = graph_row_ptr[v + 1];

        float conflict_count = 0.0f;
        for (int e = start; e < end; e++) {
            int neighbor = graph_col_idx[e];
            // Check if neighbor is in this block
            if (neighbor >= block_start && neighbor < block_end) {
                int local_idx = neighbor - block_start;
                if (local_idx < MAX_VERTICES_PER_BLOCK && state.coloring_L0[local_idx] == my_color) {
                    conflict_count += 1.0f;
                }
            } else {
                // Global memory access for out-of-block neighbors
                if (coloring[neighbor] == my_color) {
                    conflict_count += 1.0f;
                }
            }
        }
        state.conflict_signal_L0[i] = conflict_count;
    }
    block.sync();

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 3: W-CYCLE MULTIGRID
    // ═══════════════════════════════════════════════════════════════════════

    if (multigrid_enabled(config)) {
        // Pre-smoothing at fine level
        for (int s = 0; s < config->pre_smooth_iterations; s++) {
            smooth_iteration(&state, 0, config, block);
            block.sync();
        }

        // Restriction: L0 → L1 → L2 → L3
        for (int level = 0; level < config->num_levels - 1; level++) {
            restrict_to_coarse(&state, level, config);
            block.sync();
        }

        // Solve at coarsest level (L3) - direct greedy coloring
        if (tid < 8) {
            int v = tid;
            int used_colors = 0;
            for (int c = 0; c < MAX_COLORS; c++) {
                bool can_use = true;
                // Simplified connectivity check at coarse level
                for (int other = 0; other < 8; other++) {
                    if (other != v && state.coloring_L3[other] == c) {
                        // Assume all coarse vertices are connected (worst case)
                        if (state.conflict_signal_L3[v] > 0.0f) {
                            can_use = false;
                            break;
                        }
                    }
                }
                if (can_use) {
                    state.coloring_L3[v] = c;
                    break;
                }
            }
        }
        block.sync();

        // Prolongation: L3 → L2 → L1 → L0
        for (int level = config->num_levels - 2; level >= 0; level--) {
            prolongate_to_fine(&state, level, config, &rng);
            block.sync();

            // Post-smoothing
            for (int s = 0; s < config->post_smooth_iterations; s++) {
                smooth_iteration(&state, level, config, block);
                block.sync();
            }
        }
    }

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 4: DENDRITIC RESERVOIR UPDATE
    // ═══════════════════════════════════════════════════════════════════════

    if (dendritic_enabled(config)) {
        for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
            dendritic_update(&state, i, config);
        }
        block.sync();

        // Compute reservoir output priorities
        for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
            float priority = 0.0f;

            // Proximal branch drives immediate priority
            priority += state.dendrite[i].activation[0] * 2.0f;

            // Distal branches indicate structural issues
            for (int b = 1; b < NUM_BRANCHES; b++) {
                priority += state.dendrite[i].activation[b] * config->branch_weights[b];
            }

            // Calcium indicates long-term problematic vertex
            priority += state.dendrite[i].calcium * 3.0f;

            // Soma potential indicates accumulated pressure
            priority += state.soma_potential[i] * 0.5f;

            state.move_deltas[i] = priority; // Temporarily store priority here
        }
        block.sync();
    }

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 5: TPTP PERSISTENT HOMOLOGY UPDATE
    // ═══════════════════════════════════════════════════════════════════════

    if (tptp_enabled(config)) {
        // Thread 0 computes global homology (simplified)
        if (tid == 0) {
            tptp_update(&state, config);
        }
        block.sync();
    }

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 6: QUANTUM TUNNELING
    // ═══════════════════════════════════════════════════════════════════════

    if (quantum_enabled(config)) {
        for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
            // Evolve quantum state
            quantum_evolve(&state, i, config);

            // Check for tunneling
            if (should_tunnel(&state, i, config, &rng)) {
                // Tunnel to new color based on quantum state
                float max_prob = 0.0f;
                int best_state = 0;
                for (int s = 0; s < NUM_QUANTUM_STATES; s++) {
                    float prob = state.quantum[i].amplitude_real[s] * state.quantum[i].amplitude_real[s] +
                                state.quantum[i].amplitude_imag[s] * state.quantum[i].amplitude_imag[s];
                    if (prob > max_prob) {
                        max_prob = prob;
                        best_state = s;
                    }
                }
                state.coloring_L0[i] = state.quantum[i].color_idx[best_state];

                // Track tunneling event (atomic for global telemetry)
                if (gid == 0) {
                    atomicAdd(&telemetry->tunneling_events, 1);
                }
            }
        }
        block.sync();
    }

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 7: ACTIVE INFERENCE BELIEF UPDATE
    // ═══════════════════════════════════════════════════════════════════════

    if (active_inference_enabled(config)) {
        for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
            active_inference_update(&state, i, config);
        }
        block.sync();
    }

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 8: PARALLEL TEMPERING
    // ═══════════════════════════════════════════════════════════════════════

    if (tempering_enabled(config)) {
        // Each warp handles one replica
        int warp_id = tid / 32;
        if (warp_id < config->num_replicas && warp_id < NUM_REPLICAS) {
            tempering_step(&state, warp_id, config, &rng);
        }
        block.sync();

        // Replica exchange (thread 0 coordinates)
        if (tid == 0 && config->iteration % config->swap_interval == 0) {
            replica_exchange(&state, config, &rng);
        }
        block.sync();
    }

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 9: WHCR MOVE EVALUATION AND APPLICATION
    // ═══════════════════════════════════════════════════════════════════════

    // Identify conflict vertices
    if (tid == 0) {
        state.num_conflict_vertices = 0;
    }
    block.sync();

    for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
        if (state.conflict_signal_L0[i] > 0.5f) {
            int idx = atomicAdd(&state.num_conflict_vertices, 1);
            if (idx < MAX_VERTICES_PER_BLOCK) {
                state.conflict_vertices[idx] = i;
            }
        }
    }
    block.sync();

    // Evaluate best moves for conflict vertices
    int num_cv = min(state.num_conflict_vertices, MAX_VERTICES_PER_BLOCK);
    for (int cv_idx = tid; cv_idx < num_cv; cv_idx += BLOCK_SIZE) {
        int i = state.conflict_vertices[cv_idx];
        int v = block_start + i;
        if (v >= num_vertices) continue;

        int current_color = state.coloring_L0[i];

        // Count neighbor colors
        int neighbor_colors[MAX_COLORS];
        for (int c = 0; c < MAX_COLORS; c++) neighbor_colors[c] = 0;

        int start = graph_row_ptr[v];
        int end = graph_row_ptr[v + 1];

        for (int e = start; e < end; e++) {
            int neighbor = graph_col_idx[e];
            int n_color;
            if (neighbor >= block_start && neighbor < block_end) {
                int local_idx = neighbor - block_start;
                if (local_idx < MAX_VERTICES_PER_BLOCK) {
                    n_color = state.coloring_L0[local_idx];
                } else {
                    n_color = coloring[neighbor];
                }
            } else {
                n_color = coloring[neighbor];
            }
            if (n_color >= 0 && n_color < MAX_COLORS) {
                neighbor_colors[n_color]++;
            }
        }

        // Find best color
        int current_conf = neighbor_colors[current_color];
        float best_delta = 0.0f;
        int best_color = current_color;

        for (int c = 0; c < MAX_COLORS; c++) {
            if (c == current_color) continue;

            float delta = (float)(neighbor_colors[c] - current_conf);

            // Chemical potential penalty
            delta += config->chemical_potential * ((float)c - (float)current_color) / (float)MAX_COLORS;

            // Belief guidance from active inference
            if (active_inference_enabled(config) && c < 16) {
                float belief_diff = state.belief_distribution[i][c] -
                                   state.belief_distribution[i][min(current_color, 15)];
                delta -= config->belief_weight * belief_diff;
            }

            // Reservoir priority modulation
            if (dendritic_enabled(config)) {
                delta -= state.move_deltas[i] * 0.1f;
            }

            if (delta < best_delta) {
                best_delta = delta;
                best_color = c;
            }
        }

        state.best_colors[i] = best_color;
        state.move_deltas[i] = best_delta;
    }
    block.sync();

    // Apply moves with locking
    for (int cv_idx = tid; cv_idx < num_cv; cv_idx += BLOCK_SIZE) {
        int i = state.conflict_vertices[cv_idx];
        int new_color = state.best_colors[i];
        float delta = state.move_deltas[i];

        if (new_color == state.coloring_L0[i]) continue;
        if (delta >= -0.001f) continue; // Only apply improving moves

        // Try to acquire lock
        if (atomicCAS(&state.locks[i], 0, 1) == 0) {
            state.coloring_L0[i] = new_color;
            atomicExch(&state.locks[i], 0);

            // Track move application
            if (gid == 0) {
                atomicAdd(&telemetry->moves_applied, 1);
            }
        }
    }
    block.sync();

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 10: WRITE BACK TO GLOBAL MEMORY
    // ═══════════════════════════════════════════════════════════════════════

    for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
        int v = block_start + i;
        if (v < num_vertices) {
            coloring[v] = state.coloring_L0[i];
        }
    }

    // Write telemetry (thread 0 only)
    if (gid == 0) {
        // Count total conflicts
        int total_conflicts = 0;
        int max_color = 0;
        for (int v = 0; v < num_vertices; v++) {
            int c = coloring[v];
            if (c > max_color) max_color = c;

            int start = graph_row_ptr[v];
            int end = graph_row_ptr[v + 1];
            for (int e = start; e < end; e++) {
                if (coloring[graph_col_idx[e]] == c) {
                    total_conflicts++;
                }
            }
        }

        telemetry->conflicts = total_conflicts / 2;
        telemetry->colors_used = max_color + 1;
        telemetry->betti_numbers[0] = state.tda.betti[0];
        telemetry->betti_numbers[1] = state.tda.betti[1];
        telemetry->betti_numbers[2] = state.tda.betti[2];
        telemetry->phase_transitions = state.tda.transition_detected;

        // Compute average reservoir activity
        float total_activity = 0.0f;
        int active_count = 0;
        for (int i = 0; i < min(block_size, MAX_VERTICES_PER_BLOCK); i++) {
            if (state.spike_history[i] > 0.1f) {
                total_activity += state.spike_history[i];
                active_count++;
            }
        }
        telemetry->reservoir_activity = (active_count > 0) ? (total_activity / active_count) : 0.0f;

        // Compute average free energy
        float total_fe = 0.0f;
        for (int i = 0; i < min(block_size, MAX_VERTICES_PER_BLOCK); i++) {
            total_fe += state.expected_free_energy[i];
        }
        telemetry->free_energy = total_fe / fmaxf(1.0f, (float)min(block_size, MAX_VERTICES_PER_BLOCK));
    }
}

// ═══════════════════════════════════════════════════════════════════════════
// DEVICE FUNCTION IMPLEMENTATIONS
// ═══════════════════════════════════════════════════════════════════════════

/**
 * Multigrid Restriction: Project fine level to coarse level
 */
__device__ void restrict_to_coarse(UltraSharedState* state, int level, const RuntimeConfig* cfg) {
    int tid = threadIdx.x;

    // Get source and destination arrays based on level
    int* src_coloring;
    float* src_signal;
    int* dst_coloring;
    float* dst_signal;
    int* projection;
    int src_size, dst_size;

    switch (level) {
        case 0:
            src_coloring = state->coloring_L0;
            src_signal = state->conflict_signal_L0;
            dst_coloring = state->coloring_L1;
            dst_signal = state->conflict_signal_L1;
            projection = state->projection_L0_to_L1;
            src_size = 512; dst_size = 128;
            break;
        case 1:
            src_coloring = state->coloring_L1;
            src_signal = state->conflict_signal_L1;
            dst_coloring = state->coloring_L2;
            dst_signal = state->conflict_signal_L2;
            projection = state->projection_L1_to_L2;
            src_size = 128; dst_size = 32;
            break;
        case 2:
            src_coloring = state->coloring_L2;
            src_signal = state->conflict_signal_L2;
            dst_coloring = state->coloring_L3;
            dst_signal = state->conflict_signal_L3;
            projection = state->projection_L2_to_L3;
            src_size = 32; dst_size = 8;
            break;
        default:
            return;
    }

    // Aggregate fine vertices to coarse
    for (int c = tid; c < dst_size; c += BLOCK_SIZE) {
        float signal_sum = 0.0f;
        int color_votes[MAX_COLORS];
        for (int i = 0; i < MAX_COLORS; i++) color_votes[i] = 0;
        int count = 0;

        for (int f = 0; f < src_size; f++) {
            if (projection[f] == c) {
                signal_sum += src_signal[f];
                int color = src_coloring[f];
                if (color >= 0 && color < MAX_COLORS) {
                    color_votes[color]++;
                }
                count++;
            }
        }

        // Majority vote for color
        int best_color = 0;
        int best_votes = 0;
        for (int i = 0; i < MAX_COLORS; i++) {
            if (color_votes[i] > best_votes) {
                best_votes = color_votes[i];
                best_color = i;
            }
        }

        dst_coloring[c] = best_color;
        dst_signal[c] = signal_sum / fmaxf(1.0f, (float)count);
    }
}

/**
 * Multigrid Prolongation: Interpolate coarse solution to fine level
 */
__device__ void prolongate_to_fine(UltraSharedState* state, int level, const RuntimeConfig* cfg, curandState* rng) {
    int tid = threadIdx.x;

    int* src_coloring;
    int* dst_coloring;
    int* projection;
    int dst_size;

    switch (level) {
        case 0:
            src_coloring = state->coloring_L1;
            dst_coloring = state->coloring_L0;
            projection = state->projection_L0_to_L1;
            dst_size = 512;
            break;
        case 1:
            src_coloring = state->coloring_L2;
            dst_coloring = state->coloring_L1;
            projection = state->projection_L1_to_L2;
            dst_size = 128;
            break;
        case 2:
            src_coloring = state->coloring_L3;
            dst_coloring = state->coloring_L2;
            projection = state->projection_L2_to_L3;
            dst_size = 32;
            break;
        default:
            return;
    }

    // Interpolate coarse solution to fine
    for (int f = tid; f < dst_size; f += BLOCK_SIZE) {
        int c = projection[f];
        // Use coarse color as hint, blend with current
        int coarse_color = src_coloring[c];

        // Weight by prolongation weight
        if (curand_uniform(rng) < cfg->prolongation_weight) {
            dst_coloring[f] = coarse_color;
        }
    }
}

/**
 * Multigrid Smoothing: Gauss-Seidel relaxation at given level
 */
__device__ void smooth_iteration(UltraSharedState* state, int level, const RuntimeConfig* cfg, cg::thread_block block) {
    int tid = threadIdx.x;

    int* coloring;
    float* signal;
    int size;

    switch (level) {
        case 0: coloring = state->coloring_L0; signal = state->conflict_signal_L0; size = 512; break;
        case 1: coloring = state->coloring_L1; signal = state->conflict_signal_L1; size = 128; break;
        case 2: coloring = state->coloring_L2; signal = state->conflict_signal_L2; size = 32; break;
        case 3: coloring = state->coloring_L3; signal = state->conflict_signal_L3; size = 8; break;
        default: return;
    }

    // Simple smoothing: dampen high-signal vertices
    for (int i = tid; i < size; i += BLOCK_SIZE) {
        if (signal[i] > 0.5f) {
            // Dampen conflict signal
            signal[i] *= 0.9f;
        }
    }
}

/**
 * Dendritic Reservoir Update: 8-branch neuromorphic processing
 */
__device__ void dendritic_update(UltraSharedState* state, int vertex, const RuntimeConfig* cfg) {
    // Update dendritic compartments based on conflict signal
    float conflict_input = state->conflict_signal_L0[vertex];

    // Process each branch with its time constant
    for (int b = 0; b < NUM_BRANCHES; b++) {
        float tau = cfg->tau_decay[b];

        // Decay existing activation
        state->dendrite[vertex].activation[b] *= tau;

        // Add new input weighted by branch
        float input_weight = cfg->input_scaling * cfg->branch_weights[b];
        state->dendrite[vertex].activation[b] += conflict_input * input_weight;

        // Clamp activation
        state->dendrite[vertex].activation[b] = fminf(1.0f,
            fmaxf(-1.0f, state->dendrite[vertex].activation[b]));
    }

    // Update calcium (long-term memory)
    state->dendrite[vertex].calcium *= 0.99f;
    state->dendrite[vertex].calcium += conflict_input * 0.01f;
    state->dendrite[vertex].calcium = fminf(1.0f, state->dendrite[vertex].calcium);

    // Soma integration
    float soma_input = 0.0f;
    for (int b = 0; b < NUM_BRANCHES; b++) {
        soma_input += state->dendrite[vertex].activation[b] * cfg->branch_weights[b];
    }

    state->soma_potential[vertex] = (1.0f - cfg->reservoir_leak_rate) * state->soma_potential[vertex] +
                                    cfg->reservoir_leak_rate * tanhf(soma_input);

    // Check for spike
    if (state->soma_potential[vertex] > state->dendrite[vertex].threshold) {
        state->spike_history[vertex] = 0.9f * state->spike_history[vertex] + 0.1f;
        state->soma_potential[vertex] = 0.0f;
        state->dendrite[vertex].refractory = 2.0f;
    } else {
        state->spike_history[vertex] *= 0.95f;
    }

    // Update refractory period
    if (state->dendrite[vertex].refractory > 0.0f) {
        state->dendrite[vertex].refractory -= 1.0f;
    }
}

/**
 * Quantum Evolution: Schrödinger dynamics for 6-state superposition
 */
__device__ void quantum_evolve(UltraSharedState* state, int vertex, const RuntimeConfig* cfg) {
    // Evolve quantum amplitudes
    QuantumVertex* q = &state->quantum[vertex];
    float conflict = state->conflict_signal_L0[vertex];

    for (int s = 0; s < NUM_QUANTUM_STATES; s++) {
        // Energy based on conflict for this color
        int color = q->color_idx[s];
        float energy = conflict * cfg->chemical_potential * (float)color / (float)MAX_COLORS;

        // Phase evolution: U(t) = exp(-iHt/ħ)
        float phase = energy * cfg->transverse_field;
        float cos_p = cosf(phase);
        float sin_p = sinf(phase);

        // Rotate amplitude: |ψ⟩ → U|ψ⟩
        float r = q->amplitude_real[s];
        float i = q->amplitude_imag[s];
        q->amplitude_real[s] = r * cos_p - i * sin_p;
        q->amplitude_imag[s] = r * sin_p + i * cos_p;

        // Apply decoherence (interference decay)
        q->amplitude_imag[s] *= (1.0f - cfg->interference_decay);
    }

    // Normalize wavefunction to preserve ⟨ψ|ψ⟩ = 1
    float norm_sq = 0.0f;
    for (int s = 0; s < NUM_QUANTUM_STATES; s++) {
        norm_sq += q->amplitude_real[s] * q->amplitude_real[s] +
                   q->amplitude_imag[s] * q->amplitude_imag[s];
    }
    float norm = sqrtf(fmaxf(EPSILON, norm_sq));
    for (int s = 0; s < NUM_QUANTUM_STATES; s++) {
        q->amplitude_real[s] /= norm;
        q->amplitude_imag[s] /= norm;
    }
}

/**
 * TPTP Update: Persistent homology computation (simplified)
 */
__device__ void tptp_update(UltraSharedState* state, const RuntimeConfig* cfg) {
    // Simplified persistent homology computation
    // Full implementation would build Vietoris-Rips complex and compute homology

    // Count connected components (β0)
    int num_components = 0;
    int visited[512];
    for (int i = 0; i < 512; i++) visited[i] = 0;

    for (int i = 0; i < 512; i++) {
        if (!visited[i] && state->conflict_signal_L0[i] > 0.0f) {
            num_components++;
            // BFS to mark component (simplified - just mark current)
            visited[i] = 1;
        }
    }

    float prev_betti_1 = state->tda.betti[1];

    state->tda.betti[0] = (float)num_components;

    // Estimate β1 (cycles) from conflict structure
    // β1 ≈ E - V + 1 for connected graph
    int num_conflicts = 0;
    for (int i = 0; i < 512; i++) {
        if (state->conflict_signal_L0[i] > 0.0f) {
            num_conflicts++;
        }
    }
    state->tda.betti[1] = fmaxf(0.0f, (float)num_conflicts - (float)num_components + 1.0f);
    state->tda.betti[2] = 0.0f; // Would compute voids (β2)

    // Compute derivative
    state->tda.betti_1_derivative = state->tda.betti[1] - prev_betti_1;

    // Detect phase transition based on rapid Betti number changes
    float transition_score = fabsf(state->tda.betti_1_derivative);
    state->tda.transition_detected = (transition_score > cfg->transition_sensitivity) ? 1 : 0;

    // Update stability score
    state->tda.stability_score = (transition_score < 0.1f) ?
        fminf(1.0f, state->tda.stability_score + 0.1f) :
        fmaxf(0.0f, state->tda.stability_score - 0.2f);
}

/**
 * Tunneling Decision: Determine if quantum tunneling should occur
 */
__device__ bool should_tunnel(UltraSharedState* state, int vertex, const RuntimeConfig* cfg, curandState* rng) {
    QuantumVertex* q = &state->quantum[vertex];

    // Base tunneling probability
    float prob = cfg->tunneling_prob_base;

    // Boost at phase transitions
    if (state->tda.transition_detected) {
        prob *= cfg->tunneling_prob_boost;
    }

    // Boost for high-conflict vertices
    if (state->conflict_signal_L0[vertex] > 2.0f) {
        prob *= 1.5f;
    }

    // Boost for stagnant vertices (high calcium)
    if (dendritic_enabled(cfg) && state->dendrite[vertex].calcium > 0.8f) {
        prob *= 2.0f;
    }

    q->tunneling_prob = fminf(1.0f, prob);

    // Stochastic decision based on probability
    return (curand_uniform(rng) < q->tunneling_prob);
}

/**
 * Active Inference Update: Belief propagation and free energy minimization
 */
__device__ void active_inference_update(UltraSharedState* state, int vertex, const RuntimeConfig* cfg) {
    int current_color = state->coloring_L0[vertex];
    if (current_color < 0 || current_color >= 16) current_color = 0;

    // Update beliefs based on conflict observations
    float conflict = state->conflict_signal_L0[vertex];

    // Prediction error: expected no conflict, observed conflict
    float prediction_error = conflict;

    // Update belief for current color (decrease if conflicting)
    state->belief_distribution[vertex][current_color] -=
        cfg->belief_update_rate * prediction_error * cfg->precision_weight;

    // Normalize beliefs to ensure valid probability distribution
    float sum = 0.0f;
    for (int c = 0; c < 16; c++) {
        state->belief_distribution[vertex][c] = fmaxf(0.01f, state->belief_distribution[vertex][c]);
        sum += state->belief_distribution[vertex][c];
    }
    for (int c = 0; c < 16; c++) {
        state->belief_distribution[vertex][c] /= fmaxf(EPSILON, sum);
    }

    // Compute expected free energy: F = E[E] - H[P(o|s)]
    float efe = 0.0f;
    for (int c = 0; c < 16; c++) {
        float belief = state->belief_distribution[vertex][c];
        // Entropy term: -Σ p log p
        efe -= belief * logf(fmaxf(EPSILON, belief));
    }
    state->expected_free_energy[vertex] = efe;
}

/**
 * Parallel Tempering Step: Metropolis-Hastings at given temperature
 */
__device__ void tempering_step(UltraSharedState* state, int replica, const RuntimeConfig* cfg, curandState* rng) {
    // Simplified parallel tempering step
    float temp = state->temperatures[replica];
    TemperingReplica* rep = &state->replica[replica];

    int lane = threadIdx.x % 32;
    if (lane < 64) {
        int v = lane;
        int current = rep->coloring[v];

        // Propose random new color
        int new_color = (int)(curand_uniform(rng) * MAX_COLORS) % MAX_COLORS;

        // Compute energy change (simplified - would need actual graph)
        float delta_E = 0.0f; // Placeholder

        // Metropolis acceptance criterion
        bool accept = (delta_E <= 0.0f) ||
                     (curand_uniform(rng) < expf(-delta_E / fmaxf(EPSILON, temp)));

        if (accept) {
            rep->coloring[v] = new_color;
        }
    }
}

/**
 * Replica Exchange: Swap configurations between adjacent temperature replicas
 */
__device__ void replica_exchange(UltraSharedState* state, const RuntimeConfig* cfg, curandState* rng) {
    // Attempt swaps between adjacent replicas
    int max_replicas = min(cfg->num_replicas, NUM_REPLICAS);

    for (int i = 0; i < max_replicas - 1; i += 2) {
        TemperingReplica* r1 = &state->replica[i];
        TemperingReplica* r2 = &state->replica[i + 1];

        float T1 = state->temperatures[i];
        float T2 = state->temperatures[i + 1];
        float E1 = (float)r1->conflicts;
        float E2 = (float)r2->conflicts;

        // Swap probability: P = exp[(1/T1 - 1/T2)(E2 - E1)]
        float delta = (1.0f/fmaxf(EPSILON, T1) - 1.0f/fmaxf(EPSILON, T2)) * (E2 - E1);
        bool accept = (delta >= 0.0f) || (curand_uniform(rng) < expf(delta));

        if (accept) {
            // Swap colorings
            for (int v = 0; v < 64; v++) {
                int temp = r1->coloring[v];
                r1->coloring[v] = r2->coloring[v];
                r2->coloring[v] = temp;
            }
            // Swap energies
            int temp_c = r1->conflicts;
            r1->conflicts = r2->conflicts;
            r2->conflicts = temp_c;

            float temp_e = r1->energy;
            r1->energy = r2->energy;
            r2->energy = temp_e;
        }
    }
}

// ═══════════════════════════════════════════════════════════════════════════
// PART 2: ADVANCED PARALLEL TEMPERING & WHCR FUNCTIONS (Lines 1000-2000)
// ═══════════════════════════════════════════════════════════════════════════

/**
 * ReplicaState - Extended state for parallel tempering replicas
 */
struct ReplicaState {
    int coloring[512];           // Full coloring for this replica
    float energy;                // Total energy (conflicts + potential)
    int conflicts;               // Number of conflicts
    float acceptance_rate;       // Running acceptance ratio
    int moves_attempted;         // Move counter
    int moves_accepted;          // Accepted move counter
    float temperature;           // Current temperature
    int color_histogram[MAX_COLORS]; // Color usage frequency
};

/**
 * ActiveInferenceBeliefs - Extended belief state structure
 */
struct ActiveInferenceBeliefs {
    float color_beliefs[MAX_COLORS];     // Belief distribution over colors
    float expected_utility[MAX_COLORS];  // Expected utility per color
    float prediction_error;              // Cumulative prediction error
    float precision;                     // Precision weight (inverse variance)
    float policy_entropy;                // Entropy of action distribution
    int preferred_action;                // MAP estimate of best action
};

// ═══════════════════════════════════════════════════════════════════════════
// PARALLEL TEMPERING FUNCTIONS (Lines 1000-1300)
// ═══════════════════════════════════════════════════════════════════════════

/**
 * PT Replica Update - Metropolis-Hastings move at replica temperature
 * Uses warp shuffle for efficient conflict counting
 */
__device__ void pt_replica_update(
    ReplicaState* replicas,
    float* energies,
    int* colors,
    const int* __restrict__ row_ptr,
    const int* __restrict__ col_idx,
    int replica_id,
    int num_vertices,
    float temperature,
    curandState* rng
) {
    int lane = threadIdx.x % WARP_SIZE;
    int warp_id = threadIdx.x / WARP_SIZE;

    ReplicaState* replica = &replicas[replica_id];

    // Each thread in warp handles different vertices
    for (int v = lane; v < num_vertices; v += WARP_SIZE) {
        int current_color = replica->coloring[v];

        // Propose new color (random or neighbor-aware)
        int new_color;
        if (curand_uniform(rng) < 0.3f) {
            // Random color
            new_color = (int)(curand_uniform(rng) * MAX_COLORS) % MAX_COLORS;
        } else {
            // Neighbor-aware: pick least-used neighbor color
            int color_counts[MAX_COLORS];
            for (int c = 0; c < MAX_COLORS; c++) color_counts[c] = 0;

            int start = row_ptr[v];
            int end = row_ptr[v + 1];
            for (int e = start; e < end; e++) {
                int neighbor = col_idx[e];
                int nc = replica->coloring[neighbor];
                if (nc >= 0 && nc < MAX_COLORS) {
                    color_counts[nc]++;
                }
            }

            // Find least-conflicting color
            int min_conflicts = 999999;
            new_color = current_color;
            for (int c = 0; c < MAX_COLORS; c++) {
                if (color_counts[c] < min_conflicts) {
                    min_conflicts = color_counts[c];
                    new_color = c;
                }
            }
        }

        // Compute energy change
        int start = row_ptr[v];
        int end = row_ptr[v + 1];

        float old_energy = 0.0f;
        float new_energy = 0.0f;

        for (int e = start; e < end; e++) {
            int neighbor = col_idx[e];
            int neighbor_color = replica->coloring[neighbor];

            if (neighbor_color == current_color) old_energy += 1.0f;
            if (neighbor_color == new_color) new_energy += 1.0f;
        }

        float delta_E = new_energy - old_energy;

        // Metropolis acceptance with temperature
        bool accept = false;
        if (delta_E <= 0.0f) {
            accept = true;
        } else {
            float prob = expf(-delta_E / fmaxf(EPSILON, temperature));
            accept = (curand_uniform(rng) < prob);
        }

        // Apply move if accepted
        if (accept) {
            replica->coloring[v] = new_color;
            atomicAdd(&replica->moves_accepted, 1);
        }
        atomicAdd(&replica->moves_attempted, 1);
    }

    // Warp-level reduction to compute total energy
    __shared__ float warp_energies[8]; // Support up to 256 threads (8 warps)

    float thread_energy = 0.0f;
    for (int v = lane; v < num_vertices; v += WARP_SIZE) {
        int my_color = replica->coloring[v];
        int start = row_ptr[v];
        int end = row_ptr[v + 1];

        for (int e = start; e < end; e++) {
            int neighbor = col_idx[e];
            if (replica->coloring[neighbor] == my_color) {
                thread_energy += 1.0f;
            }
        }
    }

    // Warp shuffle reduction
    for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {
        thread_energy += __shfl_down_sync(0xFFFFFFFF, thread_energy, offset);
    }

    // First thread in warp writes result
    if (lane == 0) {
        warp_energies[warp_id] = thread_energy;
    }
    __syncthreads();

    // Final reduction across warps (thread 0 only)
    if (threadIdx.x == 0) {
        float total_energy = 0.0f;
        int num_warps = (blockDim.x + WARP_SIZE - 1) / WARP_SIZE;
        for (int w = 0; w < num_warps; w++) {
            total_energy += warp_energies[w];
        }
        replica->energy = total_energy / 2.0f; // Each edge counted twice
        replica->conflicts = (int)replica->energy;

        // Update acceptance rate
        if (replica->moves_attempted > 0) {
            replica->acceptance_rate = (float)replica->moves_accepted / (float)replica->moves_attempted;
        }
    }
}

/**
 * PT Exchange Criterion - Compute Metropolis acceptance for replica swap
 */
__device__ bool pt_exchange_criterion(
    float energy_i,
    float energy_j,
    float temp_i,
    float temp_j,
    curandState* rng
) {
    // Parallel tempering exchange probability:
    // P(swap) = min(1, exp[(β_i - β_j)(E_j - E_i)])
    // where β = 1/T

    float beta_i = 1.0f / fmaxf(EPSILON, temp_i);
    float beta_j = 1.0f / fmaxf(EPSILON, temp_j);

    float delta = (beta_i - beta_j) * (energy_j - energy_i);

    if (delta >= 0.0f) {
        return true; // Always accept beneficial swaps
    } else {
        float prob = expf(delta);
        return (curand_uniform(rng) < prob);
    }
}

/**
 * PT Swap Replicas - Exchange configurations between two replicas
 * Uses shared memory for efficient swapping
 */
__device__ void pt_swap_replicas(
    ReplicaState* replicas,
    int replica_a,
    int replica_b,
    int num_vertices
) {
    int tid = threadIdx.x;

    ReplicaState* ra = &replicas[replica_a];
    ReplicaState* rb = &replicas[replica_b];

    // Parallel swap of colorings
    for (int v = tid; v < num_vertices; v += blockDim.x) {
        int temp = ra->coloring[v];
        ra->coloring[v] = rb->coloring[v];
        rb->coloring[v] = temp;
    }

    // Swap energies (thread 0 only)
    if (tid == 0) {
        float temp_e = ra->energy;
        ra->energy = rb->energy;
        rb->energy = temp_e;

        int temp_c = ra->conflicts;
        ra->conflicts = rb->conflicts;
        rb->conflicts = temp_c;
    }

    __syncthreads();
}

/**
 * PT Adaptive Temperature Schedule - Adjust temperatures based on acceptance rates
 */
__device__ void pt_adaptive_temperature(
    ReplicaState* replicas,
    float* temperature_ladder,
    int num_replicas,
    float target_acceptance
) {
    int tid = threadIdx.x;

    // Each thread handles one replica
    if (tid < num_replicas) {
        ReplicaState* replica = &replicas[tid];

        // Target acceptance rate is typically 0.25-0.35
        float current_rate = replica->acceptance_rate;
        float diff = current_rate - target_acceptance;

        // Adjust temperature: increase if too low acceptance, decrease if too high
        float adjustment = 1.0f + 0.05f * diff;
        replica->temperature *= adjustment;

        // Clamp temperature to reasonable range
        replica->temperature = fminf(10.0f, fmaxf(0.1f, replica->temperature));

        // Update ladder
        temperature_ladder[tid] = replica->temperature;
    }

    __syncthreads();
}

// ═══════════════════════════════════════════════════════════════════════════
// WHCR CONFLICT REPAIR FUNCTIONS (Lines 1300-1600)
// ═══════════════════════════════════════════════════════════════════════════

/**
 * WHCR Count Conflicts - Efficiently count conflicts for a vertex
 * Uses warp shuffle for neighbor conflict aggregation
 */
__device__ int whcr_count_conflicts(
    int* colors,
    const int* __restrict__ row_ptr,
    const int* __restrict__ col_idx,
    int vertex,
    int num_vertices
) {
    if (vertex >= num_vertices) return 0;

    int my_color = colors[vertex];
    int start = row_ptr[vertex];
    int end = row_ptr[vertex + 1];

    int conflicts = 0;
    int lane = threadIdx.x % WARP_SIZE;

    // Process neighbors in parallel within warp
    for (int e = start + lane; e < end; e += WARP_SIZE) {
        int neighbor = col_idx[e];
        if (colors[neighbor] == my_color) {
            conflicts++;
        }
    }

    // Warp-level reduction using shuffle
    for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {
        conflicts += __shfl_down_sync(0xFFFFFFFF, conflicts, offset);
    }

    // First lane has total
    return __shfl_sync(0xFFFFFFFF, conflicts, 0);
}

/**
 * WHCR Wavelet Priority - Compute vertex priority using wavelet coefficients
 * Multi-resolution analysis identifies important vertices at different scales
 */
__device__ void whcr_wavelet_priority(
    float* priorities,
    int* conflict_counts,
    float* wavelet_coeffs,
    int num_vertices,
    int level,
    const RuntimeConfig* cfg
) {
    int tid = threadIdx.x;

    for (int v = tid; v < num_vertices; v += blockDim.x) {
        float priority = 0.0f;

        // Base priority from conflicts
        priority += (float)conflict_counts[v] * cfg->stress_weight;

        // Add wavelet detail coefficients at multiple scales
        // Higher-frequency details indicate local hotspots
        for (int l = 0; l < min(level, NUM_LEVELS); l++) {
            int idx = l * num_vertices + v;
            float detail = wavelet_coeffs[idx];

            // Weight higher frequencies more (they indicate sharp transitions)
            float scale_weight = powf(2.0f, (float)l);
            priority += fabsf(detail) * scale_weight * cfg->persistence_weight;
        }

        // Apply hotspot multiplier for high-priority vertices
        if (priority > 5.0f) {
            priority *= cfg->hotspot_multiplier;
        }

        priorities[v] = priority;
    }

    __syncthreads();
}

/**
 * WHCR Wavelet Transform - Haar wavelet decomposition
 * Performs 1D Haar wavelet transform on conflict signal
 */
__device__ void whcr_wavelet_transform(
    float* signal,
    float* approx,
    float* detail,
    int size
) {
    int tid = threadIdx.x;

    // Haar wavelet: approximation and detail coefficients
    int output_size = size / 2;

    for (int i = tid; i < output_size; i += blockDim.x) {
        int idx = i * 2;
        if (idx + 1 < size) {
            float s0 = signal[idx];
            float s1 = signal[idx + 1];

            // Approximation: (s0 + s1) / sqrt(2)
            approx[i] = (s0 + s1) * 0.7071067811865476f;

            // Detail: (s0 - s1) / sqrt(2)
            detail[i] = (s0 - s1) * 0.7071067811865476f;
        }
    }

    __syncthreads();
}

/**
 * WHCR Select Repair Color - Choose best color for conflict repair
 * Uses weighted color selection with wavelet-guided priorities
 */
__device__ int whcr_select_repair_color(
    int vertex,
    int* colors,
    const int* __restrict__ row_ptr,
    const int* __restrict__ col_idx,
    int num_colors,
    float* color_weights,
    float priority,
    const RuntimeConfig* cfg,
    curandState* rng
) {
    int current_color = colors[vertex];

    // Build color frequency table
    float color_scores[MAX_COLORS];
    for (int c = 0; c < MAX_COLORS; c++) {
        color_scores[c] = 0.0f;
    }

    // Count neighbor colors
    int start = row_ptr[vertex];
    int end = row_ptr[vertex + 1];

    for (int e = start; e < end; e++) {
        int neighbor = col_idx[e];
        int nc = colors[neighbor];
        if (nc >= 0 && nc < MAX_COLORS) {
            color_scores[nc] += 1.0f; // Penalty for used colors
        }
    }

    // Compute repair scores (lower is better)
    float best_score = 1e9f;
    int best_color = current_color;

    for (int c = 0; c < min(num_colors, MAX_COLORS); c++) {
        if (c == current_color) continue;

        float score = color_scores[c]; // Conflict penalty

        // Add chemical potential (prefer lower colors)
        score += cfg->chemical_potential * (float)c / (float)MAX_COLORS;

        // Weight by wavelet priority (high priority → more exploration)
        if (priority > 3.0f) {
            // High priority: add randomness for exploration
            score += curand_uniform(rng) * 2.0f;
        }

        // Apply external color weights if provided
        if (color_weights != nullptr) {
            score *= (1.0f + color_weights[c]);
        }

        if (score < best_score) {
            best_score = score;
            best_color = c;
        }
    }

    return best_color;
}

/**
 * WHCR Apply Repair - Atomically apply color repair and track delta
 */
__device__ void whcr_apply_repair(
    int* colors,
    int vertex,
    int new_color,
    int* conflict_delta,
    const int* __restrict__ row_ptr,
    const int* __restrict__ col_idx,
    int* locks
) {
    // Acquire lock
    while (atomicCAS(&locks[vertex], 0, 1) != 0) {
        // Spin wait
    }

    int old_color = colors[vertex];

    // Count old conflicts
    int old_conflicts = 0;
    int start = row_ptr[vertex];
    int end = row_ptr[vertex + 1];

    for (int e = start; e < end; e++) {
        int neighbor = col_idx[e];
        if (colors[neighbor] == old_color) {
            old_conflicts++;
        }
    }

    // Apply new color
    colors[vertex] = new_color;

    // Count new conflicts
    int new_conflicts = 0;
    for (int e = start; e < end; e++) {
        int neighbor = col_idx[e];
        if (colors[neighbor] == new_color) {
            new_conflicts++;
        }
    }

    // Update delta (negative is improvement)
    int delta = new_conflicts - old_conflicts;
    atomicAdd(conflict_delta, delta);

    // Release lock
    atomicExch(&locks[vertex], 0);
}

/**
 * WHCR Hierarchical Repair - Multi-level repair using wavelet decomposition
 */
__device__ void whcr_hierarchical_repair(
    UltraSharedState* state,
    const int* __restrict__ row_ptr,
    const int* __restrict__ col_idx,
    int num_vertices,
    const RuntimeConfig* cfg,
    curandState* rng
) {
    int tid = threadIdx.x;
    cg::thread_block block = cg::this_thread_block();

    // Perform wavelet decomposition on conflict signal
    for (int level = 0; level < cfg->num_levels; level++) {
        int size = 512 >> level; // 512, 256, 128, 64

        if (tid < size / 2) {
            whcr_wavelet_transform(
                level == 0 ? state->conflict_signal_L0 : state->wavelet_approx[level - 1],
                state->wavelet_approx[level],
                (float*)state->wavelet_detail[level],
                size
            );
        }
        block.sync();
    }

    // Compute priorities using wavelet coefficients
    float priorities[512];
    int conflict_counts[512];

    for (int v = tid; v < num_vertices; v += blockDim.x) {
        conflict_counts[v] = whcr_count_conflicts(
            state->coloring_L0,
            row_ptr,
            col_idx,
            v,
            num_vertices
        );
    }
    block.sync();

    whcr_wavelet_priority(
        priorities,
        conflict_counts,
        (float*)state->wavelet_detail[0], // Use detail coefficients
        num_vertices,
        cfg->num_levels,
        cfg
    );

    // Repair vertices in priority order (highest first)
    // Sort-free approach: iterate with threshold
    for (float threshold = 10.0f; threshold > 0.0f; threshold -= 2.0f) {
        for (int v = tid; v < num_vertices; v += blockDim.x) {
            if (priorities[v] >= threshold && conflict_counts[v] > 0) {
                int new_color = whcr_select_repair_color(
                    v,
                    state->coloring_L0,
                    row_ptr,
                    col_idx,
                    MAX_COLORS,
                    nullptr,
                    priorities[v],
                    cfg,
                    rng
                );

                if (new_color != state->coloring_L0[v]) {
                    int delta = 0;
                    whcr_apply_repair(
                        state->coloring_L0,
                        v,
                        new_color,
                        &delta,
                        row_ptr,
                        col_idx,
                        state->locks
                    );
                }
            }
        }
        block.sync();
    }
}

// ═══════════════════════════════════════════════════════════════════════════
// ACTIVE INFERENCE FUNCTIONS (Lines 1600-1900)
// ═══════════════════════════════════════════════════════════════════════════

/**
 * AI Update Beliefs - Bayesian belief update based on observations
 */
__device__ void ai_update_beliefs(
    ActiveInferenceBeliefs* beliefs,
    float* observations,
    float* predictions,
    int num_vertices,
    const RuntimeConfig* cfg
) {
    int tid = threadIdx.x;

    for (int v = tid; v < num_vertices; v += blockDim.x) {
        ActiveInferenceBeliefs* b = &beliefs[v];

        float obs = observations[v];  // Observed conflict
        float pred = predictions[v];  // Predicted conflict

        // Prediction error
        float error = obs - pred;
        b->prediction_error = 0.9f * b->prediction_error + 0.1f * fabsf(error);

        // Update precision (inverse variance) - higher for stable predictions
        if (fabsf(error) < 0.1f) {
            b->precision = fminf(10.0f, b->precision * 1.05f);
        } else {
            b->precision = fmaxf(0.1f, b->precision * 0.95f);
        }

        // Bayesian belief update: P(s|o) ∝ P(o|s) P(s)
        float likelihood_weight = cfg->precision_weight * b->precision;

        for (int c = 0; c < MAX_COLORS; c++) {
            // Likelihood: low if color would cause conflicts
            float likelihood = expf(-likelihood_weight * obs);

            // Prior (current belief)
            float prior = b->color_beliefs[c];

            // Posterior ∝ likelihood × prior
            b->color_beliefs[c] = likelihood * prior;
        }

        // Normalize beliefs
        float sum = 0.0f;
        for (int c = 0; c < MAX_COLORS; c++) {
            b->color_beliefs[c] = fmaxf(EPSILON, b->color_beliefs[c]);
            sum += b->color_beliefs[c];
        }
        for (int c = 0; c < MAX_COLORS; c++) {
            b->color_beliefs[c] /= fmaxf(EPSILON, sum);
        }
    }

    __syncthreads();
}

/**
 * AI Free Energy - Compute variational free energy
 * F = E_Q[E] - H[Q] where Q is belief distribution
 */
__device__ float ai_free_energy(
    ActiveInferenceBeliefs* beliefs,
    int vertex,
    float* conflict_observations
) {
    ActiveInferenceBeliefs* b = &beliefs[vertex];

    // Energy term: expected conflict under current beliefs
    float expected_energy = 0.0f;
    for (int c = 0; c < MAX_COLORS; c++) {
        expected_energy += b->color_beliefs[c] * conflict_observations[vertex];
    }

    // Entropy term: H[Q] = -Σ Q(s) log Q(s)
    float entropy = 0.0f;
    for (int c = 0; c < MAX_COLORS; c++) {
        float belief = b->color_beliefs[c];
        if (belief > EPSILON) {
            entropy -= belief * logf(belief);
        }
    }

    // Free energy = Energy - Entropy
    float free_energy = expected_energy - entropy;

    return free_energy;
}

/**
 * AI Action Selection - Select action (color) to minimize expected free energy
 * Implements softmax policy over expected free energies
 */
__device__ void ai_action_selection(
    ActiveInferenceBeliefs* beliefs,
    float* action_probs,
    int vertex,
    int num_actions,
    const int* __restrict__ row_ptr,
    const int* __restrict__ col_idx,
    int* current_colors,
    const RuntimeConfig* cfg,
    curandState* rng
) {
    ActiveInferenceBeliefs* b = &beliefs[vertex];

    // Compute expected free energy for each action (color)
    float efe[MAX_COLORS];
    float min_efe = 1e9f;

    for (int c = 0; c < min(num_actions, MAX_COLORS); c++) {
        // Expected utility (negative conflicts)
        int start = row_ptr[vertex];
        int end = row_ptr[vertex + 1];

        float expected_conflicts = 0.0f;
        for (int e = start; e < end; e++) {
            int neighbor = col_idx[e];
            int neighbor_color = current_colors[neighbor];
            if (neighbor_color == c) {
                expected_conflicts += 1.0f;
            }
        }

        // Information gain (entropy reduction)
        float info_gain = -logf(fmaxf(EPSILON, b->color_beliefs[c]));

        // EFE = Expected Cost - Information Gain
        efe[c] = expected_conflicts - 0.5f * info_gain;
        b->expected_utility[c] = -expected_conflicts;

        if (efe[c] < min_efe) {
            min_efe = efe[c];
        }
    }

    // Softmax over negative EFE (lower EFE → higher probability)
    float temperature = cfg->policy_temperature;
    float sum_exp = 0.0f;

    for (int c = 0; c < min(num_actions, MAX_COLORS); c++) {
        float score = -(efe[c] - min_efe) / temperature;
        action_probs[c] = expf(score);
        sum_exp += action_probs[c];
    }

    // Normalize to probability distribution
    for (int c = 0; c < min(num_actions, MAX_COLORS); c++) {
        action_probs[c] /= fmaxf(EPSILON, sum_exp);
    }

    // Compute policy entropy
    float policy_entropy = 0.0f;
    for (int c = 0; c < min(num_actions, MAX_COLORS); c++) {
        if (action_probs[c] > EPSILON) {
            policy_entropy -= action_probs[c] * logf(action_probs[c]);
        }
    }
    b->policy_entropy = policy_entropy;

    // Select action (MAP estimate)
    float max_prob = 0.0f;
    int best_action = 0;
    for (int c = 0; c < min(num_actions, MAX_COLORS); c++) {
        if (action_probs[c] > max_prob) {
            max_prob = action_probs[c];
            best_action = c;
        }
    }
    b->preferred_action = best_action;
}

/**
 * AI Precision Weighting - Adaptive precision for belief updates
 * Higher precision when predictions are accurate
 */
__device__ void ai_update_precision(
    ActiveInferenceBeliefs* beliefs,
    float* prediction_errors,
    int num_vertices,
    float adaptation_rate
) {
    int tid = threadIdx.x;

    for (int v = tid; v < num_vertices; v += blockDim.x) {
        ActiveInferenceBeliefs* b = &beliefs[v];
        float error = prediction_errors[v];

        // Adaptive precision: decrease if high error, increase if low error
        if (error > 1.0f) {
            b->precision *= (1.0f - adaptation_rate);
        } else if (error < 0.1f) {
            b->precision *= (1.0f + adaptation_rate);
        }

        // Clamp precision
        b->precision = fminf(10.0f, fmaxf(0.01f, b->precision));
    }

    __syncthreads();
}

// ═══════════════════════════════════════════════════════════════════════════
// W-CYCLE MULTIGRID FUNCTIONS (Lines 1900-2000)
// ═══════════════════════════════════════════════════════════════════════════

/**
 * Multigrid Restrict - Restriction operator for coarsening
 * Uses full-weighting restriction for better accuracy
 */
__device__ void multigrid_restrict(
    float* fine,
    float* coarse,
    int fine_size,
    int coarse_size
) {
    int tid = threadIdx.x;
    int ratio = fine_size / coarse_size;

    for (int c = tid; c < coarse_size; c += blockDim.x) {
        float sum = 0.0f;
        float weight_sum = 0.0f;

        // Full-weighting: include neighbors with weights
        int center = c * ratio;

        for (int i = 0; i < ratio; i++) {
            int idx = center + i;
            if (idx < fine_size) {
                // Center weight: 0.5, neighbor weight: 0.25
                float weight = (i == ratio / 2) ? 0.5f : 0.25f;
                sum += fine[idx] * weight;
                weight_sum += weight;
            }
        }

        coarse[c] = sum / fmaxf(EPSILON, weight_sum);
    }

    __syncthreads();
}

/**
 * Multigrid Prolongate - Interpolation operator for refinement
 * Uses linear interpolation for smooth prolongation
 */
__device__ void multigrid_prolongate(
    float* coarse,
    float* fine,
    int coarse_size,
    int fine_size
) {
    int tid = threadIdx.x;
    int ratio = fine_size / coarse_size;

    for (int f = tid; f < fine_size; f += blockDim.x) {
        int c_left = f / ratio;
        int c_right = min(c_left + 1, coarse_size - 1);

        // Linear interpolation
        float alpha = (float)(f % ratio) / (float)ratio;
        fine[f] = (1.0f - alpha) * coarse[c_left] + alpha * coarse[c_right];
    }

    __syncthreads();
}

/**
 * Multigrid Smooth - Jacobi/Gauss-Seidel smoothing
 * Uses weighted Jacobi for parallel efficiency
 */
__device__ void multigrid_smooth(
    float* data,
    float* buffer,
    int size,
    int iterations,
    float omega
) {
    int tid = threadIdx.x;

    for (int iter = 0; iter < iterations; iter++) {
        // Weighted Jacobi: x^(k+1) = (1-ω)x^k + ω D^(-1)(b - Rx^k)
        for (int i = tid; i < size; i += blockDim.x) {
            float left = (i > 0) ? data[i - 1] : data[i];
            float right = (i < size - 1) ? data[i + 1] : data[i];
            float center = data[i];

            // Simple averaging with relaxation
            float new_val = 0.25f * left + 0.5f * center + 0.25f * right;
            buffer[i] = (1.0f - omega) * center + omega * new_val;
        }
        __syncthreads();

        // Copy buffer back to data
        for (int i = tid; i < size; i += blockDim.x) {
            data[i] = buffer[i];
        }
        __syncthreads();
    }
}

/**
 * Multigrid V-Cycle - Single V-cycle iteration
 */
__device__ void multigrid_vcycle(
    float** levels,
    float** buffers,
    int* sizes,
    int num_levels,
    int pre_smooth,
    int post_smooth,
    float omega
) {
    // Downward sweep (restriction + pre-smoothing)
    for (int l = 0; l < num_levels - 1; l++) {
        multigrid_smooth(levels[l], buffers[l], sizes[l], pre_smooth, omega);
        multigrid_restrict(levels[l], levels[l + 1], sizes[l], sizes[l + 1]);
    }

    // Coarsest level solve (extra smoothing)
    int coarsest = num_levels - 1;
    multigrid_smooth(levels[coarsest], buffers[coarsest], sizes[coarsest], 10, omega);

    // Upward sweep (prolongation + post-smoothing)
    for (int l = num_levels - 2; l >= 0; l--) {
        multigrid_prolongate(levels[l + 1], buffers[l], sizes[l + 1], sizes[l]);

        // Add correction
        for (int i = threadIdx.x; i < sizes[l]; i += blockDim.x) {
            levels[l][i] += buffers[l][i];
        }
        __syncthreads();

        multigrid_smooth(levels[l], buffers[l], sizes[l], post_smooth, omega);
    }
}

/**
 * Multigrid W-Cycle - More aggressive W-cycle (visits coarse levels twice)
 */
__device__ void multigrid_wcycle(
    float** levels,
    float** buffers,
    int* sizes,
    int num_levels,
    int pre_smooth,
    int post_smooth,
    float omega
) {
    int tid = threadIdx.x;

    // W-cycle: recurse twice at each level
    // For simplicity, implement as double V-cycle
    for (int cycle = 0; cycle < 2; cycle++) {
        multigrid_vcycle(levels, buffers, sizes, num_levels, pre_smooth, post_smooth, omega);
    }
}

// ═══════════════════════════════════════════════════════════════════════════
// END OF PART 2: ADVANCED FUNCTIONS
// ═══════════════════════════════════════════════════════════════════════════

// ═══════════════════════════════════════════════════════════════════════════
// PART 3: MAIN ULTRA KERNEL ENTRY POINTS & ORCHESTRATION (Lines 2000-3000)
// ═══════════════════════════════════════════════════════════════════════════

/**
 * Ultra Shared Memory Orchestrator
 *
 * Manages the complex 98KB shared memory layout across all subsystems.
 * Provides compile-time offsets and runtime validation.
 */
struct UltraSharedMemoryOrchestrator {
    // Memory partitions (offsets in bytes from shared base)
    static constexpr size_t DENDRITIC_OFFSET = 0;
    static constexpr size_t DENDRITIC_SIZE = 24 * 1024; // 24KB

    static constexpr size_t QUANTUM_OFFSET = DENDRITIC_OFFSET + DENDRITIC_SIZE;
    static constexpr size_t QUANTUM_SIZE = 16 * 1024; // 16KB

    static constexpr size_t REPLICA_OFFSET = QUANTUM_OFFSET + QUANTUM_SIZE;
    static constexpr size_t REPLICA_SIZE = 24 * 1024; // 24KB (12 replicas)

    static constexpr size_t WHCR_OFFSET = REPLICA_OFFSET + REPLICA_SIZE;
    static constexpr size_t WHCR_SIZE = 16 * 1024; // 16KB

    static constexpr size_t INFERENCE_OFFSET = WHCR_OFFSET + WHCR_SIZE;
    static constexpr size_t INFERENCE_SIZE = 8 * 1024; // 8KB

    static constexpr size_t WORK_OFFSET = INFERENCE_OFFSET + INFERENCE_SIZE;
    static constexpr size_t WORK_SIZE = 10 * 1024; // 10KB

    static constexpr size_t TOTAL_SIZE = WORK_OFFSET + WORK_SIZE; // 98KB

    /**
     * Validate shared memory usage at compile time
     */
    __device__ __forceinline__ static bool validate() {
        return TOTAL_SIZE <= 100 * 1024; // RTX 3060 has 100KB shared memory
    }

    /**
     * Get dendritic state pointer from shared memory base
     */
    __device__ __forceinline__ static DendriticState* get_dendritic(char* smem_base) {
        return reinterpret_cast<DendriticState*>(smem_base + DENDRITIC_OFFSET);
    }

    /**
     * Get quantum state pointer from shared memory base
     */
    __device__ __forceinline__ static QuantumVertex* get_quantum(char* smem_base) {
        return reinterpret_cast<QuantumVertex*>(smem_base + QUANTUM_OFFSET);
    }

    /**
     * Get replica state pointer from shared memory base
     */
    __device__ __forceinline__ static TemperingReplica* get_replicas(char* smem_base) {
        return reinterpret_cast<TemperingReplica*>(smem_base + REPLICA_OFFSET);
    }

    /**
     * Get work buffer pointer from shared memory base
     */
    __device__ __forceinline__ static float* get_work_buffer(char* smem_base) {
        return reinterpret_cast<float*>(smem_base + WORK_OFFSET);
    }
};

/**
 * Ultra Kernel Configuration - Extended runtime parameters
 */
struct UltraKernelConfig {
    // Core parameters
    int num_vertices;
    int num_edges;
    int max_iterations;

    // GPU resources
    int num_blocks;
    int threads_per_block;
    int shared_mem_size;

    // Optimization flags
    bool enable_cooperative_groups;
    bool enable_vectorization;
    bool enable_async_memcpy;

    // Performance tuning
    int warp_specialization_factor;
    int occupancy_target;
    float memory_bandwidth_fraction;

    // Convergence criteria
    float conflict_tolerance;
    int stagnation_limit;
    bool early_stopping;
};

// ═══════════════════════════════════════════════════════════════════════════
// MAIN ULTRA KERNEL ENTRY POINT
// ═══════════════════════════════════════════════════════════════════════════

/**
 * dr_whcr_ultra_fused_kernel
 *
 * The crown jewel of PRISM - fully fused ultra optimization kernel.
 * Combines all 8 advanced techniques with cooperative grid synchronization.
 *
 * Features:
 * - Cooperative grid-level synchronization
 * - 98KB shared memory orchestration
 * - Vectorized memory access (float4)
 * - Optimal occupancy (RTX 3060: 50% occupancy target)
 * - Warp-specialized execution
 *
 * Requirements:
 * - CUDA 11.0+ for cooperative groups
 * - Compute Capability 8.6+ (RTX 3060)
 * - Must be launched via cudaLaunchCooperativeKernel
 */
extern "C" __global__ void __launch_bounds__(256, 4) dr_whcr_ultra_fused_kernel(
    // Graph structure (CSR format)
    const int* __restrict__ row_ptr,
    const int* __restrict__ col_idx,
    int num_vertices,
    int num_edges,

    // State arrays (global memory)
    int* __restrict__ colors,
    int* __restrict__ best_colors,
    int* __restrict__ best_num_colors,
    int* __restrict__ conflicts,

    // Dendritic state (global)
    float* __restrict__ dendritic_state_global,
    float* __restrict__ soma_potential_global,

    // Quantum state (global)
    float* __restrict__ quantum_real_global,
    float* __restrict__ quantum_imag_global,

    // Parallel tempering state (global)
    float* __restrict__ replica_temps,
    float* __restrict__ replica_energies,
    int* __restrict__ replica_colors,

    // Active inference state (global)
    float* __restrict__ belief_state_global,
    float* __restrict__ free_energy_global,

    // Configuration
    RuntimeConfig config,

    // RNG state
    curandState* __restrict__ rng_states,

    // Telemetry output
    KernelTelemetry* __restrict__ telemetry,

    // Iteration counter
    int iteration
) {
    // ═══════════════════════════════════════════════════════════════════════
    // COOPERATIVE GROUPS SETUP
    // ═══════════════════════════════════════════════════════════════════════

    cg::grid_group grid = cg::this_grid();
    cg::thread_block block = cg::this_thread_block();

    int gid = blockIdx.x * blockDim.x + threadIdx.x;
    int tid = threadIdx.x;
    int bid = blockIdx.x;
    int lane_id = tid % 32;
    int warp_id = tid / 32;

    // ═══════════════════════════════════════════════════════════════════════
    // SHARED MEMORY ALLOCATION
    // ═══════════════════════════════════════════════════════════════════════

    extern __shared__ char shared_mem[];
    UltraSharedState* state = reinterpret_cast<UltraSharedState*>(shared_mem);

    // Validate shared memory layout
    static_assert(sizeof(UltraSharedState) <= 100 * 1024, "Shared memory exceeds 100KB limit");

    // Initialize RNG for this thread
    curandState local_rng = rng_states[gid];

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 1: INITIALIZE SHARED MEMORY (Vectorized)
    // ═══════════════════════════════════════════════════════════════════════

    // Calculate vertex partition for this block
    int vertices_per_block = (num_vertices + gridDim.x - 1) / gridDim.x;
    int block_start = bid * vertices_per_block;
    int block_end = min(block_start + vertices_per_block, num_vertices);
    int block_vertex_count = block_end - block_start;

    // Vectorized initialization of quantum state (float4 for coalescing)
    if (quantum_enabled(&config)) {
        int quantum_elements = min(block_vertex_count, MAX_VERTICES_PER_BLOCK) * NUM_QUANTUM_STATES;
        float4* quantum_real_vec = reinterpret_cast<float4*>(&state->quantum[0].amplitude_real[0]);
        float4* quantum_imag_vec = reinterpret_cast<float4*>(&state->quantum[0].amplitude_imag[0]);

        float amp = 1.0f / sqrtf((float)NUM_QUANTUM_STATES);
        float4 init_real = make_float4(amp, amp, amp, amp);
        float4 init_imag = make_float4(0.0f, 0.0f, 0.0f, 0.0f);

        for (int i = tid; i < quantum_elements / 4; i += blockDim.x) {
            quantum_real_vec[i] = init_real;
            quantum_imag_vec[i] = init_imag;
        }
    }

    // Vectorized initialization of belief distributions (float4)
    if (active_inference_enabled(&config)) {
        int belief_elements = min(block_vertex_count, MAX_VERTICES_PER_BLOCK) * 16;
        float4* belief_vec = reinterpret_cast<float4*>(&state->belief_distribution[0][0]);

        float4 uniform_belief = make_float4(1.0f/16.0f, 1.0f/16.0f, 1.0f/16.0f, 1.0f/16.0f);

        for (int i = tid; i < belief_elements / 4; i += blockDim.x) {
            belief_vec[i] = uniform_belief;
        }
    }

    // Initialize dendritic state
    if (dendritic_enabled(&config)) {
        for (int i = tid; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i += blockDim.x) {
            for (int b = 0; b < NUM_BRANCHES; b++) {
                state->dendrite[i].activation[b] = 0.0f;
            }
            state->dendrite[i].calcium = 0.0f;
            state->dendrite[i].threshold = 0.5f;
            state->dendrite[i].refractory = 0.0f;

            state->soma_potential[i] = 0.0f;
            state->spike_history[i] = 0.0f;
        }
    }

    // Initialize temperature ladder for parallel tempering
    if (tempering_enabled(&config)) {
        for (int i = tid; i < 16; i += blockDim.x) {
            if (i < 8) {
                state->temperatures[i] = config.temperatures[i];
            } else {
                state->temperatures[i] = 1.0f;
            }
        }
    }

    // Initialize TPTP state (single thread)
    if (tid == 0 && tptp_enabled(&config)) {
        state->tda.betti[0] = 0.0f;
        state->tda.betti[1] = 0.0f;
        state->tda.betti[2] = 0.0f;
        state->tda.max_persistence = 0.0f;
        state->tda.stability_score = 0.0f;
        state->tda.transition_detected = 0;
        state->tda.betti_1_derivative = 0.0f;
    }

    // Initialize projection mappings for multigrid
    if (multigrid_enabled(&config)) {
        for (int i = tid; i < 512; i += blockDim.x) {
            state->projection_L0_to_L1[i] = i / 4;
        }
        for (int i = tid; i < 128; i += blockDim.x) {
            state->projection_L1_to_L2[i] = i / 4;
        }
        for (int i = tid; i < 32; i += blockDim.x) {
            state->projection_L2_to_L3[i] = i / 4;
        }
    }

    // Initialize locks and work buffers
    for (int i = tid; i < MAX_VERTICES_PER_BLOCK; i += blockDim.x) {
        state->locks[i] = 0;
        state->move_deltas[i] = 0.0f;
        state->best_colors[i] = 0;
    }

    if (tid == 0) {
        state->num_conflict_vertices = 0;
    }

    block.sync();

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 2: LOAD COLORING FROM GLOBAL MEMORY (Vectorized)
    // ═══════════════════════════════════════════════════════════════════════

    // Coalesced loading of coloring
    for (int i = tid; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i += blockDim.x) {
        int v = block_start + i;
        if (v < num_vertices) {
            state->coloring_L0[i] = colors[v];
        }
    }

    block.sync();

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 3: COMPUTE INITIAL CONFLICT SIGNALS
    // ═══════════════════════════════════════════════════════════════════════

    for (int i = tid; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i += blockDim.x) {
        int v = block_start + i;
        if (v >= num_vertices) continue;

        int my_color = state->coloring_L0[i];
        int start = row_ptr[v];
        int end = row_ptr[v + 1];

        float conflict_count = 0.0f;

        // Count conflicts with neighbors
        for (int e = start; e < end; e++) {
            int neighbor = col_idx[e];
            int n_color;

            // Check if neighbor is in this block's shared memory
            if (neighbor >= block_start && neighbor < block_end) {
                int local_idx = neighbor - block_start;
                if (local_idx < MAX_VERTICES_PER_BLOCK) {
                    n_color = state->coloring_L0[local_idx];
                } else {
                    n_color = colors[neighbor];
                }
            } else {
                n_color = colors[neighbor];
            }

            if (n_color == my_color) {
                conflict_count += 1.0f;
            }
        }

        state->conflict_signal_L0[i] = conflict_count;
    }

    block.sync();

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 4: WARP-SPECIALIZED EXECUTION
    // ═══════════════════════════════════════════════════════════════════════

    // Assign specialized tasks to warps based on warp_id
    // Warp 0: Multigrid operations
    // Warp 1: Dendritic reservoir updates
    // Warp 2: Quantum evolution
    // Warp 3: Active inference
    // Warp 4-7: Parallel tempering replicas

    if (warp_id == 0 && multigrid_enabled(&config)) {
        // ═══════════════════════════════════════════════════════════════════
        // WARP 0: W-CYCLE MULTIGRID
        // ═══════════════════════════════════════════════════════════════════

        // Pre-smoothing at fine level
        for (int s = 0; s < config.pre_smooth_iterations; s++) {
            smooth_iteration(state, 0, &config, block);
        }

        // Restriction cascade: L0 → L1 → L2 → L3
        for (int level = 0; level < config.num_levels - 1; level++) {
            restrict_to_coarse(state, level, &config);
        }

        // Coarsest level direct solve
        if (lane_id < 8) {
            int v = lane_id;
            // Simple greedy coloring at coarsest level
            bool conflicted = state->conflict_signal_L3[v] > 0.0f;
            if (conflicted) {
                state->coloring_L3[v] = (state->coloring_L3[v] + 1) % MAX_COLORS;
            }
        }

        // Prolongation cascade: L3 → L2 → L1 → L0
        for (int level = config.num_levels - 2; level >= 0; level--) {
            prolongate_to_fine(state, level, &config, &local_rng);

            // Post-smoothing
            for (int s = 0; s < config.post_smooth_iterations; s++) {
                smooth_iteration(state, level, &config, block);
            }
        }
    }

    block.sync();

    if (warp_id == 1 && dendritic_enabled(&config)) {
        // ═══════════════════════════════════════════════════════════════════
        // WARP 1: DENDRITIC RESERVOIR UPDATE
        // ═══════════════════════════════════════════════════════════════════

        for (int i = lane_id; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i += 32) {
            dendritic_update(state, i, &config);

            // Compute neuromorphic priority
            float priority = 0.0f;
            priority += state->dendrite[i].activation[0] * 2.0f;
            for (int b = 1; b < NUM_BRANCHES; b++) {
                priority += state->dendrite[i].activation[b] * config.branch_weights[b];
            }
            priority += state->dendrite[i].calcium * 3.0f;
            priority += state->soma_potential[i] * 0.5f;

            // Store priority for WHCR weighting
            state->move_deltas[i] = priority;
        }
    }

    block.sync();

    if (warp_id == 2 && quantum_enabled(&config)) {
        // ═══════════════════════════════════════════════════════════════════
        // WARP 2: QUANTUM EVOLUTION & TUNNELING
        // ═══════════════════════════════════════════════════════════════════

        for (int i = lane_id; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i += 32) {
            // Evolve quantum state
            quantum_evolve(state, i, &config);

            // Check for quantum tunneling event
            if (should_tunnel(state, i, &config, &local_rng)) {
                // Collapse wavefunction to highest amplitude state
                float max_prob = 0.0f;
                int best_state = 0;

                for (int s = 0; s < NUM_QUANTUM_STATES; s++) {
                    float real = state->quantum[i].amplitude_real[s];
                    float imag = state->quantum[i].amplitude_imag[s];
                    float prob = real * real + imag * imag;

                    if (prob > max_prob) {
                        max_prob = prob;
                        best_state = s;
                    }
                }

                // Tunnel to new color
                int new_color = state->quantum[i].color_idx[best_state];
                state->coloring_L0[i] = new_color;

                // Atomic increment of tunneling events
                if (gid == blockIdx.x * blockDim.x) {
                    atomicAdd(&telemetry->tunneling_events, 1);
                }
            }
        }
    }

    block.sync();

    if (warp_id == 3 && active_inference_enabled(&config)) {
        // ═══════════════════════════════════════════════════════════════════
        // WARP 3: ACTIVE INFERENCE BELIEF UPDATE
        // ═══════════════════════════════════════════════════════════════════

        for (int i = lane_id; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i += 32) {
            active_inference_update(state, i, &config);
        }
    }

    block.sync();

    if (warp_id >= 4 && warp_id < 8 && tempering_enabled(&config)) {
        // ═══════════════════════════════════════════════════════════════════
        // WARPS 4-7: PARALLEL TEMPERING (4 replicas per warp)
        // ═══════════════════════════════════════════════════════════════════

        int replica_id = (warp_id - 4) * 3 + (lane_id / 11);
        if (replica_id < config.num_replicas && replica_id < NUM_REPLICAS) {
            tempering_step(state, replica_id, &config, &local_rng);
        }
    }

    block.sync();

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 5: TPTP PERSISTENT HOMOLOGY (Single thread)
    // ═══════════════════════════════════════════════════════════════════════

    if (tid == 0 && tptp_enabled(&config)) {
        tptp_update(state, &config);

        if (state->tda.transition_detected) {
            atomicAdd(&telemetry->phase_transitions, 1);
        }
    }

    block.sync();

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 6: REPLICA EXCHANGE (Parallel Tempering)
    // ═══════════════════════════════════════════════════════════════════════

    if (tid == 0 && tempering_enabled(&config)) {
        if (iteration % config.swap_interval == 0) {
            replica_exchange(state, &config, &local_rng);
        }
    }

    block.sync();

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 7: WHCR CONFLICT REPAIR (All threads)
    // ═══════════════════════════════════════════════════════════════════════

    // Identify conflict vertices
    if (tid == 0) {
        state->num_conflict_vertices = 0;
    }
    block.sync();

    for (int i = tid; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i += blockDim.x) {
        if (state->conflict_signal_L0[i] > 0.5f) {
            int idx = atomicAdd(&state->num_conflict_vertices, 1);
            if (idx < MAX_VERTICES_PER_BLOCK) {
                state->conflict_vertices[idx] = i;
            }
        }
    }
    block.sync();

    // Evaluate best moves for conflict vertices
    int num_cv = min(state->num_conflict_vertices, MAX_VERTICES_PER_BLOCK);
    for (int cv_idx = tid; cv_idx < num_cv; cv_idx += blockDim.x) {
        int i = state->conflict_vertices[cv_idx];
        int v = block_start + i;
        if (v >= num_vertices) continue;

        int current_color = state->coloring_L0[i];

        // Count neighbor color histogram
        int neighbor_colors[MAX_COLORS];
        for (int c = 0; c < MAX_COLORS; c++) neighbor_colors[c] = 0;

        int start = row_ptr[v];
        int end = row_ptr[v + 1];

        for (int e = start; e < end; e++) {
            int neighbor = col_idx[e];
            int n_color;

            if (neighbor >= block_start && neighbor < block_end) {
                int local_idx = neighbor - block_start;
                if (local_idx < MAX_VERTICES_PER_BLOCK) {
                    n_color = state->coloring_L0[local_idx];
                } else {
                    n_color = colors[neighbor];
                }
            } else {
                n_color = colors[neighbor];
            }

            if (n_color >= 0 && n_color < MAX_COLORS) {
                neighbor_colors[n_color]++;
            }
        }

        // Find best color with multi-objective scoring
        int current_conflicts = neighbor_colors[current_color];
        float best_score = 1e9f;
        int best_color = current_color;

        for (int c = 0; c < MAX_COLORS; c++) {
            if (c == current_color) continue;

            float score = 0.0f;

            // Primary: conflict reduction
            score += (float)neighbor_colors[c] * config.stress_weight;

            // Chemical potential (prefer lower colors)
            score += config.chemical_potential * ((float)c / (float)MAX_COLORS);

            // Active inference belief guidance
            if (active_inference_enabled(&config) && c < 16 && current_color < 16) {
                float belief_diff = state->belief_distribution[i][current_color] -
                                   state->belief_distribution[i][c];
                score += config.belief_weight * belief_diff;
            }

            // Dendritic reservoir priority
            if (dendritic_enabled(&config)) {
                score += config.stress_weight * state->move_deltas[i] * 0.1f;
            }

            // TPTP persistence weight (boost moves near phase transitions)
            if (tptp_enabled(&config) && state->tda.transition_detected) {
                score *= (1.0f + config.persistence_weight * 0.5f);
            }

            if (score < best_score) {
                best_score = score;
                best_color = c;
            }
        }

        state->best_colors[i] = best_color;
        state->move_deltas[i] = best_score;
    }
    block.sync();

    // Apply moves with atomic locking
    for (int cv_idx = tid; cv_idx < num_cv; cv_idx += blockDim.x) {
        int i = state->conflict_vertices[cv_idx];
        int new_color = state->best_colors[i];

        if (new_color == state->coloring_L0[i]) continue;

        // Metropolis acceptance (simulated annealing)
        float delta = state->move_deltas[i];
        float temp = config.global_temperature;

        bool accept = (delta <= 0.0f) ||
                     (curand_uniform(&local_rng) < expf(-delta / fmaxf(EPSILON, temp)));

        if (accept) {
            // Acquire lock
            if (atomicCAS(&state->locks[i], 0, 1) == 0) {
                state->coloring_L0[i] = new_color;
                atomicExch(&state->locks[i], 0);

                if (gid == blockIdx.x * blockDim.x) {
                    atomicAdd(&telemetry->moves_applied, 1);
                }
            }
        }
    }

    block.sync();

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 8: WRITE BACK TO GLOBAL MEMORY (Vectorized)
    // ═══════════════════════════════════════════════════════════════════════

    // Coalesced write-back of coloring
    for (int i = tid; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i += blockDim.x) {
        int v = block_start + i;
        if (v < num_vertices) {
            colors[v] = state->coloring_L0[i];
        }
    }

    // Save RNG state
    rng_states[gid] = local_rng;

    // ═══════════════════════════════════════════════════════════════════════
    // PHASE 9: GLOBAL TELEMETRY COLLECTION (Grid-wide reduction)
    // ═══════════════════════════════════════════════════════════════════════

    grid.sync(); // Cooperative kernel synchronization

    // Block 0 computes global telemetry
    if (bid == 0 && tid == 0) {
        int total_conflicts = 0;
        int max_color = 0;

        for (int v = 0; v < num_vertices; v++) {
            int c = colors[v];
            if (c > max_color) max_color = c;

            int start = row_ptr[v];
            int end = row_ptr[v + 1];

            for (int e = start; e < end; e++) {
                if (colors[col_idx[e]] == c) {
                    total_conflicts++;
                }
            }
        }

        telemetry->conflicts = total_conflicts / 2;
        telemetry->colors_used = max_color + 1;

        // Copy TPTP metrics
        telemetry->betti_numbers[0] = state->tda.betti[0];
        telemetry->betti_numbers[1] = state->tda.betti[1];
        telemetry->betti_numbers[2] = state->tda.betti[2];

        // Compute average reservoir activity
        float total_activity = 0.0f;
        int active_count = 0;
        for (int i = 0; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i++) {
            if (state->spike_history[i] > 0.1f) {
                total_activity += state->spike_history[i];
                active_count++;
            }
        }
        telemetry->reservoir_activity = (active_count > 0) ?
            (total_activity / (float)active_count) : 0.0f;

        // Compute average free energy
        float total_fe = 0.0f;
        for (int i = 0; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i++) {
            total_fe += state->expected_free_energy[i];
        }
        telemetry->free_energy = total_fe / fmaxf(1.0f,
            (float)min(block_vertex_count, MAX_VERTICES_PER_BLOCK));

        // Find best replica for parallel tempering
        if (tempering_enabled(&config)) {
            int best_replica = 0;
            int min_conflicts = INT_MAX;
            for (int r = 0; r < min(config.num_replicas, NUM_REPLICAS); r++) {
                if (state->replica[r].conflicts < min_conflicts) {
                    min_conflicts = state->replica[r].conflicts;
                    best_replica = r;
                }
            }
            telemetry->best_replica = best_replica;
        }
    }
}

// ═══════════════════════════════════════════════════════════════════════════
// HELPER KERNELS
// ═══════════════════════════════════════════════════════════════════════════

/**
 * ultra_init_kernel - Initialize all GPU state for optimization
 */
extern "C" __global__ void ultra_init_kernel(
    const int* __restrict__ row_ptr,
    const int* __restrict__ col_idx,
    int* __restrict__ colors,
    int* __restrict__ best_colors,
    int num_vertices,
    int num_edges,
    unsigned long long seed
) {
    int gid = blockIdx.x * blockDim.x + threadIdx.x;

    curandState rng;
    curand_init(seed, gid, 0, &rng);

    // Greedy coloring initialization
    for (int v = gid; v < num_vertices; v += blockDim.x * gridDim.x) {
        int used_colors[64] = {0};

        int start = row_ptr[v];
        int end = row_ptr[v + 1];

        // Mark neighbor colors
        for (int e = start; e < end; e++) {
            int neighbor = col_idx[e];
            if (neighbor < v) {
                int n_color = colors[neighbor];
                if (n_color >= 0 && n_color < 64) {
                    used_colors[n_color] = 1;
                }
            }
        }

        // Find first available color
        int assigned_color = 0;
        for (int c = 0; c < 64; c++) {
            if (!used_colors[c]) {
                assigned_color = c;
                break;
            }
        }

        colors[v] = assigned_color;
        best_colors[v] = assigned_color;
    }
}

/**
 * ultra_finalize_kernel - Final validation and compaction
 */
extern "C" __global__ void ultra_finalize_kernel(
    const int* __restrict__ row_ptr,
    const int* __restrict__ col_idx,
    int* __restrict__ colors,
    int* __restrict__ conflict_map,
    int num_vertices
) {
    int gid = blockIdx.x * blockDim.x + threadIdx.x;

    for (int v = gid; v < num_vertices; v += blockDim.x * gridDim.x) {
        int my_color = colors[v];
        int conflicts = 0;

        int start = row_ptr[v];
        int end = row_ptr[v + 1];

        for (int e = start; e < end; e++) {
            if (colors[col_idx[e]] == my_color) {
                conflicts++;
            }
        }

        conflict_map[v] = conflicts;
    }
}

/**
 * ultra_telemetry_kernel - Comprehensive telemetry collection
 */
extern "C" __global__ void ultra_telemetry_kernel(
    const int* __restrict__ row_ptr,
    const int* __restrict__ col_idx,
    const int* __restrict__ colors,
    KernelTelemetry* __restrict__ telemetry,
    int num_vertices
) {
    __shared__ int s_conflicts;
    __shared__ int s_max_color;

    int tid = threadIdx.x;
    int gid = blockIdx.x * blockDim.x + threadIdx.x;

    if (tid == 0) {
        s_conflicts = 0;
        s_max_color = 0;
    }
    __syncthreads();

    // Accumulate metrics
    for (int v = gid; v < num_vertices; v += blockDim.x * gridDim.x) {
        int my_color = colors[v];
        atomicMax(&s_max_color, my_color);

        int start = row_ptr[v];
        int end = row_ptr[v + 1];

        int vertex_conflicts = 0;
        for (int e = start; e < end; e++) {
            if (colors[col_idx[e]] == my_color) {
                vertex_conflicts++;
            }
        }

        atomicAdd(&s_conflicts, vertex_conflicts);
    }
    __syncthreads();

    if (tid == 0) {
        atomicAdd(&telemetry->conflicts, s_conflicts / 2);
        atomicMax(&telemetry->colors_used, s_max_color + 1);
    }
}

// ═══════════════════════════════════════════════════════════════════════════
// END OF PART 3: ULTRA KERNEL ENTRY POINTS & ORCHESTRATION
// ═══════════════════════════════════════════════════════════════════════════
