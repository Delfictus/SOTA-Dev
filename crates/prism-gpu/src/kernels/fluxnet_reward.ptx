//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-37061995
// Cuda compilation tools, release 13.1, V13.1.115
// Based on NVVM 21.0.0
//

.version 9.1
.target sm_120
.address_size 64

	// .globl	compute_batch_rewards
// _ZZ19aggregate_gradientsE12shared_grads has been demoted
// _ZZ24compute_batch_statisticsE14shared_rewards has been demoted
// _ZZ24compute_batch_statisticsE16shared_distances has been demoted
// _ZZ17normalize_weightsE3sum has been demoted

.visible .entry compute_batch_rewards(
	.param .u64 .ptr .align 1 compute_batch_rewards_param_0,
	.param .u64 .ptr .align 1 compute_batch_rewards_param_1,
	.param .u64 .ptr .align 1 compute_batch_rewards_param_2,
	.param .u32 compute_batch_rewards_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd1, [compute_batch_rewards_param_0];
	ld.param.b64 	%rd2, [compute_batch_rewards_param_2];
	ld.param.b32 	%r2, [compute_batch_rewards_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB0_2;
	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	shl.b32 	%r6, %r1, 3;
	mul.wide.s32 	%rd5, %r6, 4;
	add.s64 	%rd6, %rd3, %rd5;
	ld.global.nc.b32 	%r7, [%rd6];
	ld.global.nc.b32 	%r8, [%rd6+8];
	ld.global.nc.b32 	%r10, [%rd6+12];
	mov.b32 	%r12, 0f40800000;
	div.approx.ftz.f32 	%r13, %r7, %r12;
	mov.b32 	%r14, 0f3F800000;
	sub.ftz.f32 	%r15, %r14, %r13;
	max.ftz.f32 	%r16, %r15, 0f00000000;
	mul.ftz.f32 	%r17, %r16, 0f3F000000;
	fma.rn.ftz.f32 	%r18, %r8, 0f3E99999A, %r17;
	fma.rn.ftz.f32 	%r19, %r10, 0f3E4CCCCD, %r18;
	setp.gt.ftz.f32 	%p2, %r16, 0f3F4CCCCD;
	setp.gt.ftz.f32 	%p3, %r8, 0f3F333333;
	setp.gt.ftz.f32 	%p4, %r10, 0f3F19999A;
	add.ftz.f32 	%r20, %r19, 0f3DCCCCCD;
	selp.f32 	%r21, %r20, %r19, %p4;
	selp.f32 	%r22, %r21, %r19, %p3;
	selp.f32 	%r23, %r22, %r19, %p2;
	max.ftz.f32 	%r24, %r23, 0f00000000;
	min.ftz.f32 	%r25, %r24, 0f3F800000;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd4, %rd7;
	st.global.b32 	[%rd8], %r25;
$L__BB0_2:
	ret;

}
	// .globl	compute_gradients
.visible .entry compute_gradients(
	.param .u64 .ptr .align 1 compute_gradients_param_0,
	.param .u64 .ptr .align 1 compute_gradients_param_1,
	.param .u64 .ptr .align 1 compute_gradients_param_2,
	.param .f32 compute_gradients_param_3,
	.param .f32 compute_gradients_param_4,
	.param .u32 compute_gradients_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<12>;

	ld.param.b64 	%rd1, [compute_gradients_param_0];
	ld.param.b64 	%rd2, [compute_gradients_param_1];
	ld.param.b64 	%rd3, [compute_gradients_param_2];
	ld.param.b32 	%r2, [compute_gradients_param_3];
	ld.param.b32 	%r3, [compute_gradients_param_4];
	ld.param.b32 	%r4, [compute_gradients_param_5];
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	setp.ge.s32 	%p1, %r1, %r4;
	@%p1 bra 	$L__BB1_2;
	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd3;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.nc.b32 	%r8, [%rd8];
	sub.ftz.f32 	%r9, %r8, %r2;
	mul.ftz.f32 	%r10, %r3, %r9;
	shl.b32 	%r11, %r1, 3;
	mul.wide.s32 	%rd9, %r11, 4;
	add.s64 	%rd10, %rd4, %rd9;
	ld.global.nc.b32 	%r12, [%rd10];
	mul.ftz.f32 	%r13, %r10, %r12;
	add.s64 	%rd11, %rd5, %rd9;
	st.global.b32 	[%rd11], %r13;
	ld.global.nc.b32 	%r14, [%rd10+4];
	mul.ftz.f32 	%r15, %r10, %r14;
	st.global.b32 	[%rd11+4], %r15;
	ld.global.nc.b32 	%r16, [%rd10+8];
	mul.ftz.f32 	%r17, %r10, %r16;
	st.global.b32 	[%rd11+8], %r17;
	ld.global.nc.b32 	%r18, [%rd10+12];
	mul.ftz.f32 	%r19, %r10, %r18;
	st.global.b32 	[%rd11+12], %r19;
	ld.global.nc.b32 	%r20, [%rd10+16];
	mul.ftz.f32 	%r21, %r10, %r20;
	st.global.b32 	[%rd11+16], %r21;
	ld.global.nc.b32 	%r22, [%rd10+20];
	mul.ftz.f32 	%r23, %r10, %r22;
	st.global.b32 	[%rd11+20], %r23;
	ld.global.nc.b32 	%r24, [%rd10+24];
	mul.ftz.f32 	%r25, %r10, %r24;
	st.global.b32 	[%rd11+24], %r25;
	ld.global.nc.b32 	%r26, [%rd10+28];
	mul.ftz.f32 	%r27, %r10, %r26;
	st.global.b32 	[%rd11+28], %r27;
$L__BB1_2:
	ret;

}
	// .globl	aggregate_gradients
.visible .entry aggregate_gradients(
	.param .u64 .ptr .align 1 aggregate_gradients_param_0,
	.param .u64 .ptr .align 1 aggregate_gradients_param_1,
	.param .u32 aggregate_gradients_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<33>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 4 .b8 _ZZ19aggregate_gradientsE12shared_grads[8192];
	ld.param.b64 	%rd2, [aggregate_gradients_param_0];
	ld.param.b64 	%rd3, [aggregate_gradients_param_1];
	ld.param.b32 	%r13, [aggregate_gradients_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	setp.gt.u32 	%p1, %r2, 7;
	@%p1 bra 	$L__BB2_10;
	setp.ge.s32 	%p2, %r1, %r13;
	mov.b32 	%r31, 0f00000000;
	@%p2 bra 	$L__BB2_4;
	mov.u32 	%r3, %ntid.x;
	cvta.to.global.u64 	%rd1, %rd2;
	mov.b32 	%r31, 0f00000000;
	mov.b32 	%r29, %r1;
$L__BB2_3:
	shl.b32 	%r16, %r29, 3;
	add.s32 	%r17, %r16, %r2;
	mul.wide.s32 	%rd4, %r17, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.nc.b32 	%r18, [%rd5];
	add.ftz.f32 	%r31, %r31, %r18;
	add.s32 	%r29, %r29, %r3;
	setp.lt.s32 	%p3, %r29, %r13;
	@%p3 bra 	$L__BB2_3;
$L__BB2_4:
	shl.b32 	%r19, %r1, 2;
	mov.b32 	%r20, _ZZ19aggregate_gradientsE12shared_grads;
	add.s32 	%r9, %r20, %r19;
	st.shared.b32 	[%r9], %r31;
	bar.sync 	0;
	mov.u32 	%r32, %ntid.x;
	setp.lt.u32 	%p4, %r32, 2;
	@%p4 bra 	$L__BB2_8;
$L__BB2_5:
	shr.u32 	%r12, %r32, 1;
	setp.ge.u32 	%p5, %r1, %r12;
	@%p5 bra 	$L__BB2_7;
	shl.b32 	%r21, %r12, 2;
	add.s32 	%r22, %r9, %r21;
	ld.shared.b32 	%r23, [%r22];
	ld.shared.b32 	%r24, [%r9];
	add.ftz.f32 	%r25, %r23, %r24;
	st.shared.b32 	[%r9], %r25;
$L__BB2_7:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r32, 3;
	mov.b32 	%r32, %r12;
	@%p6 bra 	$L__BB2_5;
$L__BB2_8:
	setp.ne.s32 	%p7, %r1, 0;
	@%p7 bra 	$L__BB2_10;
	ld.shared.b32 	%r26, [_ZZ19aggregate_gradientsE12shared_grads];
	cvt.rn.f32.s32 	%r27, %r13;
	div.approx.ftz.f32 	%r28, %r26, %r27;
	cvta.to.global.u64 	%rd6, %rd3;
	mul.wide.u32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.b32 	[%rd8], %r28;
$L__BB2_10:
	ret;

}
	// .globl	compute_batch_statistics
.visible .entry compute_batch_statistics(
	.param .u64 .ptr .align 1 compute_batch_statistics_param_0,
	.param .u64 .ptr .align 1 compute_batch_statistics_param_1,
	.param .u64 .ptr .align 1 compute_batch_statistics_param_2,
	.param .u32 compute_batch_statistics_param_3
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<462>;
	.reg .b64 	%rd<10>;
	// demoted variable
	.shared .align 4 .b8 _ZZ24compute_batch_statisticsE14shared_rewards[1024];
	// demoted variable
	.shared .align 4 .b8 _ZZ24compute_batch_statisticsE16shared_distances[1024];
	ld.param.b64 	%rd1, [compute_batch_statistics_param_0];
	ld.param.b64 	%rd2, [compute_batch_statistics_param_1];
	ld.param.b64 	%rd3, [compute_batch_statistics_param_2];
	ld.param.b32 	%r135, [compute_batch_statistics_param_3];
	mov.u32 	%r1, %tid.x;
	setp.ge.s32 	%p1, %r1, %r135;
	mov.b32 	%r384, 0f00000000;
	mov.b32 	%r385, %r384;
	@%p1 bra 	$L__BB3_2;
	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.u32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd4, %rd6;
	ld.global.nc.b32 	%r385, [%rd7];
	add.s64 	%rd8, %rd5, %rd6;
	ld.global.nc.b32 	%r384, [%rd8];
$L__BB3_2:
	shl.b32 	%r137, %r1, 2;
	mov.b32 	%r138, _ZZ24compute_batch_statisticsE14shared_rewards;
	add.s32 	%r139, %r138, %r137;
	st.shared.b32 	[%r139], %r385;
	mov.b32 	%r140, _ZZ24compute_batch_statisticsE16shared_distances;
	add.s32 	%r141, %r140, %r137;
	st.shared.b32 	[%r141], %r384;
	bar.sync 	0;
	setp.ne.s32 	%p2, %r1, 0;
	@%p2 bra 	$L__BB3_28;
	setp.lt.s32 	%p3, %r135, 1;
	mov.b32 	%r436, 0f00000000;
	mov.b32 	%r435, 0f501502F9;
	mov.b32 	%r434, 0fD01502F9;
	mov.b32 	%r437, %r436;
	mov.b32 	%r438, %r435;
	mov.b32 	%r439, %r434;
	@%p3 bra 	$L__BB3_15;
	and.b32 	%r6, %r135, 7;
	setp.lt.u32 	%p4, %r135, 8;
	mov.b32 	%r437, 0f00000000;
	mov.b32 	%r438, 0f501502F9;
	mov.b32 	%r439, 0fD01502F9;
	mov.b32 	%r414, 0;
	mov.b32 	%r436, %r437;
	mov.b32 	%r435, %r438;
	mov.b32 	%r434, %r439;
	@%p4 bra 	$L__BB3_7;
	and.b32 	%r414, %r135, 2147483640;
	mov.b32 	%r155, _ZZ24compute_batch_statisticsE14shared_rewards;
	add.s32 	%r387, %r155, 16;
	mov.b32 	%r156, _ZZ24compute_batch_statisticsE16shared_distances;
	add.s32 	%r386, %r156, 16;
	neg.s32 	%r388, %r414;
	mov.b32 	%r437, 0f00000000;
	mov.b32 	%r438, 0f501502F9;
	mov.b32 	%r439, 0fD01502F9;
$L__BB3_6:
	.pragma "nounroll";
	ld.shared.b32 	%r157, [%r387+-16];
	add.ftz.f32 	%r158, %r437, %r157;
	ld.shared.b32 	%r159, [%r386+-16];
	add.ftz.f32 	%r160, %r436, %r159;
	min.ftz.f32 	%r161, %r435, %r157;
	max.ftz.f32 	%r162, %r434, %r157;
	min.ftz.f32 	%r163, %r438, %r159;
	max.ftz.f32 	%r164, %r439, %r159;
	ld.shared.b32 	%r165, [%r387+-12];
	add.ftz.f32 	%r166, %r158, %r165;
	ld.shared.b32 	%r167, [%r386+-12];
	add.ftz.f32 	%r168, %r160, %r167;
	min.ftz.f32 	%r169, %r161, %r165;
	max.ftz.f32 	%r170, %r162, %r165;
	min.ftz.f32 	%r171, %r163, %r167;
	max.ftz.f32 	%r172, %r164, %r167;
	ld.shared.b32 	%r173, [%r387+-8];
	add.ftz.f32 	%r174, %r166, %r173;
	ld.shared.b32 	%r175, [%r386+-8];
	add.ftz.f32 	%r176, %r168, %r175;
	min.ftz.f32 	%r177, %r169, %r173;
	max.ftz.f32 	%r178, %r170, %r173;
	min.ftz.f32 	%r179, %r171, %r175;
	max.ftz.f32 	%r180, %r172, %r175;
	ld.shared.b32 	%r181, [%r387+-4];
	add.ftz.f32 	%r182, %r174, %r181;
	ld.shared.b32 	%r183, [%r386+-4];
	add.ftz.f32 	%r184, %r176, %r183;
	min.ftz.f32 	%r185, %r177, %r181;
	max.ftz.f32 	%r186, %r178, %r181;
	min.ftz.f32 	%r187, %r179, %r183;
	max.ftz.f32 	%r188, %r180, %r183;
	ld.shared.b32 	%r189, [%r387];
	add.ftz.f32 	%r190, %r182, %r189;
	ld.shared.b32 	%r191, [%r386];
	add.ftz.f32 	%r192, %r184, %r191;
	min.ftz.f32 	%r193, %r185, %r189;
	max.ftz.f32 	%r194, %r186, %r189;
	min.ftz.f32 	%r195, %r187, %r191;
	max.ftz.f32 	%r196, %r188, %r191;
	ld.shared.b32 	%r197, [%r387+4];
	add.ftz.f32 	%r198, %r190, %r197;
	ld.shared.b32 	%r199, [%r386+4];
	add.ftz.f32 	%r200, %r192, %r199;
	min.ftz.f32 	%r201, %r193, %r197;
	max.ftz.f32 	%r202, %r194, %r197;
	min.ftz.f32 	%r203, %r195, %r199;
	max.ftz.f32 	%r204, %r196, %r199;
	ld.shared.b32 	%r205, [%r387+8];
	add.ftz.f32 	%r206, %r198, %r205;
	ld.shared.b32 	%r207, [%r386+8];
	add.ftz.f32 	%r208, %r200, %r207;
	min.ftz.f32 	%r209, %r201, %r205;
	max.ftz.f32 	%r210, %r202, %r205;
	min.ftz.f32 	%r211, %r203, %r207;
	max.ftz.f32 	%r212, %r204, %r207;
	ld.shared.b32 	%r213, [%r387+12];
	add.ftz.f32 	%r437, %r206, %r213;
	ld.shared.b32 	%r214, [%r386+12];
	add.ftz.f32 	%r436, %r208, %r214;
	min.ftz.f32 	%r435, %r209, %r213;
	max.ftz.f32 	%r434, %r210, %r213;
	min.ftz.f32 	%r438, %r211, %r214;
	max.ftz.f32 	%r439, %r212, %r214;
	add.s32 	%r388, %r388, 8;
	add.s32 	%r387, %r387, 32;
	add.s32 	%r386, %r386, 32;
	setp.ne.s32 	%p5, %r388, 0;
	@%p5 bra 	$L__BB3_6;
$L__BB3_7:
	setp.eq.s32 	%p6, %r6, 0;
	@%p6 bra 	$L__BB3_15;
	and.b32 	%r40, %r135, 3;
	setp.lt.u32 	%p7, %r6, 4;
	@%p7 bra 	$L__BB3_10;
	shl.b32 	%r216, %r414, 2;
	mov.b32 	%r217, _ZZ24compute_batch_statisticsE14shared_rewards;
	add.s32 	%r218, %r217, %r216;
	ld.shared.b32 	%r219, [%r218];
	add.ftz.f32 	%r220, %r437, %r219;
	mov.b32 	%r221, _ZZ24compute_batch_statisticsE16shared_distances;
	add.s32 	%r222, %r221, %r216;
	ld.shared.b32 	%r223, [%r222];
	add.ftz.f32 	%r224, %r436, %r223;
	min.ftz.f32 	%r225, %r435, %r219;
	max.ftz.f32 	%r226, %r434, %r219;
	min.ftz.f32 	%r227, %r438, %r223;
	max.ftz.f32 	%r228, %r439, %r223;
	ld.shared.b32 	%r229, [%r218+4];
	add.ftz.f32 	%r230, %r220, %r229;
	ld.shared.b32 	%r231, [%r222+4];
	add.ftz.f32 	%r232, %r224, %r231;
	min.ftz.f32 	%r233, %r225, %r229;
	max.ftz.f32 	%r234, %r226, %r229;
	min.ftz.f32 	%r235, %r227, %r231;
	max.ftz.f32 	%r236, %r228, %r231;
	ld.shared.b32 	%r237, [%r218+8];
	add.ftz.f32 	%r238, %r230, %r237;
	ld.shared.b32 	%r239, [%r222+8];
	add.ftz.f32 	%r240, %r232, %r239;
	min.ftz.f32 	%r241, %r233, %r237;
	max.ftz.f32 	%r242, %r234, %r237;
	min.ftz.f32 	%r243, %r235, %r239;
	max.ftz.f32 	%r244, %r236, %r239;
	ld.shared.b32 	%r245, [%r218+12];
	add.ftz.f32 	%r437, %r238, %r245;
	ld.shared.b32 	%r246, [%r222+12];
	add.ftz.f32 	%r436, %r240, %r246;
	min.ftz.f32 	%r435, %r241, %r245;
	max.ftz.f32 	%r434, %r242, %r245;
	min.ftz.f32 	%r438, %r243, %r246;
	max.ftz.f32 	%r439, %r244, %r246;
	add.s32 	%r414, %r414, 4;
$L__BB3_10:
	setp.eq.s32 	%p8, %r40, 0;
	@%p8 bra 	$L__BB3_15;
	setp.eq.s32 	%p9, %r40, 1;
	@%p9 bra 	$L__BB3_13;
	shl.b32 	%r248, %r414, 2;
	mov.b32 	%r249, _ZZ24compute_batch_statisticsE14shared_rewards;
	add.s32 	%r250, %r249, %r248;
	ld.shared.b32 	%r251, [%r250];
	add.ftz.f32 	%r252, %r437, %r251;
	mov.b32 	%r253, _ZZ24compute_batch_statisticsE16shared_distances;
	add.s32 	%r254, %r253, %r248;
	ld.shared.b32 	%r255, [%r254];
	add.ftz.f32 	%r256, %r436, %r255;
	min.ftz.f32 	%r257, %r435, %r251;
	max.ftz.f32 	%r258, %r434, %r251;
	min.ftz.f32 	%r259, %r438, %r255;
	max.ftz.f32 	%r260, %r439, %r255;
	ld.shared.b32 	%r261, [%r250+4];
	add.ftz.f32 	%r437, %r252, %r261;
	ld.shared.b32 	%r262, [%r254+4];
	add.ftz.f32 	%r436, %r256, %r262;
	min.ftz.f32 	%r435, %r257, %r261;
	max.ftz.f32 	%r434, %r258, %r261;
	min.ftz.f32 	%r438, %r259, %r262;
	max.ftz.f32 	%r439, %r260, %r262;
	add.s32 	%r414, %r414, 2;
$L__BB3_13:
	and.b32 	%r263, %r135, 1;
	setp.ne.b32 	%p10, %r263, 0;
	not.pred 	%p11, %p10;
	@%p11 bra 	$L__BB3_15;
	shl.b32 	%r264, %r414, 2;
	mov.b32 	%r265, _ZZ24compute_batch_statisticsE14shared_rewards;
	add.s32 	%r266, %r265, %r264;
	ld.shared.b32 	%r267, [%r266];
	add.ftz.f32 	%r437, %r437, %r267;
	mov.b32 	%r268, _ZZ24compute_batch_statisticsE16shared_distances;
	add.s32 	%r269, %r268, %r264;
	ld.shared.b32 	%r270, [%r269];
	add.ftz.f32 	%r436, %r436, %r270;
	min.ftz.f32 	%r435, %r435, %r267;
	max.ftz.f32 	%r434, %r434, %r267;
	min.ftz.f32 	%r438, %r438, %r270;
	max.ftz.f32 	%r439, %r439, %r270;
$L__BB3_15:
	setp.lt.s32 	%p12, %r135, 1;
	cvt.rn.f32.s32 	%r93, %r135;
	div.approx.ftz.f32 	%r94, %r437, %r93;
	div.approx.ftz.f32 	%r95, %r436, %r93;
	mov.b32 	%r460, 0f00000000;
	mov.b32 	%r461, %r460;
	@%p12 bra 	$L__BB3_27;
	and.b32 	%r96, %r135, 7;
	setp.lt.u32 	%p13, %r135, 8;
	mov.b32 	%r461, 0f00000000;
	mov.b32 	%r452, 0;
	mov.b32 	%r460, %r461;
	@%p13 bra 	$L__BB3_19;
	and.b32 	%r452, %r135, 2147483640;
	mov.b32 	%r278, _ZZ24compute_batch_statisticsE14shared_rewards;
	add.s32 	%r441, %r278, 16;
	mov.b32 	%r279, _ZZ24compute_batch_statisticsE16shared_distances;
	add.s32 	%r440, %r279, 16;
	neg.s32 	%r442, %r452;
	mov.b32 	%r461, 0f00000000;
$L__BB3_18:
	.pragma "nounroll";
	ld.shared.b32 	%r280, [%r441+-16];
	sub.ftz.f32 	%r281, %r280, %r94;
	ld.shared.b32 	%r282, [%r440+-16];
	sub.ftz.f32 	%r283, %r282, %r95;
	fma.rn.ftz.f32 	%r284, %r281, %r281, %r460;
	fma.rn.ftz.f32 	%r285, %r283, %r283, %r461;
	ld.shared.b32 	%r286, [%r441+-12];
	sub.ftz.f32 	%r287, %r286, %r94;
	ld.shared.b32 	%r288, [%r440+-12];
	sub.ftz.f32 	%r289, %r288, %r95;
	fma.rn.ftz.f32 	%r290, %r287, %r287, %r284;
	fma.rn.ftz.f32 	%r291, %r289, %r289, %r285;
	ld.shared.b32 	%r292, [%r441+-8];
	sub.ftz.f32 	%r293, %r292, %r94;
	ld.shared.b32 	%r294, [%r440+-8];
	sub.ftz.f32 	%r295, %r294, %r95;
	fma.rn.ftz.f32 	%r296, %r293, %r293, %r290;
	fma.rn.ftz.f32 	%r297, %r295, %r295, %r291;
	ld.shared.b32 	%r298, [%r441+-4];
	sub.ftz.f32 	%r299, %r298, %r94;
	ld.shared.b32 	%r300, [%r440+-4];
	sub.ftz.f32 	%r301, %r300, %r95;
	fma.rn.ftz.f32 	%r302, %r299, %r299, %r296;
	fma.rn.ftz.f32 	%r303, %r301, %r301, %r297;
	ld.shared.b32 	%r304, [%r441];
	sub.ftz.f32 	%r305, %r304, %r94;
	ld.shared.b32 	%r306, [%r440];
	sub.ftz.f32 	%r307, %r306, %r95;
	fma.rn.ftz.f32 	%r308, %r305, %r305, %r302;
	fma.rn.ftz.f32 	%r309, %r307, %r307, %r303;
	ld.shared.b32 	%r310, [%r441+4];
	sub.ftz.f32 	%r311, %r310, %r94;
	ld.shared.b32 	%r312, [%r440+4];
	sub.ftz.f32 	%r313, %r312, %r95;
	fma.rn.ftz.f32 	%r314, %r311, %r311, %r308;
	fma.rn.ftz.f32 	%r315, %r313, %r313, %r309;
	ld.shared.b32 	%r316, [%r441+8];
	sub.ftz.f32 	%r317, %r316, %r94;
	ld.shared.b32 	%r318, [%r440+8];
	sub.ftz.f32 	%r319, %r318, %r95;
	fma.rn.ftz.f32 	%r320, %r317, %r317, %r314;
	fma.rn.ftz.f32 	%r321, %r319, %r319, %r315;
	ld.shared.b32 	%r322, [%r441+12];
	sub.ftz.f32 	%r323, %r322, %r94;
	ld.shared.b32 	%r324, [%r440+12];
	sub.ftz.f32 	%r325, %r324, %r95;
	fma.rn.ftz.f32 	%r460, %r323, %r323, %r320;
	fma.rn.ftz.f32 	%r461, %r325, %r325, %r321;
	add.s32 	%r442, %r442, 8;
	add.s32 	%r441, %r441, 32;
	add.s32 	%r440, %r440, 32;
	setp.ne.s32 	%p14, %r442, 0;
	@%p14 bra 	$L__BB3_18;
$L__BB3_19:
	setp.eq.s32 	%p15, %r96, 0;
	@%p15 bra 	$L__BB3_27;
	and.b32 	%r114, %r135, 3;
	setp.lt.u32 	%p16, %r96, 4;
	@%p16 bra 	$L__BB3_22;
	shl.b32 	%r327, %r452, 2;
	mov.b32 	%r328, _ZZ24compute_batch_statisticsE14shared_rewards;
	add.s32 	%r329, %r328, %r327;
	ld.shared.b32 	%r330, [%r329];
	sub.ftz.f32 	%r331, %r330, %r94;
	mov.b32 	%r332, _ZZ24compute_batch_statisticsE16shared_distances;
	add.s32 	%r333, %r332, %r327;
	ld.shared.b32 	%r334, [%r333];
	sub.ftz.f32 	%r335, %r334, %r95;
	fma.rn.ftz.f32 	%r336, %r331, %r331, %r460;
	fma.rn.ftz.f32 	%r337, %r335, %r335, %r461;
	ld.shared.b32 	%r338, [%r329+4];
	sub.ftz.f32 	%r339, %r338, %r94;
	ld.shared.b32 	%r340, [%r333+4];
	sub.ftz.f32 	%r341, %r340, %r95;
	fma.rn.ftz.f32 	%r342, %r339, %r339, %r336;
	fma.rn.ftz.f32 	%r343, %r341, %r341, %r337;
	ld.shared.b32 	%r344, [%r329+8];
	sub.ftz.f32 	%r345, %r344, %r94;
	ld.shared.b32 	%r346, [%r333+8];
	sub.ftz.f32 	%r347, %r346, %r95;
	fma.rn.ftz.f32 	%r348, %r345, %r345, %r342;
	fma.rn.ftz.f32 	%r349, %r347, %r347, %r343;
	ld.shared.b32 	%r350, [%r329+12];
	sub.ftz.f32 	%r351, %r350, %r94;
	ld.shared.b32 	%r352, [%r333+12];
	sub.ftz.f32 	%r353, %r352, %r95;
	fma.rn.ftz.f32 	%r460, %r351, %r351, %r348;
	fma.rn.ftz.f32 	%r461, %r353, %r353, %r349;
	add.s32 	%r452, %r452, 4;
$L__BB3_22:
	setp.eq.s32 	%p17, %r114, 0;
	@%p17 bra 	$L__BB3_27;
	setp.eq.s32 	%p18, %r114, 1;
	@%p18 bra 	$L__BB3_25;
	shl.b32 	%r355, %r452, 2;
	mov.b32 	%r356, _ZZ24compute_batch_statisticsE14shared_rewards;
	add.s32 	%r357, %r356, %r355;
	ld.shared.b32 	%r358, [%r357];
	sub.ftz.f32 	%r359, %r358, %r94;
	mov.b32 	%r360, _ZZ24compute_batch_statisticsE16shared_distances;
	add.s32 	%r361, %r360, %r355;
	ld.shared.b32 	%r362, [%r361];
	sub.ftz.f32 	%r363, %r362, %r95;
	fma.rn.ftz.f32 	%r364, %r359, %r359, %r460;
	fma.rn.ftz.f32 	%r365, %r363, %r363, %r461;
	ld.shared.b32 	%r366, [%r357+4];
	sub.ftz.f32 	%r367, %r366, %r94;
	ld.shared.b32 	%r368, [%r361+4];
	sub.ftz.f32 	%r369, %r368, %r95;
	fma.rn.ftz.f32 	%r460, %r367, %r367, %r364;
	fma.rn.ftz.f32 	%r461, %r369, %r369, %r365;
	add.s32 	%r452, %r452, 2;
$L__BB3_25:
	and.b32 	%r370, %r135, 1;
	setp.ne.b32 	%p19, %r370, 0;
	not.pred 	%p20, %p19;
	@%p20 bra 	$L__BB3_27;
	shl.b32 	%r371, %r452, 2;
	mov.b32 	%r372, _ZZ24compute_batch_statisticsE14shared_rewards;
	add.s32 	%r373, %r372, %r371;
	ld.shared.b32 	%r374, [%r373];
	sub.ftz.f32 	%r375, %r374, %r94;
	mov.b32 	%r376, _ZZ24compute_batch_statisticsE16shared_distances;
	add.s32 	%r377, %r376, %r371;
	ld.shared.b32 	%r378, [%r377];
	sub.ftz.f32 	%r379, %r378, %r95;
	fma.rn.ftz.f32 	%r460, %r375, %r375, %r460;
	fma.rn.ftz.f32 	%r461, %r379, %r379, %r461;
$L__BB3_27:
	cvta.to.global.u64 	%rd9, %rd3;
	st.global.b32 	[%rd9], %r94;
	div.approx.ftz.f32 	%r380, %r460, %r93;
	sqrt.approx.ftz.f32 	%r381, %r380;
	st.global.b32 	[%rd9+4], %r381;
	st.global.b32 	[%rd9+8], %r435;
	st.global.b32 	[%rd9+12], %r434;
	st.global.b32 	[%rd9+16], %r95;
	div.approx.ftz.f32 	%r382, %r461, %r93;
	sqrt.approx.ftz.f32 	%r383, %r382;
	st.global.b32 	[%rd9+20], %r383;
	st.global.b32 	[%rd9+24], %r438;
	st.global.b32 	[%rd9+28], %r439;
$L__BB3_28:
	ret;

}
	// .globl	update_weights_momentum
.visible .entry update_weights_momentum(
	.param .u64 .ptr .align 1 update_weights_momentum_param_0,
	.param .u64 .ptr .align 1 update_weights_momentum_param_1,
	.param .u64 .ptr .align 1 update_weights_momentum_param_2,
	.param .f32 update_weights_momentum_param_3,
	.param .f32 update_weights_momentum_param_4,
	.param .f32 update_weights_momentum_param_5,
	.param .f32 update_weights_momentum_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<11>;

	ld.param.b64 	%rd1, [update_weights_momentum_param_0];
	ld.param.b64 	%rd2, [update_weights_momentum_param_1];
	ld.param.b64 	%rd3, [update_weights_momentum_param_2];
	ld.param.b32 	%r2, [update_weights_momentum_param_3];
	ld.param.b32 	%r3, [update_weights_momentum_param_4];
	ld.param.b32 	%r4, [update_weights_momentum_param_5];
	ld.param.b32 	%r5, [update_weights_momentum_param_6];
	mov.u32 	%r1, %tid.x;
	setp.gt.u32 	%p1, %r1, 7;
	@%p1 bra 	$L__BB4_2;
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.u32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd4, %rd7;
	ld.global.nc.b32 	%r6, [%rd8];
	add.s64 	%rd9, %rd5, %rd7;
	ld.global.b32 	%r7, [%rd9];
	mul.ftz.f32 	%r8, %r3, %r7;
	fma.rn.ftz.f32 	%r9, %r2, %r6, %r8;
	st.global.b32 	[%rd9], %r9;
	add.s64 	%rd10, %rd6, %rd7;
	ld.global.b32 	%r10, [%rd10];
	add.ftz.f32 	%r11, %r10, %r9;
	max.ftz.f32 	%r12, %r4, %r11;
	min.ftz.f32 	%r13, %r5, %r12;
	st.global.b32 	[%rd10], %r13;
$L__BB4_2:
	ret;

}
	// .globl	normalize_weights
.visible .entry normalize_weights(
	.param .u64 .ptr .align 1 normalize_weights_param_0,
	.param .u32 normalize_weights_param_1
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<109>;
	.reg .b64 	%rd<18>;
	// demoted variable
	.shared .align 4 .f32 _ZZ17normalize_weightsE3sum;
	ld.param.b64 	%rd8, [normalize_weights_param_0];
	ld.param.b32 	%r31, [normalize_weights_param_1];
	cvta.to.global.u64 	%rd1, %rd8;
	mov.u32 	%r1, %tid.x;
	setp.ne.s32 	%p1, %r1, 0;
	@%p1 bra 	$L__BB5_15;
	st.shared.b32 	[_ZZ17normalize_weightsE3sum], 0;
	setp.lt.s32 	%p2, %r31, 1;
	@%p2 bra 	$L__BB5_15;
	and.b32 	%r2, %r31, 15;
	setp.lt.u32 	%p3, %r31, 16;
	mov.b32 	%r102, 0;
	mov.b32 	%r108, 0f00000000;
	@%p3 bra 	$L__BB5_5;
	and.b32 	%r102, %r31, 2147483632;
	neg.s32 	%r95, %r102;
	add.s64 	%rd16, %rd1, 32;
	mov.b32 	%r108, 0f00000000;
$L__BB5_4:
	.pragma "nounroll";
	ld.global.b32 	%r36, [%rd16+-32];
	add.ftz.f32 	%r37, %r36, %r108;
	ld.global.b32 	%r38, [%rd16+-28];
	add.ftz.f32 	%r39, %r38, %r37;
	ld.global.b32 	%r40, [%rd16+-24];
	add.ftz.f32 	%r41, %r40, %r39;
	ld.global.b32 	%r42, [%rd16+-20];
	add.ftz.f32 	%r43, %r42, %r41;
	ld.global.b32 	%r44, [%rd16+-16];
	add.ftz.f32 	%r45, %r44, %r43;
	ld.global.b32 	%r46, [%rd16+-12];
	add.ftz.f32 	%r47, %r46, %r45;
	ld.global.b32 	%r48, [%rd16+-8];
	add.ftz.f32 	%r49, %r48, %r47;
	ld.global.b32 	%r50, [%rd16+-4];
	add.ftz.f32 	%r51, %r50, %r49;
	ld.global.b32 	%r52, [%rd16];
	add.ftz.f32 	%r53, %r52, %r51;
	ld.global.b32 	%r54, [%rd16+4];
	add.ftz.f32 	%r55, %r54, %r53;
	ld.global.b32 	%r56, [%rd16+8];
	add.ftz.f32 	%r57, %r56, %r55;
	ld.global.b32 	%r58, [%rd16+12];
	add.ftz.f32 	%r59, %r58, %r57;
	ld.global.b32 	%r60, [%rd16+16];
	add.ftz.f32 	%r61, %r60, %r59;
	ld.global.b32 	%r62, [%rd16+20];
	add.ftz.f32 	%r63, %r62, %r61;
	ld.global.b32 	%r64, [%rd16+24];
	add.ftz.f32 	%r65, %r64, %r63;
	ld.global.b32 	%r66, [%rd16+28];
	add.ftz.f32 	%r108, %r66, %r65;
	add.s32 	%r95, %r95, 16;
	add.s64 	%rd16, %rd16, 64;
	setp.ne.s32 	%p4, %r95, 0;
	@%p4 bra 	$L__BB5_4;
$L__BB5_5:
	setp.eq.s32 	%p5, %r2, 0;
	@%p5 bra 	$L__BB5_14;
	and.b32 	%r12, %r31, 7;
	setp.lt.u32 	%p6, %r2, 8;
	@%p6 bra 	$L__BB5_8;
	mul.wide.u32 	%rd9, %r102, 4;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.b32 	%r68, [%rd10];
	add.ftz.f32 	%r69, %r68, %r108;
	ld.global.b32 	%r70, [%rd10+4];
	add.ftz.f32 	%r71, %r70, %r69;
	ld.global.b32 	%r72, [%rd10+8];
	add.ftz.f32 	%r73, %r72, %r71;
	ld.global.b32 	%r74, [%rd10+12];
	add.ftz.f32 	%r75, %r74, %r73;
	ld.global.b32 	%r76, [%rd10+16];
	add.ftz.f32 	%r77, %r76, %r75;
	ld.global.b32 	%r78, [%rd10+20];
	add.ftz.f32 	%r79, %r78, %r77;
	ld.global.b32 	%r80, [%rd10+24];
	add.ftz.f32 	%r81, %r80, %r79;
	ld.global.b32 	%r82, [%rd10+28];
	add.ftz.f32 	%r108, %r82, %r81;
	add.s32 	%r102, %r102, 8;
$L__BB5_8:
	setp.eq.s32 	%p7, %r12, 0;
	@%p7 bra 	$L__BB5_14;
	and.b32 	%r18, %r31, 3;
	setp.lt.u32 	%p8, %r12, 4;
	@%p8 bra 	$L__BB5_11;
	mul.wide.u32 	%rd11, %r102, 4;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.b32 	%r84, [%rd12];
	add.ftz.f32 	%r85, %r84, %r108;
	ld.global.b32 	%r86, [%rd12+4];
	add.ftz.f32 	%r87, %r86, %r85;
	ld.global.b32 	%r88, [%rd12+8];
	add.ftz.f32 	%r89, %r88, %r87;
	ld.global.b32 	%r90, [%rd12+12];
	add.ftz.f32 	%r108, %r90, %r89;
	add.s32 	%r102, %r102, 4;
$L__BB5_11:
	setp.eq.s32 	%p9, %r18, 0;
	@%p9 bra 	$L__BB5_14;
	mul.wide.u32 	%rd13, %r102, 4;
	add.s64 	%rd17, %rd1, %rd13;
	neg.s32 	%r106, %r18;
$L__BB5_13:
	.pragma "nounroll";
	ld.global.b32 	%r91, [%rd17];
	add.ftz.f32 	%r108, %r91, %r108;
	add.s64 	%rd17, %rd17, 4;
	add.s32 	%r106, %r106, 1;
	setp.ne.s32 	%p10, %r106, 0;
	@%p10 bra 	$L__BB5_13;
$L__BB5_14:
	st.shared.b32 	[_ZZ17normalize_weightsE3sum], %r108;
$L__BB5_15:
	bar.sync 	0;
	setp.ge.s32 	%p11, %r1, %r31;
	ld.shared.b32 	%r92, [_ZZ17normalize_weightsE3sum];
	setp.leu.ftz.f32 	%p12, %r92, 0f00000000;
	or.pred 	%p13, %p11, %p12;
	@%p13 bra 	$L__BB5_17;
	mul.wide.u32 	%rd14, %r1, 4;
	add.s64 	%rd15, %rd1, %rd14;
	ld.global.b32 	%r93, [%rd15];
	div.approx.ftz.f32 	%r94, %r93, %r92;
	st.global.b32 	[%rd15], %r94;
$L__BB5_17:
	ret;

}
