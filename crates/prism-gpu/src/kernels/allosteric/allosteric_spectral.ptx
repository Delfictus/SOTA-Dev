//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-37061995
// Cuda compilation tools, release 13.1, V13.1.115
// Based on NVVM 21.0.0
//

.version 9.1
.target sm_120
.address_size 64

	// .globl	build_contact_matrix
// _ZZ20build_contact_matrixE13tile_coords_i has been demoted
// _ZZ20build_contact_matrixE13tile_coords_j has been demoted
// _ZZ15matvec_multiplyE8shared_x has been demoted
// _ZZ14vector_norm_sqE10shared_sum has been demoted

.visible .entry build_contact_matrix(
	.param .u64 .ptr .align 1 build_contact_matrix_param_0,
	.param .u64 .ptr .align 1 build_contact_matrix_param_1,
	.param .u32 build_contact_matrix_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<50>;
	.reg .b64 	%rd<11>;
	// demoted variable
	.shared .align 4 .b8 _ZZ20build_contact_matrixE13tile_coords_i[192];
	// demoted variable
	.shared .align 4 .b8 _ZZ20build_contact_matrixE13tile_coords_j[192];
	ld.param.b64 	%rd3, [build_contact_matrix_param_0];
	ld.param.b64 	%rd2, [build_contact_matrix_param_1];
	ld.param.b32 	%r10, [build_contact_matrix_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r11, %ctaid.y;
	shl.b32 	%r12, %r11, 4;
	mov.u32 	%r1, %tid.y;
	add.s32 	%r13, %r12, %r1;
	mov.u32 	%r14, %ctaid.x;
	shl.b32 	%r15, %r14, 4;
	mov.u32 	%r3, %tid.x;
	add.s32 	%r4, %r15, %r3;
	setp.gt.u32 	%p2, %r3, 2;
	setp.ge.s32 	%p3, %r13, %r10;
	mov.b32 	%r16, _ZZ20build_contact_matrixE13tile_coords_i;
	mad.lo.s32 	%r5, %r1, 12, %r16;
	or.pred 	%p4, %p2, %p3;
	@%p4 bra 	$L__BB0_2;
	mad.lo.s32 	%r17, %r13, 3, %r3;
	mul.wide.u32 	%rd4, %r17, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.nc.b32 	%r18, [%rd5];
	shl.b32 	%r19, %r3, 2;
	add.s32 	%r20, %r5, %r19;
	st.shared.b32 	[%r20], %r18;
$L__BB0_2:
	setp.gt.u32 	%p5, %r3, 2;
	setp.ge.s32 	%p6, %r4, %r10;
	or.pred 	%p7, %p5, %p6;
	@%p7 bra 	$L__BB0_4;
	mad.lo.s32 	%r21, %r4, 3, %r3;
	mul.wide.u32 	%rd6, %r21, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.nc.b32 	%r22, [%rd7];
	mov.b32 	%r23, _ZZ20build_contact_matrixE13tile_coords_j;
	mad.lo.s32 	%r24, %r1, 12, %r23;
	shl.b32 	%r25, %r3, 2;
	add.s32 	%r26, %r24, %r25;
	st.shared.b32 	[%r26], %r22;
$L__BB0_4:
	bar.sync 	0;
	max.s32 	%r28, %r13, %r4;
	setp.lt.s32 	%p1, %r28, %r10;
	setp.ge.s32 	%p8, %r28, %r10;
	setp.eq.s32 	%p9, %r13, %r4;
	or.pred 	%p10, %p9, %p8;
	mov.b32 	%r49, 0f00000000;
	@%p10 bra 	$L__BB0_7;
	ld.shared.b32 	%r30, [%r5];
	mov.b32 	%r31, _ZZ20build_contact_matrixE13tile_coords_j;
	mad.lo.s32 	%r32, %r3, 12, %r31;
	ld.shared.b32 	%r33, [%r32];
	sub.ftz.f32 	%r34, %r30, %r33;
	ld.shared.b32 	%r35, [%r5+4];
	ld.shared.b32 	%r36, [%r32+4];
	sub.ftz.f32 	%r37, %r35, %r36;
	ld.shared.b32 	%r38, [%r5+8];
	ld.shared.b32 	%r39, [%r32+8];
	sub.ftz.f32 	%r40, %r38, %r39;
	mul.ftz.f32 	%r41, %r37, %r37;
	fma.rn.ftz.f32 	%r42, %r34, %r34, %r41;
	fma.rn.ftz.f32 	%r6, %r40, %r40, %r42;
	setp.geu.ftz.f32 	%p11, %r6, 0f42C80000;
	@%p11 bra 	$L__BB0_7;
	neg.ftz.f32 	%r43, %r6;
	mov.b32 	%r44, 0f42900000;
	div.approx.ftz.f32 	%r45, %r43, %r44;
	mul.ftz.f32 	%r46, %r45, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%r49, %r46;
$L__BB0_7:
	not.pred 	%p12, %p1;
	@%p12 bra 	$L__BB0_9;
	mad.lo.s32 	%r47, %r10, %r13, %r4;
	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.s32 	%rd9, %r47, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.b32 	[%rd10], %r49;
$L__BB0_9:
	ret;

}
	// .globl	compute_degrees
.visible .entry compute_degrees(
	.param .u64 .ptr .align 1 compute_degrees_param_0,
	.param .u64 .ptr .align 1 compute_degrees_param_1,
	.param .u32 compute_degrees_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<119>;
	.reg .b64 	%rd<16>;

	ld.param.b64 	%rd4, [compute_degrees_param_0];
	ld.param.b64 	%rd3, [compute_degrees_param_1];
	ld.param.b32 	%r36, [compute_degrees_param_2];
	cvta.to.global.u64 	%rd1, %rd4;
	mov.u32 	%r37, %ctaid.x;
	mov.u32 	%r38, %ntid.x;
	mov.u32 	%r39, %tid.x;
	mad.lo.s32 	%r1, %r37, %r38, %r39;
	setp.ge.s32 	%p1, %r1, %r36;
	@%p1 bra 	$L__BB1_15;
	setp.lt.s32 	%p2, %r36, 1;
	mov.b32 	%r118, 0f00000000;
	@%p2 bra 	$L__BB1_14;
	mul.lo.s32 	%r2, %r36, %r1;
	and.b32 	%r3, %r36, 15;
	setp.lt.u32 	%p3, %r36, 16;
	mov.b32 	%r118, 0f00000000;
	mov.b32 	%r110, 0;
	@%p3 bra 	$L__BB1_5;
	and.b32 	%r110, %r36, 2147483632;
	neg.s32 	%r104, %r110;
	add.s64 	%rd2, %rd1, 32;
	mov.b32 	%r118, 0f00000000;
	mov.b32 	%r103, %r2;
$L__BB1_4:
	.pragma "nounroll";
	mul.wide.s32 	%rd5, %r103, 4;
	add.s64 	%rd6, %rd2, %rd5;
	ld.global.nc.b32 	%r45, [%rd6+-32];
	add.ftz.f32 	%r46, %r118, %r45;
	ld.global.nc.b32 	%r47, [%rd6+-28];
	add.ftz.f32 	%r48, %r46, %r47;
	ld.global.nc.b32 	%r49, [%rd6+-24];
	add.ftz.f32 	%r50, %r48, %r49;
	ld.global.nc.b32 	%r51, [%rd6+-20];
	add.ftz.f32 	%r52, %r50, %r51;
	ld.global.nc.b32 	%r53, [%rd6+-16];
	add.ftz.f32 	%r54, %r52, %r53;
	ld.global.nc.b32 	%r55, [%rd6+-12];
	add.ftz.f32 	%r56, %r54, %r55;
	ld.global.nc.b32 	%r57, [%rd6+-8];
	add.ftz.f32 	%r58, %r56, %r57;
	ld.global.nc.b32 	%r59, [%rd6+-4];
	add.ftz.f32 	%r60, %r58, %r59;
	ld.global.nc.b32 	%r61, [%rd6];
	add.ftz.f32 	%r62, %r60, %r61;
	ld.global.nc.b32 	%r63, [%rd6+4];
	add.ftz.f32 	%r64, %r62, %r63;
	ld.global.nc.b32 	%r65, [%rd6+8];
	add.ftz.f32 	%r66, %r64, %r65;
	ld.global.nc.b32 	%r67, [%rd6+12];
	add.ftz.f32 	%r68, %r66, %r67;
	ld.global.nc.b32 	%r69, [%rd6+16];
	add.ftz.f32 	%r70, %r68, %r69;
	ld.global.nc.b32 	%r71, [%rd6+20];
	add.ftz.f32 	%r72, %r70, %r71;
	ld.global.nc.b32 	%r73, [%rd6+24];
	add.ftz.f32 	%r74, %r72, %r73;
	ld.global.nc.b32 	%r75, [%rd6+28];
	add.ftz.f32 	%r118, %r74, %r75;
	add.s32 	%r104, %r104, 16;
	add.s32 	%r103, %r103, 16;
	setp.ne.s32 	%p4, %r104, 0;
	@%p4 bra 	$L__BB1_4;
$L__BB1_5:
	setp.eq.s32 	%p5, %r3, 0;
	@%p5 bra 	$L__BB1_14;
	and.b32 	%r15, %r36, 7;
	setp.lt.u32 	%p6, %r3, 8;
	@%p6 bra 	$L__BB1_8;
	add.s32 	%r77, %r110, %r2;
	mul.wide.s32 	%rd7, %r77, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.b32 	%r78, [%rd8];
	add.ftz.f32 	%r79, %r118, %r78;
	ld.global.nc.b32 	%r80, [%rd8+4];
	add.ftz.f32 	%r81, %r79, %r80;
	ld.global.nc.b32 	%r82, [%rd8+8];
	add.ftz.f32 	%r83, %r81, %r82;
	ld.global.nc.b32 	%r84, [%rd8+12];
	add.ftz.f32 	%r85, %r83, %r84;
	ld.global.nc.b32 	%r86, [%rd8+16];
	add.ftz.f32 	%r87, %r85, %r86;
	ld.global.nc.b32 	%r88, [%rd8+20];
	add.ftz.f32 	%r89, %r87, %r88;
	ld.global.nc.b32 	%r90, [%rd8+24];
	add.ftz.f32 	%r91, %r89, %r90;
	ld.global.nc.b32 	%r92, [%rd8+28];
	add.ftz.f32 	%r118, %r91, %r92;
	add.s32 	%r110, %r110, 8;
$L__BB1_8:
	setp.eq.s32 	%p7, %r15, 0;
	@%p7 bra 	$L__BB1_14;
	and.b32 	%r21, %r36, 3;
	setp.lt.u32 	%p8, %r15, 4;
	@%p8 bra 	$L__BB1_11;
	add.s32 	%r94, %r110, %r2;
	mul.wide.s32 	%rd9, %r94, 4;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.nc.b32 	%r95, [%rd10];
	add.ftz.f32 	%r96, %r118, %r95;
	ld.global.nc.b32 	%r97, [%rd10+4];
	add.ftz.f32 	%r98, %r96, %r97;
	ld.global.nc.b32 	%r99, [%rd10+8];
	add.ftz.f32 	%r100, %r98, %r99;
	ld.global.nc.b32 	%r101, [%rd10+12];
	add.ftz.f32 	%r118, %r100, %r101;
	add.s32 	%r110, %r110, 4;
$L__BB1_11:
	setp.eq.s32 	%p9, %r21, 0;
	@%p9 bra 	$L__BB1_14;
	add.s32 	%r116, %r110, %r2;
	neg.s32 	%r115, %r21;
$L__BB1_13:
	.pragma "nounroll";
	mul.wide.s32 	%rd11, %r116, 4;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.nc.b32 	%r102, [%rd12];
	add.ftz.f32 	%r118, %r118, %r102;
	add.s32 	%r116, %r116, 1;
	add.s32 	%r115, %r115, 1;
	setp.ne.s32 	%p10, %r115, 0;
	@%p10 bra 	$L__BB1_13;
$L__BB1_14:
	cvta.to.global.u64 	%rd13, %rd3;
	mul.wide.s32 	%rd14, %r1, 4;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.b32 	[%rd15], %r118;
$L__BB1_15:
	ret;

}
	// .globl	compute_normalized_laplacian
.visible .entry compute_normalized_laplacian(
	.param .u64 .ptr .align 1 compute_normalized_laplacian_param_0,
	.param .u64 .ptr .align 1 compute_normalized_laplacian_param_1,
	.param .u64 .ptr .align 1 compute_normalized_laplacian_param_2,
	.param .u32 compute_normalized_laplacian_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<27>;
	.reg .b64 	%rd<15>;

	ld.param.b64 	%rd1, [compute_normalized_laplacian_param_0];
	ld.param.b64 	%rd2, [compute_normalized_laplacian_param_1];
	ld.param.b64 	%rd3, [compute_normalized_laplacian_param_2];
	ld.param.b32 	%r8, [compute_normalized_laplacian_param_3];
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %tid.y;
	mad.lo.s32 	%r1, %r9, %r10, %r11;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %ntid.x;
	mov.u32 	%r14, %tid.x;
	mad.lo.s32 	%r2, %r12, %r13, %r14;
	max.s32 	%r15, %r1, %r2;
	setp.ge.s32 	%p1, %r15, %r8;
	@%p1 bra 	$L__BB2_5;
	setp.eq.s32 	%p2, %r1, %r2;
	mov.b32 	%r26, 0f3F800000;
	@%p2 bra 	$L__BB2_4;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.u32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.b32 	%r3, [%rd6];
	mul.wide.s32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd4, %rd7;
	ld.global.nc.b32 	%r18, [%rd8];
	mad.lo.s32 	%r19, %r8, %r1, %r2;
	cvta.to.global.u64 	%rd9, %rd1;
	mul.wide.s32 	%rd10, %r19, 4;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.nc.b32 	%r20, [%rd11];
	setp.leu.ftz.f32 	%p3, %r3, 0f2EDBE6FF;
	setp.leu.ftz.f32 	%p4, %r18, 0f2EDBE6FF;
	or.pred 	%p5, %p3, %p4;
	setp.leu.ftz.f32 	%p6, %r20, 0f00000000;
	mov.b32 	%r26, 0f00000000;
	or.pred 	%p7, %p5, %p6;
	@%p7 bra 	$L__BB2_4;
	rsqrt.approx.ftz.f32 	%r21, %r3;
	rsqrt.approx.ftz.f32 	%r22, %r18;
	neg.ftz.f32 	%r23, %r20;
	mul.ftz.f32 	%r24, %r21, %r23;
	mul.ftz.f32 	%r26, %r24, %r22;
$L__BB2_4:
	mad.lo.s32 	%r25, %r8, %r1, %r2;
	cvta.to.global.u64 	%rd12, %rd3;
	mul.wide.s32 	%rd13, %r25, 4;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.b32 	[%rd14], %r26;
$L__BB2_5:
	ret;

}
	// .globl	matvec_multiply
.visible .entry matvec_multiply(
	.param .u64 .ptr .align 1 matvec_multiply_param_0,
	.param .u64 .ptr .align 1 matvec_multiply_param_1,
	.param .u64 .ptr .align 1 matvec_multiply_param_2,
	.param .u32 matvec_multiply_param_3
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<175>;
	.reg .b64 	%rd<20>;
	// demoted variable
	.shared .align 4 .b8 _ZZ15matvec_multiplyE8shared_x[1024];
	ld.param.b64 	%rd7, [matvec_multiply_param_0];
	ld.param.b64 	%rd5, [matvec_multiply_param_1];
	ld.param.b64 	%rd6, [matvec_multiply_param_2];
	ld.param.b32 	%r46, [matvec_multiply_param_3];
	cvta.to.global.u64 	%rd1, %rd7;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p1, %r1, %r46;
	@%p1 bra 	$L__BB3_21;
	shl.b32 	%r49, %r2, 2;
	mov.b32 	%r50, _ZZ15matvec_multiplyE8shared_x;
	add.s32 	%r3, %r50, %r49;
	mul.lo.s32 	%r4, %r46, %r1;
	add.s64 	%rd2, %rd1, 32;
	cvta.to.global.u64 	%rd3, %rd5;
	mov.b32 	%r174, 0f00000000;
	mov.b32 	%r158, 0;
	mov.b32 	%r157, %r46;
$L__BB3_2:
	max.s32 	%r52, %r157, 1;
	min.s32 	%r8, %r52, 256;
	and.b32 	%r9, %r8, 7;
	and.b32 	%r10, %r8, 15;
	add.s32 	%r11, %r158, %r2;
	setp.ge.s32 	%p2, %r11, %r46;
	mov.b32 	%r160, 0f00000000;
	@%p2 bra 	$L__BB3_4;
	mul.wide.u32 	%rd8, %r11, 4;
	add.s64 	%rd9, %rd3, %rd8;
	ld.global.nc.b32 	%r160, [%rd9];
$L__BB3_4:
	st.shared.b32 	[%r3], %r160;
	bar.sync 	0;
	setp.le.s32 	%p3, %r46, %r158;
	@%p3 bra 	$L__BB3_18;
	add.s32 	%r14, %r158, %r4;
	setp.lt.s32 	%p4, %r157, 16;
	mov.b32 	%r169, 0;
	@%p4 bra 	$L__BB3_8;
	and.b32 	%r169, %r8, 496;
	mov.b32 	%r56, _ZZ15matvec_multiplyE8shared_x;
	add.s32 	%r161, %r56, 32;
	neg.s32 	%r163, %r169;
	mov.b32 	%r162, %r14;
$L__BB3_7:
	.pragma "nounroll";
	mul.wide.s32 	%rd10, %r162, 4;
	add.s64 	%rd11, %rd2, %rd10;
	ld.global.nc.b32 	%r57, [%rd11+-32];
	ld.shared.b32 	%r58, [%r161+-32];
	fma.rn.ftz.f32 	%r59, %r57, %r58, %r174;
	ld.global.nc.b32 	%r60, [%rd11+-28];
	ld.shared.b32 	%r61, [%r161+-28];
	fma.rn.ftz.f32 	%r62, %r60, %r61, %r59;
	ld.global.nc.b32 	%r63, [%rd11+-24];
	ld.shared.b32 	%r64, [%r161+-24];
	fma.rn.ftz.f32 	%r65, %r63, %r64, %r62;
	ld.global.nc.b32 	%r66, [%rd11+-20];
	ld.shared.b32 	%r67, [%r161+-20];
	fma.rn.ftz.f32 	%r68, %r66, %r67, %r65;
	ld.global.nc.b32 	%r69, [%rd11+-16];
	ld.shared.b32 	%r70, [%r161+-16];
	fma.rn.ftz.f32 	%r71, %r69, %r70, %r68;
	ld.global.nc.b32 	%r72, [%rd11+-12];
	ld.shared.b32 	%r73, [%r161+-12];
	fma.rn.ftz.f32 	%r74, %r72, %r73, %r71;
	ld.global.nc.b32 	%r75, [%rd11+-8];
	ld.shared.b32 	%r76, [%r161+-8];
	fma.rn.ftz.f32 	%r77, %r75, %r76, %r74;
	ld.global.nc.b32 	%r78, [%rd11+-4];
	ld.shared.b32 	%r79, [%r161+-4];
	fma.rn.ftz.f32 	%r80, %r78, %r79, %r77;
	ld.global.nc.b32 	%r81, [%rd11];
	ld.shared.b32 	%r82, [%r161];
	fma.rn.ftz.f32 	%r83, %r81, %r82, %r80;
	ld.global.nc.b32 	%r84, [%rd11+4];
	ld.shared.b32 	%r85, [%r161+4];
	fma.rn.ftz.f32 	%r86, %r84, %r85, %r83;
	ld.global.nc.b32 	%r87, [%rd11+8];
	ld.shared.b32 	%r88, [%r161+8];
	fma.rn.ftz.f32 	%r89, %r87, %r88, %r86;
	ld.global.nc.b32 	%r90, [%rd11+12];
	ld.shared.b32 	%r91, [%r161+12];
	fma.rn.ftz.f32 	%r92, %r90, %r91, %r89;
	ld.global.nc.b32 	%r93, [%rd11+16];
	ld.shared.b32 	%r94, [%r161+16];
	fma.rn.ftz.f32 	%r95, %r93, %r94, %r92;
	ld.global.nc.b32 	%r96, [%rd11+20];
	ld.shared.b32 	%r97, [%r161+20];
	fma.rn.ftz.f32 	%r98, %r96, %r97, %r95;
	ld.global.nc.b32 	%r99, [%rd11+24];
	ld.shared.b32 	%r100, [%r161+24];
	fma.rn.ftz.f32 	%r101, %r99, %r100, %r98;
	ld.global.nc.b32 	%r102, [%rd11+28];
	ld.shared.b32 	%r103, [%r161+28];
	fma.rn.ftz.f32 	%r174, %r102, %r103, %r101;
	add.s32 	%r163, %r163, 16;
	add.s32 	%r162, %r162, 16;
	add.s32 	%r161, %r161, 64;
	setp.ne.s32 	%p5, %r163, 0;
	@%p5 bra 	$L__BB3_7;
$L__BB3_8:
	setp.eq.s32 	%p6, %r10, 0;
	@%p6 bra 	$L__BB3_18;
	setp.lt.u32 	%p7, %r10, 8;
	@%p7 bra 	$L__BB3_11;
	add.s32 	%r105, %r14, %r169;
	mul.wide.s32 	%rd12, %r105, 4;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.nc.b32 	%r106, [%rd13];
	shl.b32 	%r107, %r169, 2;
	mov.b32 	%r108, _ZZ15matvec_multiplyE8shared_x;
	add.s32 	%r109, %r108, %r107;
	ld.shared.b32 	%r110, [%r109];
	fma.rn.ftz.f32 	%r111, %r106, %r110, %r174;
	ld.global.nc.b32 	%r112, [%rd13+4];
	ld.shared.b32 	%r113, [%r109+4];
	fma.rn.ftz.f32 	%r114, %r112, %r113, %r111;
	ld.global.nc.b32 	%r115, [%rd13+8];
	ld.shared.b32 	%r116, [%r109+8];
	fma.rn.ftz.f32 	%r117, %r115, %r116, %r114;
	ld.global.nc.b32 	%r118, [%rd13+12];
	ld.shared.b32 	%r119, [%r109+12];
	fma.rn.ftz.f32 	%r120, %r118, %r119, %r117;
	ld.global.nc.b32 	%r121, [%rd13+16];
	ld.shared.b32 	%r122, [%r109+16];
	fma.rn.ftz.f32 	%r123, %r121, %r122, %r120;
	ld.global.nc.b32 	%r124, [%rd13+20];
	ld.shared.b32 	%r125, [%r109+20];
	fma.rn.ftz.f32 	%r126, %r124, %r125, %r123;
	ld.global.nc.b32 	%r127, [%rd13+24];
	ld.shared.b32 	%r128, [%r109+24];
	fma.rn.ftz.f32 	%r129, %r127, %r128, %r126;
	ld.global.nc.b32 	%r130, [%rd13+28];
	ld.shared.b32 	%r131, [%r109+28];
	fma.rn.ftz.f32 	%r174, %r130, %r131, %r129;
	add.s32 	%r169, %r169, 8;
$L__BB3_11:
	setp.eq.s32 	%p8, %r9, 0;
	@%p8 bra 	$L__BB3_18;
	and.b32 	%r33, %r8, 3;
	setp.lt.u32 	%p9, %r9, 4;
	@%p9 bra 	$L__BB3_14;
	add.s32 	%r133, %r14, %r169;
	mul.wide.s32 	%rd14, %r133, 4;
	add.s64 	%rd15, %rd1, %rd14;
	ld.global.nc.b32 	%r134, [%rd15];
	shl.b32 	%r135, %r169, 2;
	mov.b32 	%r136, _ZZ15matvec_multiplyE8shared_x;
	add.s32 	%r137, %r136, %r135;
	ld.shared.b32 	%r138, [%r137];
	fma.rn.ftz.f32 	%r139, %r134, %r138, %r174;
	ld.global.nc.b32 	%r140, [%rd15+4];
	ld.shared.b32 	%r141, [%r137+4];
	fma.rn.ftz.f32 	%r142, %r140, %r141, %r139;
	ld.global.nc.b32 	%r143, [%rd15+8];
	ld.shared.b32 	%r144, [%r137+8];
	fma.rn.ftz.f32 	%r145, %r143, %r144, %r142;
	ld.global.nc.b32 	%r146, [%rd15+12];
	ld.shared.b32 	%r147, [%r137+12];
	fma.rn.ftz.f32 	%r174, %r146, %r147, %r145;
	add.s32 	%r169, %r169, 4;
$L__BB3_14:
	setp.eq.s32 	%p10, %r33, 0;
	@%p10 bra 	$L__BB3_18;
	add.s32 	%r148, %r14, %r169;
	mul.wide.s32 	%rd16, %r148, 4;
	add.s64 	%rd4, %rd1, %rd16;
	ld.global.nc.b32 	%r149, [%rd4];
	shl.b32 	%r150, %r169, 2;
	mov.b32 	%r151, _ZZ15matvec_multiplyE8shared_x;
	add.s32 	%r39, %r151, %r150;
	ld.shared.b32 	%r152, [%r39];
	fma.rn.ftz.f32 	%r174, %r149, %r152, %r174;
	setp.eq.s32 	%p11, %r33, 1;
	@%p11 bra 	$L__BB3_18;
	ld.global.nc.b32 	%r153, [%rd4+4];
	ld.shared.b32 	%r154, [%r39+4];
	fma.rn.ftz.f32 	%r174, %r153, %r154, %r174;
	setp.eq.s32 	%p12, %r33, 2;
	@%p12 bra 	$L__BB3_18;
	ld.global.nc.b32 	%r155, [%rd4+8];
	ld.shared.b32 	%r156, [%r39+8];
	fma.rn.ftz.f32 	%r174, %r155, %r156, %r174;
$L__BB3_18:
	bar.sync 	0;
	add.s32 	%r158, %r158, 256;
	setp.lt.s32 	%p13, %r158, %r46;
	add.s32 	%r157, %r157, -256;
	@%p13 bra 	$L__BB3_2;
	setp.ne.s32 	%p14, %r2, 0;
	@%p14 bra 	$L__BB3_21;
	cvta.to.global.u64 	%rd17, %rd6;
	mul.wide.u32 	%rd18, %r1, 4;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.b32 	[%rd19], %r174;
$L__BB3_21:
	ret;

}
	// .globl	vector_norm_sq
.visible .entry vector_norm_sq(
	.param .u64 .ptr .align 1 vector_norm_sq_param_0,
	.param .u64 .ptr .align 1 vector_norm_sq_param_1,
	.param .u32 vector_norm_sq_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<7>;
	// demoted variable
	.shared .align 4 .b8 _ZZ14vector_norm_sqE10shared_sum[1024];
	ld.param.b64 	%rd1, [vector_norm_sq_param_0];
	ld.param.b64 	%rd2, [vector_norm_sq_param_1];
	ld.param.b32 	%r10, [vector_norm_sq_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r23, %ntid.x;
	mad.lo.s32 	%r3, %r11, %r23, %r1;
	setp.ge.s32 	%p1, %r3, %r10;
	mov.b32 	%r22, 0f00000000;
	@%p1 bra 	$L__BB4_2;
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r3, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.b32 	%r12, [%rd5];
	mul.ftz.f32 	%r22, %r12, %r12;
$L__BB4_2:
	shl.b32 	%r13, %r1, 2;
	mov.b32 	%r14, _ZZ14vector_norm_sqE10shared_sum;
	add.s32 	%r6, %r14, %r13;
	st.shared.b32 	[%r6], %r22;
	bar.sync 	0;
	setp.lt.u32 	%p2, %r23, 2;
	@%p2 bra 	$L__BB4_6;
$L__BB4_3:
	shr.u32 	%r8, %r23, 1;
	setp.ge.u32 	%p3, %r1, %r8;
	@%p3 bra 	$L__BB4_5;
	shl.b32 	%r15, %r8, 2;
	add.s32 	%r16, %r6, %r15;
	ld.shared.b32 	%r17, [%r16];
	ld.shared.b32 	%r18, [%r6];
	add.ftz.f32 	%r19, %r17, %r18;
	st.shared.b32 	[%r6], %r19;
$L__BB4_5:
	bar.sync 	0;
	setp.gt.u32 	%p4, %r23, 3;
	mov.b32 	%r23, %r8;
	@%p4 bra 	$L__BB4_3;
$L__BB4_6:
	setp.ne.s32 	%p5, %r1, 0;
	@%p5 bra 	$L__BB4_8;
	ld.shared.b32 	%r20, [_ZZ14vector_norm_sqE10shared_sum];
	cvta.to.global.u64 	%rd6, %rd2;
	atom.global.add.f32 	%r21, [%rd6], %r20;
$L__BB4_8:
	ret;

}
	// .globl	normalize_and_deflate
.visible .entry normalize_and_deflate(
	.param .u64 .ptr .align 1 normalize_and_deflate_param_0,
	.param .u64 .ptr .align 1 normalize_and_deflate_param_1,
	.param .f32 normalize_and_deflate_param_2,
	.param .f32 normalize_and_deflate_param_3,
	.param .u32 normalize_and_deflate_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd2, [normalize_and_deflate_param_0];
	ld.param.b64 	%rd3, [normalize_and_deflate_param_1];
	ld.param.b32 	%r3, [normalize_and_deflate_param_2];
	ld.param.b32 	%r4, [normalize_and_deflate_param_3];
	ld.param.b32 	%r5, [normalize_and_deflate_param_4];
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r8;
	setp.ge.s32 	%p1, %r1, %r5;
	@%p1 bra 	$L__BB5_3;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd1, %rd4, %rd5;
	ld.global.b32 	%r9, [%rd1];
	div.approx.ftz.f32 	%r2, %r9, %r3;
	st.global.b32 	[%rd1], %r2;
	setp.eq.s64 	%p2, %rd3, 0;
	@%p2 bra 	$L__BB5_3;
	cvta.to.global.u64 	%rd6, %rd3;
	add.s64 	%rd8, %rd6, %rd5;
	ld.global.nc.b32 	%r10, [%rd8];
	mul.ftz.f32 	%r11, %r4, %r10;
	sub.ftz.f32 	%r12, %r2, %r11;
	st.global.b32 	[%rd1], %r12;
$L__BB5_3:
	ret;

}
	// .globl	kmeans_assign
.visible .entry kmeans_assign(
	.param .u64 .ptr .align 1 kmeans_assign_param_0,
	.param .u64 .ptr .align 1 kmeans_assign_param_1,
	.param .u64 .ptr .align 1 kmeans_assign_param_2,
	.param .u32 kmeans_assign_param_3,
	.param .u32 kmeans_assign_param_4,
	.param .u32 kmeans_assign_param_5
)
{
	.reg .pred 	%p<19>;
	.reg .b32 	%r<169>;
	.reg .b64 	%rd<40>;

	ld.param.b64 	%rd8, [kmeans_assign_param_0];
	ld.param.b64 	%rd9, [kmeans_assign_param_1];
	ld.param.b64 	%rd7, [kmeans_assign_param_2];
	ld.param.b32 	%r59, [kmeans_assign_param_3];
	ld.param.b32 	%r57, [kmeans_assign_param_4];
	ld.param.b32 	%r58, [kmeans_assign_param_5];
	cvta.to.global.u64 	%rd1, %rd9;
	cvta.to.global.u64 	%rd2, %rd8;
	mov.u32 	%r60, %ctaid.x;
	mov.u32 	%r61, %ntid.x;
	mov.u32 	%r62, %tid.x;
	mad.lo.s32 	%r1, %r60, %r61, %r62;
	setp.ge.s32 	%p1, %r1, %r59;
	@%p1 bra 	$L__BB6_23;
	setp.lt.s32 	%p2, %r58, 1;
	mov.b32 	%r168, 0;
	@%p2 bra 	$L__BB6_22;
	setp.lt.s32 	%p3, %r57, 1;
	mul.lo.s32 	%r2, %r57, %r1;
	@%p3 bra 	$L__BB6_16;
	and.b32 	%r3, %r57, 7;
	and.b32 	%r4, %r57, 3;
	and.b32 	%r5, %r57, 2147483640;
	and.b32 	%r6, %r57, 1;
	neg.s32 	%r7, %r5;
	add.s64 	%rd3, %rd2, 16;
	mov.b32 	%r143, 0f7149F2CA;
	mov.b32 	%r141, 0;
	mov.b32 	%r168, %r141;
$L__BB6_4:
	setp.lt.u32 	%p10, %r57, 8;
	mul.lo.s32 	%r11, %r141, %r57;
	mov.b32 	%r156, 0f00000000;
	mov.b32 	%r151, 0;
	@%p10 bra 	$L__BB6_7;
	mul.wide.u32 	%rd10, %r11, 4;
	add.s64 	%rd11, %rd1, %rd10;
	add.s64 	%rd39, %rd11, 16;
	mov.b32 	%r156, 0f00000000;
	mov.b32 	%r144, %r2;
	mov.b32 	%r145, %r7;
$L__BB6_6:
	.pragma "nounroll";
	mul.wide.s32 	%rd12, %r144, 4;
	add.s64 	%rd13, %rd3, %rd12;
	ld.global.nc.b32 	%r76, [%rd13+-16];
	ld.global.nc.b32 	%r77, [%rd39+-16];
	sub.ftz.f32 	%r78, %r76, %r77;
	fma.rn.ftz.f32 	%r79, %r78, %r78, %r156;
	ld.global.nc.b32 	%r80, [%rd13+-12];
	ld.global.nc.b32 	%r81, [%rd39+-12];
	sub.ftz.f32 	%r82, %r80, %r81;
	fma.rn.ftz.f32 	%r83, %r82, %r82, %r79;
	ld.global.nc.b32 	%r84, [%rd13+-8];
	ld.global.nc.b32 	%r85, [%rd39+-8];
	sub.ftz.f32 	%r86, %r84, %r85;
	fma.rn.ftz.f32 	%r87, %r86, %r86, %r83;
	ld.global.nc.b32 	%r88, [%rd13+-4];
	ld.global.nc.b32 	%r89, [%rd39+-4];
	sub.ftz.f32 	%r90, %r88, %r89;
	fma.rn.ftz.f32 	%r91, %r90, %r90, %r87;
	ld.global.nc.b32 	%r92, [%rd13];
	ld.global.nc.b32 	%r93, [%rd39];
	sub.ftz.f32 	%r94, %r92, %r93;
	fma.rn.ftz.f32 	%r95, %r94, %r94, %r91;
	ld.global.nc.b32 	%r96, [%rd13+4];
	ld.global.nc.b32 	%r97, [%rd39+4];
	sub.ftz.f32 	%r98, %r96, %r97;
	fma.rn.ftz.f32 	%r99, %r98, %r98, %r95;
	ld.global.nc.b32 	%r100, [%rd13+8];
	ld.global.nc.b32 	%r101, [%rd39+8];
	sub.ftz.f32 	%r102, %r100, %r101;
	fma.rn.ftz.f32 	%r103, %r102, %r102, %r99;
	ld.global.nc.b32 	%r104, [%rd13+12];
	ld.global.nc.b32 	%r105, [%rd39+12];
	sub.ftz.f32 	%r106, %r104, %r105;
	fma.rn.ftz.f32 	%r156, %r106, %r106, %r103;
	add.s32 	%r145, %r145, 8;
	add.s32 	%r144, %r144, 8;
	add.s64 	%rd39, %rd39, 32;
	setp.ne.s32 	%p11, %r145, 0;
	mov.b32 	%r151, %r5;
	@%p11 bra 	$L__BB6_6;
$L__BB6_7:
	setp.eq.s32 	%p12, %r3, 0;
	@%p12 bra 	$L__BB6_15;
	add.s32 	%r108, %r3, -1;
	setp.lt.u32 	%p13, %r108, 3;
	@%p13 bra 	$L__BB6_10;
	add.s32 	%r109, %r151, %r2;
	mul.wide.s32 	%rd14, %r109, 4;
	add.s64 	%rd15, %rd2, %rd14;
	ld.global.nc.b32 	%r110, [%rd15];
	add.s32 	%r111, %r151, %r11;
	mul.wide.u32 	%rd16, %r111, 4;
	add.s64 	%rd17, %rd1, %rd16;
	ld.global.nc.b32 	%r112, [%rd17];
	sub.ftz.f32 	%r113, %r110, %r112;
	fma.rn.ftz.f32 	%r114, %r113, %r113, %r156;
	ld.global.nc.b32 	%r115, [%rd15+4];
	cvt.u64.u32 	%rd18, %r11;
	cvt.u64.u32 	%rd19, %r151;
	add.s64 	%rd20, %rd19, %rd18;
	shl.b64 	%rd21, %rd20, 2;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.nc.b32 	%r116, [%rd22+4];
	sub.ftz.f32 	%r117, %r115, %r116;
	fma.rn.ftz.f32 	%r118, %r117, %r117, %r114;
	ld.global.nc.b32 	%r119, [%rd15+8];
	ld.global.nc.b32 	%r120, [%rd22+8];
	sub.ftz.f32 	%r121, %r119, %r120;
	fma.rn.ftz.f32 	%r122, %r121, %r121, %r118;
	ld.global.nc.b32 	%r123, [%rd15+12];
	ld.global.nc.b32 	%r124, [%rd22+12];
	sub.ftz.f32 	%r125, %r123, %r124;
	fma.rn.ftz.f32 	%r156, %r125, %r125, %r122;
	add.s32 	%r151, %r151, 4;
$L__BB6_10:
	setp.eq.s32 	%p14, %r4, 0;
	@%p14 bra 	$L__BB6_15;
	setp.eq.s32 	%p15, %r4, 1;
	@%p15 bra 	$L__BB6_13;
	add.s32 	%r127, %r151, %r2;
	mul.wide.s32 	%rd23, %r127, 4;
	add.s64 	%rd24, %rd2, %rd23;
	ld.global.nc.b32 	%r128, [%rd24];
	add.s32 	%r129, %r151, %r11;
	mul.wide.u32 	%rd25, %r129, 4;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.nc.b32 	%r130, [%rd26];
	sub.ftz.f32 	%r131, %r128, %r130;
	fma.rn.ftz.f32 	%r132, %r131, %r131, %r156;
	ld.global.nc.b32 	%r133, [%rd24+4];
	cvt.u64.u32 	%rd27, %r11;
	cvt.u64.u32 	%rd28, %r151;
	add.s64 	%rd29, %rd28, %rd27;
	shl.b64 	%rd30, %rd29, 2;
	add.s64 	%rd31, %rd1, %rd30;
	ld.global.nc.b32 	%r134, [%rd31+4];
	sub.ftz.f32 	%r135, %r133, %r134;
	fma.rn.ftz.f32 	%r156, %r135, %r135, %r132;
	add.s32 	%r151, %r151, 2;
$L__BB6_13:
	setp.eq.s32 	%p16, %r6, 0;
	@%p16 bra 	$L__BB6_15;
	add.s32 	%r136, %r151, %r2;
	mul.wide.s32 	%rd32, %r136, 4;
	add.s64 	%rd33, %rd2, %rd32;
	ld.global.nc.b32 	%r137, [%rd33];
	add.s32 	%r138, %r151, %r11;
	mul.wide.u32 	%rd34, %r138, 4;
	add.s64 	%rd35, %rd1, %rd34;
	ld.global.nc.b32 	%r139, [%rd35];
	sub.ftz.f32 	%r140, %r137, %r139;
	fma.rn.ftz.f32 	%r156, %r140, %r140, %r156;
$L__BB6_15:
	setp.lt.ftz.f32 	%p17, %r156, %r143;
	selp.f32 	%r143, %r156, %r143, %p17;
	selp.b32 	%r168, %r141, %r168, %p17;
	add.s32 	%r141, %r141, 1;
	setp.eq.s32 	%p18, %r141, %r58;
	@%p18 bra 	$L__BB6_22;
	bra.uni 	$L__BB6_4;
$L__BB6_16:
	and.b32 	%r36, %r58, 3;
	setp.lt.u32 	%p4, %r58, 4;
	mov.b32 	%r163, 0f7149F2CA;
	mov.b32 	%r164, 0;
	mov.b32 	%r168, %r164;
	@%p4 bra 	$L__BB6_19;
	and.b32 	%r164, %r58, 2147483644;
	mov.b32 	%r163, 0f7149F2CA;
	mov.b32 	%r157, 0;
	mov.b32 	%r168, %r157;
$L__BB6_18:
	setp.gt.ftz.f32 	%p5, %r163, 0f00000000;
	selp.f32 	%r163, 0f00000000, %r163, %p5;
	selp.b32 	%r168, %r157, %r168, %p5;
	add.s32 	%r157, %r157, 4;
	setp.ne.s32 	%p6, %r157, %r164;
	@%p6 bra 	$L__BB6_18;
$L__BB6_19:
	setp.eq.s32 	%p7, %r36, 0;
	@%p7 bra 	$L__BB6_22;
	mov.b32 	%r167, 0;
$L__BB6_21:
	.pragma "nounroll";
	setp.gt.ftz.f32 	%p8, %r163, 0f00000000;
	selp.f32 	%r163, 0f00000000, %r163, %p8;
	selp.b32 	%r168, %r164, %r168, %p8;
	add.s32 	%r164, %r164, 1;
	add.s32 	%r167, %r167, 1;
	setp.ne.s32 	%p9, %r167, %r36;
	@%p9 bra 	$L__BB6_21;
$L__BB6_22:
	cvta.to.global.u64 	%rd36, %rd7;
	mul.wide.s32 	%rd37, %r1, 4;
	add.s64 	%rd38, %rd36, %rd37;
	st.global.b32 	[%rd38], %r168;
$L__BB6_23:
	ret;

}
	// .globl	kmeans_update_centroids
.visible .entry kmeans_update_centroids(
	.param .u64 .ptr .align 1 kmeans_update_centroids_param_0,
	.param .u64 .ptr .align 1 kmeans_update_centroids_param_1,
	.param .u64 .ptr .align 1 kmeans_update_centroids_param_2,
	.param .u64 .ptr .align 1 kmeans_update_centroids_param_3,
	.param .u32 kmeans_update_centroids_param_4,
	.param .u32 kmeans_update_centroids_param_5,
	.param .u32 kmeans_update_centroids_param_6
)
{
	.reg .pred 	%p<31>;
	.reg .b32 	%r<231>;
	.reg .b64 	%rd<54>;

	ld.param.b64 	%rd10, [kmeans_update_centroids_param_0];
	ld.param.b64 	%rd11, [kmeans_update_centroids_param_1];
	ld.param.b64 	%rd9, [kmeans_update_centroids_param_3];
	ld.param.b32 	%r115, [kmeans_update_centroids_param_4];
	ld.param.b32 	%r116, [kmeans_update_centroids_param_5];
	ld.param.b32 	%r117, [kmeans_update_centroids_param_6];
	cvta.to.global.u64 	%rd1, %rd10;
	cvta.to.global.u64 	%rd2, %rd11;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p1, %r1, %r117;
	setp.ge.s32 	%p2, %r2, %r116;
	or.pred 	%p3, %p2, %p1;
	@%p3 bra 	$L__BB7_46;
	setp.lt.s32 	%p4, %r115, 1;
	mov.b32 	%r187, 0;
	mov.b32 	%r186, 0f00000000;
	@%p4 bra 	$L__BB7_42;
	setp.eq.s32 	%p5, %r2, 0;
	selp.b32 	%r3, 1, 0, %p5;
	and.b32 	%r4, %r115, 7;
	setp.lt.u32 	%p6, %r115, 8;
	mov.b32 	%r186, 0f00000000;
	mov.b32 	%r217, 0;
	mov.b32 	%r187, %r217;
	@%p6 bra 	$L__BB7_21;
	and.b32 	%r217, %r115, 2147483640;
	shl.b32 	%r126, %r116, 1;
	add.s32 	%r183, %r2, %r126;
	shl.b32 	%r7, %r116, 3;
	mad.lo.s32 	%r182, %r116, 3, %r2;
	shl.b32 	%r127, %r116, 2;
	add.s32 	%r181, %r2, %r127;
	mad.lo.s32 	%r180, %r116, 5, %r2;
	mad.lo.s32 	%r179, %r116, 6, %r2;
	mad.lo.s32 	%r178, %r116, 7, %r2;
	neg.s32 	%r177, %r217;
	add.s32 	%r176, %r2, %r116;
	add.s64 	%rd53, %rd2, 16;
	mov.b32 	%r186, 0f00000000;
	mov.b32 	%r187, 0;
	mov.b32 	%r175, %r2;
$L__BB7_4:
	.pragma "nounroll";
	ld.global.nc.b32 	%r128, [%rd53+-16];
	setp.ne.s32 	%p7, %r128, %r1;
	@%p7 bra 	$L__BB7_6;
	mul.wide.s32 	%rd12, %r175, 4;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.nc.b32 	%r129, [%rd13];
	add.ftz.f32 	%r186, %r186, %r129;
	add.s32 	%r187, %r187, %r3;
$L__BB7_6:
	ld.global.nc.b32 	%r130, [%rd53+-12];
	setp.ne.s32 	%p8, %r130, %r1;
	@%p8 bra 	$L__BB7_8;
	mul.wide.s32 	%rd14, %r176, 4;
	add.s64 	%rd15, %rd1, %rd14;
	ld.global.nc.b32 	%r131, [%rd15];
	add.ftz.f32 	%r186, %r186, %r131;
	add.s32 	%r187, %r187, %r3;
$L__BB7_8:
	ld.global.nc.b32 	%r132, [%rd53+-8];
	setp.ne.s32 	%p9, %r132, %r1;
	@%p9 bra 	$L__BB7_10;
	mul.wide.s32 	%rd16, %r183, 4;
	add.s64 	%rd17, %rd1, %rd16;
	ld.global.nc.b32 	%r133, [%rd17];
	add.ftz.f32 	%r186, %r186, %r133;
	add.s32 	%r187, %r187, %r3;
$L__BB7_10:
	ld.global.nc.b32 	%r134, [%rd53+-4];
	setp.ne.s32 	%p10, %r134, %r1;
	@%p10 bra 	$L__BB7_12;
	mul.wide.s32 	%rd18, %r182, 4;
	add.s64 	%rd19, %rd1, %rd18;
	ld.global.nc.b32 	%r135, [%rd19];
	add.ftz.f32 	%r186, %r186, %r135;
	add.s32 	%r187, %r187, %r3;
$L__BB7_12:
	ld.global.nc.b32 	%r136, [%rd53];
	setp.ne.s32 	%p11, %r136, %r1;
	@%p11 bra 	$L__BB7_14;
	mul.wide.s32 	%rd20, %r181, 4;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.nc.b32 	%r137, [%rd21];
	add.ftz.f32 	%r186, %r186, %r137;
	add.s32 	%r187, %r187, %r3;
$L__BB7_14:
	ld.global.nc.b32 	%r138, [%rd53+4];
	setp.ne.s32 	%p12, %r138, %r1;
	@%p12 bra 	$L__BB7_16;
	mul.wide.s32 	%rd22, %r180, 4;
	add.s64 	%rd23, %rd1, %rd22;
	ld.global.nc.b32 	%r139, [%rd23];
	add.ftz.f32 	%r186, %r186, %r139;
	add.s32 	%r187, %r187, %r3;
$L__BB7_16:
	ld.global.nc.b32 	%r140, [%rd53+8];
	setp.ne.s32 	%p13, %r140, %r1;
	@%p13 bra 	$L__BB7_18;
	mul.wide.s32 	%rd24, %r179, 4;
	add.s64 	%rd25, %rd1, %rd24;
	ld.global.nc.b32 	%r141, [%rd25];
	add.ftz.f32 	%r186, %r186, %r141;
	add.s32 	%r187, %r187, %r3;
$L__BB7_18:
	ld.global.nc.b32 	%r142, [%rd53+12];
	setp.ne.s32 	%p14, %r142, %r1;
	@%p14 bra 	$L__BB7_20;
	mul.wide.s32 	%rd26, %r178, 4;
	add.s64 	%rd27, %rd1, %rd26;
	ld.global.nc.b32 	%r143, [%rd27];
	add.ftz.f32 	%r186, %r186, %r143;
	add.s32 	%r187, %r187, %r3;
$L__BB7_20:
	add.s32 	%r183, %r183, %r7;
	add.s32 	%r182, %r182, %r7;
	add.s32 	%r181, %r181, %r7;
	add.s32 	%r180, %r180, %r7;
	add.s32 	%r179, %r179, %r7;
	add.s32 	%r178, %r178, %r7;
	add.s32 	%r177, %r177, 8;
	add.s32 	%r176, %r176, %r7;
	add.s32 	%r175, %r175, %r7;
	add.s64 	%rd53, %rd53, 32;
	setp.ne.s32 	%p15, %r177, 0;
	@%p15 bra 	$L__BB7_4;
$L__BB7_21:
	setp.eq.s32 	%p16, %r4, 0;
	@%p16 bra 	$L__BB7_42;
	and.b32 	%r72, %r115, 3;
	setp.lt.u32 	%p17, %r4, 4;
	@%p17 bra 	$L__BB7_32;
	mul.wide.u32 	%rd28, %r217, 4;
	add.s64 	%rd6, %rd2, %rd28;
	ld.global.nc.b32 	%r146, [%rd6];
	setp.ne.s32 	%p18, %r146, %r1;
	@%p18 bra 	$L__BB7_25;
	mad.lo.s32 	%r147, %r217, %r116, %r2;
	mul.wide.s32 	%rd29, %r147, 4;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.nc.b32 	%r148, [%rd30];
	add.ftz.f32 	%r186, %r186, %r148;
	add.s32 	%r187, %r187, %r3;
$L__BB7_25:
	add.s32 	%r77, %r217, 1;
	ld.global.nc.b32 	%r149, [%rd6+4];
	setp.ne.s32 	%p19, %r149, %r1;
	@%p19 bra 	$L__BB7_27;
	mad.lo.s32 	%r150, %r77, %r116, %r2;
	mul.wide.s32 	%rd31, %r150, 4;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.nc.b32 	%r151, [%rd32];
	add.ftz.f32 	%r186, %r186, %r151;
	add.s32 	%r187, %r187, %r3;
$L__BB7_27:
	ld.global.nc.b32 	%r152, [%rd6+8];
	setp.ne.s32 	%p20, %r152, %r1;
	@%p20 bra 	$L__BB7_29;
	mad.lo.s32 	%r153, %r116, %r77, %r116;
	add.s32 	%r154, %r153, %r2;
	mul.wide.s32 	%rd33, %r154, 4;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.nc.b32 	%r155, [%rd34];
	add.ftz.f32 	%r186, %r186, %r155;
	add.s32 	%r187, %r187, %r3;
$L__BB7_29:
	ld.global.nc.b32 	%r156, [%rd6+12];
	setp.ne.s32 	%p21, %r156, %r1;
	@%p21 bra 	$L__BB7_31;
	add.s32 	%r157, %r77, 2;
	mad.lo.s32 	%r158, %r157, %r116, %r2;
	mul.wide.s32 	%rd35, %r158, 4;
	add.s64 	%rd36, %rd1, %rd35;
	ld.global.nc.b32 	%r159, [%rd36];
	add.ftz.f32 	%r186, %r186, %r159;
	add.s32 	%r187, %r187, %r3;
$L__BB7_31:
	add.s32 	%r217, %r77, 3;
$L__BB7_32:
	setp.eq.s32 	%p22, %r72, 0;
	@%p22 bra 	$L__BB7_42;
	setp.eq.s32 	%p23, %r72, 1;
	@%p23 bra 	$L__BB7_39;
	mul.wide.u32 	%rd37, %r217, 4;
	add.s64 	%rd7, %rd2, %rd37;
	ld.global.nc.b32 	%r162, [%rd7];
	setp.ne.s32 	%p24, %r162, %r1;
	@%p24 bra 	$L__BB7_36;
	mad.lo.s32 	%r163, %r217, %r116, %r2;
	mul.wide.s32 	%rd38, %r163, 4;
	add.s64 	%rd39, %rd1, %rd38;
	ld.global.nc.b32 	%r164, [%rd39];
	add.ftz.f32 	%r186, %r186, %r164;
	add.s32 	%r187, %r187, %r3;
$L__BB7_36:
	add.s32 	%r100, %r217, 1;
	ld.global.nc.b32 	%r165, [%rd7+4];
	setp.ne.s32 	%p25, %r165, %r1;
	@%p25 bra 	$L__BB7_38;
	mad.lo.s32 	%r166, %r100, %r116, %r2;
	mul.wide.s32 	%rd40, %r166, 4;
	add.s64 	%rd41, %rd1, %rd40;
	ld.global.nc.b32 	%r167, [%rd41];
	add.ftz.f32 	%r186, %r186, %r167;
	add.s32 	%r187, %r187, %r3;
$L__BB7_38:
	add.s32 	%r217, %r100, 1;
$L__BB7_39:
	and.b32 	%r168, %r115, 1;
	setp.ne.b32 	%p26, %r168, 0;
	not.pred 	%p27, %p26;
	@%p27 bra 	$L__BB7_42;
	mul.wide.u32 	%rd42, %r217, 4;
	add.s64 	%rd43, %rd2, %rd42;
	ld.global.nc.b32 	%r169, [%rd43];
	setp.ne.s32 	%p28, %r169, %r1;
	@%p28 bra 	$L__BB7_42;
	mad.lo.s32 	%r170, %r217, %r116, %r2;
	mul.wide.s32 	%rd44, %r170, 4;
	add.s64 	%rd45, %rd1, %rd44;
	ld.global.nc.b32 	%r171, [%rd45];
	add.ftz.f32 	%r186, %r186, %r171;
	add.s32 	%r187, %r187, %r3;
$L__BB7_42:
	setp.ne.s32 	%p29, %r2, 0;
	@%p29 bra 	$L__BB7_44;
	cvta.to.global.u64 	%rd46, %rd9;
	mul.wide.u32 	%rd47, %r1, 4;
	add.s64 	%rd48, %rd46, %rd47;
	st.global.b32 	[%rd48], %r187;
$L__BB7_44:
	setp.lt.s32 	%p30, %r187, 1;
	@%p30 bra 	$L__BB7_46;
	ld.param.b64 	%rd52, [kmeans_update_centroids_param_2];
	cvt.rn.f32.u32 	%r172, %r187;
	div.approx.ftz.f32 	%r173, %r186, %r172;
	mad.lo.s32 	%r174, %r116, %r1, %r2;
	cvta.to.global.u64 	%rd49, %rd52;
	mul.wide.s32 	%rd50, %r174, 4;
	add.s64 	%rd51, %rd49, %rd50;
	st.global.b32 	[%rd51], %r173;
$L__BB7_46:
	ret;

}
	// .globl	fused_spectral_init
.visible .entry fused_spectral_init(
	.param .u64 .ptr .align 1 fused_spectral_init_param_0,
	.param .u64 .ptr .align 1 fused_spectral_init_param_1,
	.param .u64 .ptr .align 1 fused_spectral_init_param_2,
	.param .u64 .ptr .align 1 fused_spectral_init_param_3,
	.param .u64 .ptr .align 1 fused_spectral_init_param_4,
	.param .u32 fused_spectral_init_param_5
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<156>;
	.reg .b64 	%rd<50>;

	ld.param.b64 	%rd10, [fused_spectral_init_param_0];
	ld.param.b64 	%rd13, [fused_spectral_init_param_1];
	ld.param.b64 	%rd14, [fused_spectral_init_param_2];
	ld.param.b64 	%rd11, [fused_spectral_init_param_3];
	ld.param.b64 	%rd12, [fused_spectral_init_param_4];
	ld.param.b32 	%r40, [fused_spectral_init_param_5];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd13;
	mov.u32 	%r41, %ctaid.y;
	mov.u32 	%r42, %ntid.y;
	mov.u32 	%r43, %tid.y;
	mad.lo.s32 	%r1, %r41, %r42, %r43;
	mov.u32 	%r44, %ctaid.x;
	mov.u32 	%r45, %ntid.x;
	mov.u32 	%r46, %tid.x;
	mad.lo.s32 	%r2, %r44, %r45, %r46;
	max.s32 	%r47, %r1, %r2;
	setp.ge.s32 	%p1, %r47, %r40;
	mov.b32 	%r140, 0f00000000;
	@%p1 bra 	$L__BB8_23;
	setp.eq.s32 	%p2, %r1, %r2;
	@%p2 bra 	$L__BB8_4;
	cvta.to.global.u64 	%rd15, %rd10;
	mul.lo.s32 	%r50, %r1, 3;
	mul.wide.u32 	%rd16, %r50, 4;
	add.s64 	%rd17, %rd15, %rd16;
	ld.global.nc.b32 	%r51, [%rd17];
	mul.lo.s32 	%r52, %r2, 3;
	mul.wide.s32 	%rd18, %r52, 4;
	add.s64 	%rd19, %rd15, %rd18;
	ld.global.nc.b32 	%r53, [%rd19];
	sub.ftz.f32 	%r54, %r51, %r53;
	ld.global.nc.b32 	%r55, [%rd17+4];
	ld.global.nc.b32 	%r56, [%rd19+4];
	sub.ftz.f32 	%r57, %r55, %r56;
	ld.global.nc.b32 	%r58, [%rd17+8];
	ld.global.nc.b32 	%r59, [%rd19+8];
	sub.ftz.f32 	%r60, %r58, %r59;
	mul.ftz.f32 	%r61, %r57, %r57;
	fma.rn.ftz.f32 	%r62, %r54, %r54, %r61;
	fma.rn.ftz.f32 	%r3, %r60, %r60, %r62;
	setp.geu.ftz.f32 	%p3, %r3, 0f42C80000;
	@%p3 bra 	$L__BB8_4;
	neg.ftz.f32 	%r63, %r3;
	mov.b32 	%r64, 0f42900000;
	div.approx.ftz.f32 	%r65, %r63, %r64;
	mul.ftz.f32 	%r66, %r65, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%r140, %r66;
$L__BB8_4:
	mul.lo.s32 	%r6, %r40, %r1;
	add.s32 	%r7, %r6, %r2;
	mul.wide.s32 	%rd20, %r7, 4;
	add.s64 	%rd21, %rd2, %rd20;
	st.global.b32 	[%rd21], %r140;
	bar.sync 	0;
	setp.ne.s32 	%p4, %r2, 0;
	mul.wide.u32 	%rd22, %r1, 4;
	add.s64 	%rd3, %rd1, %rd22;
	@%p4 bra 	$L__BB8_18;
	and.b32 	%r8, %r40, 15;
	setp.lt.u32 	%p5, %r40, 16;
	mov.b32 	%r154, 0f00000000;
	mov.b32 	%r147, 0;
	@%p5 bra 	$L__BB8_8;
	and.b32 	%r147, %r40, 2147483632;
	neg.s32 	%r141, %r147;
	mul.wide.u32 	%rd23, %r6, 4;
	add.s64 	%rd24, %rd23, %rd2;
	add.s64 	%rd48, %rd24, 32;
	mov.b32 	%r154, 0f00000000;
$L__BB8_7:
	.pragma "nounroll";
	ld.global.b32 	%r71, [%rd48+-32];
	add.ftz.f32 	%r72, %r154, %r71;
	ld.global.b32 	%r73, [%rd48+-28];
	add.ftz.f32 	%r74, %r72, %r73;
	ld.global.b32 	%r75, [%rd48+-24];
	add.ftz.f32 	%r76, %r74, %r75;
	ld.global.b32 	%r77, [%rd48+-20];
	add.ftz.f32 	%r78, %r76, %r77;
	ld.global.b32 	%r79, [%rd48+-16];
	add.ftz.f32 	%r80, %r78, %r79;
	ld.global.b32 	%r81, [%rd48+-12];
	add.ftz.f32 	%r82, %r80, %r81;
	ld.global.b32 	%r83, [%rd48+-8];
	add.ftz.f32 	%r84, %r82, %r83;
	ld.global.b32 	%r85, [%rd48+-4];
	add.ftz.f32 	%r86, %r84, %r85;
	ld.global.b32 	%r87, [%rd48];
	add.ftz.f32 	%r88, %r86, %r87;
	ld.global.b32 	%r89, [%rd48+4];
	add.ftz.f32 	%r90, %r88, %r89;
	ld.global.b32 	%r91, [%rd48+8];
	add.ftz.f32 	%r92, %r90, %r91;
	ld.global.b32 	%r93, [%rd48+12];
	add.ftz.f32 	%r94, %r92, %r93;
	ld.global.b32 	%r95, [%rd48+16];
	add.ftz.f32 	%r96, %r94, %r95;
	ld.global.b32 	%r97, [%rd48+20];
	add.ftz.f32 	%r98, %r96, %r97;
	ld.global.b32 	%r99, [%rd48+24];
	add.ftz.f32 	%r100, %r98, %r99;
	ld.global.b32 	%r101, [%rd48+28];
	add.ftz.f32 	%r154, %r100, %r101;
	add.s32 	%r141, %r141, 16;
	add.s64 	%rd48, %rd48, 64;
	setp.ne.s32 	%p6, %r141, 0;
	@%p6 bra 	$L__BB8_7;
$L__BB8_8:
	setp.eq.s32 	%p7, %r8, 0;
	@%p7 bra 	$L__BB8_17;
	and.b32 	%r18, %r40, 7;
	setp.lt.u32 	%p8, %r8, 8;
	@%p8 bra 	$L__BB8_11;
	add.s32 	%r103, %r147, %r6;
	mul.wide.u32 	%rd25, %r103, 4;
	add.s64 	%rd26, %rd2, %rd25;
	ld.global.b32 	%r104, [%rd26];
	add.ftz.f32 	%r105, %r154, %r104;
	cvt.u64.u32 	%rd27, %r6;
	cvt.u64.u32 	%rd28, %r147;
	add.s64 	%rd29, %rd28, %rd27;
	shl.b64 	%rd30, %rd29, 2;
	add.s64 	%rd31, %rd2, %rd30;
	ld.global.b32 	%r106, [%rd31+4];
	add.ftz.f32 	%r107, %r105, %r106;
	ld.global.b32 	%r108, [%rd31+8];
	add.ftz.f32 	%r109, %r107, %r108;
	ld.global.b32 	%r110, [%rd31+12];
	add.ftz.f32 	%r111, %r109, %r110;
	ld.global.b32 	%r112, [%rd31+16];
	add.ftz.f32 	%r113, %r111, %r112;
	ld.global.b32 	%r114, [%rd31+20];
	add.ftz.f32 	%r115, %r113, %r114;
	ld.global.b32 	%r116, [%rd31+24];
	add.ftz.f32 	%r117, %r115, %r116;
	ld.global.b32 	%r118, [%rd31+28];
	add.ftz.f32 	%r154, %r117, %r118;
	add.s32 	%r147, %r147, 8;
$L__BB8_11:
	setp.eq.s32 	%p9, %r18, 0;
	@%p9 bra 	$L__BB8_17;
	and.b32 	%r24, %r40, 3;
	setp.lt.u32 	%p10, %r18, 4;
	@%p10 bra 	$L__BB8_14;
	add.s32 	%r120, %r147, %r6;
	mul.wide.u32 	%rd32, %r120, 4;
	add.s64 	%rd33, %rd2, %rd32;
	ld.global.b32 	%r121, [%rd33];
	add.ftz.f32 	%r122, %r154, %r121;
	cvt.u64.u32 	%rd34, %r6;
	cvt.u64.u32 	%rd35, %r147;
	add.s64 	%rd36, %rd35, %rd34;
	shl.b64 	%rd37, %rd36, 2;
	add.s64 	%rd38, %rd2, %rd37;
	ld.global.b32 	%r123, [%rd38+4];
	add.ftz.f32 	%r124, %r122, %r123;
	ld.global.b32 	%r125, [%rd38+8];
	add.ftz.f32 	%r126, %r124, %r125;
	ld.global.b32 	%r127, [%rd38+12];
	add.ftz.f32 	%r154, %r126, %r127;
	add.s32 	%r147, %r147, 4;
$L__BB8_14:
	setp.eq.s32 	%p11, %r24, 0;
	@%p11 bra 	$L__BB8_17;
	add.s32 	%r128, %r147, %r6;
	mul.wide.u32 	%rd39, %r128, 4;
	add.s64 	%rd49, %rd2, %rd39;
	neg.s32 	%r152, %r24;
$L__BB8_16:
	.pragma "nounroll";
	ld.global.b32 	%r129, [%rd49];
	add.ftz.f32 	%r154, %r154, %r129;
	add.s64 	%rd49, %rd49, 4;
	add.s32 	%r152, %r152, 1;
	setp.ne.s32 	%p12, %r152, 0;
	@%p12 bra 	$L__BB8_16;
$L__BB8_17:
	st.global.b32 	[%rd3], %r154;
$L__BB8_18:
	setp.eq.s32 	%p13, %r1, %r2;
	bar.sync 	0;
	mov.b32 	%r155, 0f3F800000;
	@%p13 bra 	$L__BB8_21;
	ld.global.b32 	%r36, [%rd3];
	mul.wide.s32 	%rd40, %r2, 4;
	add.s64 	%rd41, %rd1, %rd40;
	ld.global.b32 	%r132, [%rd41];
	setp.leu.ftz.f32 	%p14, %r36, 0f2EDBE6FF;
	setp.leu.ftz.f32 	%p15, %r132, 0f2EDBE6FF;
	or.pred 	%p16, %p14, %p15;
	setp.leu.ftz.f32 	%p17, %r140, 0f00000000;
	mov.b32 	%r155, 0f00000000;
	or.pred 	%p18, %p16, %p17;
	@%p18 bra 	$L__BB8_21;
	neg.ftz.f32 	%r133, %r140;
	rsqrt.approx.ftz.f32 	%r134, %r36;
	mul.ftz.f32 	%r135, %r134, %r133;
	rsqrt.approx.ftz.f32 	%r136, %r132;
	mul.ftz.f32 	%r155, %r135, %r136;
$L__BB8_21:
	setp.ne.s32 	%p19, %r2, 0;
	cvta.to.global.u64 	%rd42, %rd11;
	add.s64 	%rd44, %rd42, %rd20;
	st.global.b32 	[%rd44], %r155;
	@%p19 bra 	$L__BB8_23;
	cvt.rn.f32.u32 	%r137, %r40;
	rsqrt.approx.ftz.f32 	%r138, %r137;
	cvta.to.global.u64 	%rd45, %rd12;
	add.s64 	%rd47, %rd45, %rd22;
	st.global.b32 	[%rd47], %r138;
$L__BB8_23:
	ret;

}
