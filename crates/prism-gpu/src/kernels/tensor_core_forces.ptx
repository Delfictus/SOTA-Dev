//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-37061995
// Cuda compilation tools, release 13.1, V13.1.115
// Based on NVVM 21.0.0
//

.version 9.1
.target sm_120
.address_size 64

	// .globl	tensor_core_nonbonded
// _ZZ21tensor_core_nonbondedE8s_energy has been demoted
// _ZZ29tensor_core_nonbonded_batchedE8s_energy has been demoted

.visible .entry tensor_core_nonbonded(
	.param .u64 .ptr .align 1 tensor_core_nonbonded_param_0,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_param_1,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_param_2,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_param_3,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_param_4,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_param_5,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_param_6,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_param_7,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_param_8,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_param_9,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_param_10,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_param_11,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_param_12,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_param_13,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_param_14,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_param_15,
	.param .u32 tensor_core_nonbonded_param_16,
	.param .u32 tensor_core_nonbonded_param_17
)
{
	.reg .pred 	%p<28>;
	.reg .b32 	%r<168>;
	.reg .b64 	%rd<49>;
	// demoted variable
	.shared .align 4 .b8 _ZZ21tensor_core_nonbondedE8s_energy[128];
	ld.param.b64 	%rd17, [tensor_core_nonbonded_param_0];
	ld.param.b64 	%rd18, [tensor_core_nonbonded_param_2];
	ld.param.b64 	%rd19, [tensor_core_nonbonded_param_4];
	ld.param.b64 	%rd20, [tensor_core_nonbonded_param_5];
	ld.param.b64 	%rd21, [tensor_core_nonbonded_param_6];
	ld.param.b64 	%rd12, [tensor_core_nonbonded_param_11];
	ld.param.b64 	%rd13, [tensor_core_nonbonded_param_12];
	ld.param.b64 	%rd14, [tensor_core_nonbonded_param_13];
	ld.param.b64 	%rd15, [tensor_core_nonbonded_param_14];
	ld.param.b64 	%rd16, [tensor_core_nonbonded_param_15];
	ld.param.b32 	%r54, [tensor_core_nonbonded_param_16];
	ld.param.b32 	%r56, [tensor_core_nonbonded_param_17];
	cvta.to.global.u64 	%rd1, %rd18;
	cvta.to.global.u64 	%rd2, %rd21;
	cvta.to.global.u64 	%rd3, %rd20;
	cvta.to.global.u64 	%rd4, %rd19;
	cvta.to.global.u64 	%rd5, %rd17;
	mov.u32 	%r1, %tid.x;
	shr.u32 	%r2, %r1, 5;
	and.b32 	%r3, %r1, 31;
	mov.u32 	%r57, %ctaid.x;
	shl.b32 	%r58, %r57, 3;
	add.s32 	%r4, %r58, %r2;
	setp.ge.s32 	%p2, %r4, %r56;
	mov.b32 	%r151, 0f00000000;
	@%p2 bra 	$L__BB0_14;
	cvta.to.global.u64 	%rd6, %rd16;
	cvta.to.global.u64 	%rd7, %rd12;
	cvta.to.global.u64 	%rd22, %rd13;
	mul.lo.s32 	%r5, %r4, 3;
	cvt.s64.s32 	%rd8, %r4;
	mul.wide.s32 	%rd23, %r4, 4;
	add.s64 	%rd24, %rd22, %rd23;
	ld.global.nc.b32 	%r6, [%rd24];
	setp.lt.s32 	%p3, %r6, 1;
	mov.b32 	%r154, 0f00000000;
	mov.b32 	%r153, %r154;
	mov.b32 	%r152, %r154;
	mov.b32 	%r151, %r154;
	@%p3 bra 	$L__BB0_13;
	mul.wide.s32 	%rd25, %r5, 4;
	add.s64 	%rd26, %rd5, %rd25;
	ld.global.nc.b32 	%r7, [%rd26];
	ld.global.nc.b32 	%r8, [%rd26+4];
	ld.global.nc.b32 	%r9, [%rd26+8];
	shl.b64 	%rd27, %rd8, 2;
	add.s64 	%rd28, %rd4, %rd27;
	ld.global.nc.b32 	%r10, [%rd28];
	add.s64 	%rd29, %rd3, %rd27;
	ld.global.nc.b32 	%r11, [%rd29];
	add.s64 	%rd30, %rd7, %rd27;
	ld.global.nc.b32 	%r62, [%rd30];
	add.s64 	%rd31, %rd6, %rd27;
	ld.global.nc.b32 	%r63, [%rd31];
	add.s64 	%rd32, %rd2, %rd27;
	ld.global.nc.b32 	%r64, [%rd32];
	add.s32 	%r12, %r62, %r3;
	min.s32 	%r14, %r63, %r54;
	setp.lt.s32 	%p1, %r14, 1;
	mul.lo.s32 	%r13, %r54, %r4;
	mul.ftz.f32 	%r15, %r64, 0f43A60824;
	cvta.to.global.u64 	%rd9, %rd14;
	cvta.to.global.u64 	%rd10, %rd15;
	mov.b32 	%r151, 0f00000000;
	mov.b32 	%r150, 0;
	mov.b32 	%r152, %r151;
	mov.b32 	%r153, %r151;
	mov.b32 	%r154, %r151;
$L__BB0_3:
	sub.s32 	%r65, %r6, %r150;
	min.s32 	%r66, %r65, 8;
	setp.ge.u32 	%p4, %r3, %r66;
	bar.warp.sync 	-1;
	bar.warp.sync 	-1;
	@%p4 bra 	$L__BB0_12;
	add.s32 	%r67, %r12, %r150;
	mul.wide.s32 	%rd33, %r67, 4;
	add.s64 	%rd34, %rd9, %rd33;
	ld.global.nc.b32 	%r21, [%rd34];
	@%p1 bra 	$L__BB0_8;
	mov.b32 	%r155, 0;
	bra.uni 	$L__BB0_7;
$L__BB0_6:
	add.s32 	%r155, %r155, 1;
	setp.eq.s32 	%p6, %r155, %r14;
	@%p6 bra 	$L__BB0_8;
$L__BB0_7:
	add.s32 	%r69, %r155, %r13;
	mul.wide.s32 	%rd35, %r69, 4;
	add.s64 	%rd36, %rd10, %rd35;
	ld.global.nc.b32 	%r70, [%rd36];
	setp.eq.s32 	%p5, %r70, %r21;
	@%p5 bra 	$L__BB0_12;
	bra.uni 	$L__BB0_6;
$L__BB0_8:
	mul.lo.s32 	%r24, %r21, 3;
	mul.wide.s32 	%rd37, %r24, 4;
	add.s64 	%rd38, %rd5, %rd37;
	ld.global.nc.b32 	%r71, [%rd38];
	ld.global.nc.b32 	%r72, [%rd38+4];
	ld.global.nc.b32 	%r73, [%rd38+8];
	sub.ftz.f32 	%r25, %r71, %r7;
	sub.ftz.f32 	%r26, %r72, %r8;
	sub.ftz.f32 	%r27, %r73, %r9;
	mul.ftz.f32 	%r74, %r26, %r26;
	fma.rn.ftz.f32 	%r75, %r25, %r25, %r74;
	fma.rn.ftz.f32 	%r28, %r27, %r27, %r75;
	setp.geu.ftz.f32 	%p7, %r28, 0f42C80000;
	setp.leu.ftz.f32 	%p8, %r28, 0f2EDBE6FF;
	or.pred 	%p9, %p7, %p8;
	@%p9 bra 	$L__BB0_12;
	sqrt.approx.ftz.f32 	%r76, %r28;
	add.ftz.f32 	%r77, %r76, 0f3DCCCCCD;
	rcp.approx.ftz.f32 	%r78, %r77;
	mul.ftz.f32 	%r79, %r78, %r78;
	mul.ftz.f32 	%r80, %r79, %r79;
	mul.ftz.f32 	%r81, %r79, %r80;
	mul.wide.s32 	%rd39, %r21, 4;
	add.s64 	%rd40, %rd4, %rd39;
	ld.global.nc.b32 	%r82, [%rd40];
	add.s64 	%rd41, %rd3, %rd39;
	ld.global.nc.b32 	%r83, [%rd41];
	add.ftz.f32 	%r84, %r10, %r82;
	mul.ftz.f32 	%r85, %r84, 0f3F000000;
	mul.ftz.f32 	%r86, %r11, %r83;
	sqrt.approx.ftz.f32 	%r87, %r86;
	mul.ftz.f32 	%r88, %r85, %r85;
	mul.ftz.f32 	%r89, %r85, %r88;
	mul.ftz.f32 	%r90, %r89, %r89;
	mul.ftz.f32 	%r91, %r90, %r90;
	mul.ftz.f32 	%r92, %r87, 0f40800000;
	mul.ftz.f32 	%r93, %r81, %r91;
	mul.ftz.f32 	%r94, %r81, %r93;
	mul.ftz.f32 	%r95, %r81, %r90;
	sub.ftz.f32 	%r96, %r94, %r95;
	mul.ftz.f32 	%r29, %r92, %r96;
	mul.ftz.f32 	%r97, %r87, 0f41C00000;
	add.ftz.f32 	%r98, %r91, %r91;
	mul.ftz.f32 	%r99, %r81, %r98;
	mul.ftz.f32 	%r100, %r81, %r99;
	sub.ftz.f32 	%r101, %r100, %r95;
	mul.ftz.f32 	%r102, %r97, %r101;
	mul.ftz.f32 	%r103, %r79, %r102;
	add.s64 	%rd42, %rd2, %rd39;
	ld.global.nc.b32 	%r104, [%rd42];
	mul.ftz.f32 	%r105, %r15, %r104;
	mul.ftz.f32 	%r106, %r105, 0f3E800000;
	mul.ftz.f32 	%r30, %r79, %r106;
	add.ftz.f32 	%r107, %r30, %r30;
	fma.rn.ftz.f32 	%r156, %r78, %r107, %r103;
	abs.ftz.f32 	%r108, %r156;
	mul.ftz.f32 	%r32, %r76, %r108;
	setp.leu.ftz.f32 	%p10, %r32, 0f42A00000;
	@%p10 bra 	$L__BB0_11;
	mov.b32 	%r109, 0f42A00000;
	div.approx.ftz.f32 	%r110, %r109, %r32;
	mul.ftz.f32 	%r156, %r156, %r110;
$L__BB0_11:
	mul.ftz.f32 	%r111, %r25, %r156;
	sub.ftz.f32 	%r154, %r154, %r111;
	mul.ftz.f32 	%r112, %r26, %r156;
	sub.ftz.f32 	%r153, %r153, %r112;
	mul.ftz.f32 	%r113, %r27, %r156;
	sub.ftz.f32 	%r152, %r152, %r113;
	mul.wide.s32 	%rd43, %r24, 4;
	add.s64 	%rd44, %rd1, %rd43;
	atom.global.add.f32 	%r114, [%rd44], %r111;
	atom.global.add.f32 	%r115, [%rd44+4], %r112;
	atom.global.add.f32 	%r116, [%rd44+8], %r113;
	add.ftz.f32 	%r117, %r30, %r29;
	add.ftz.f32 	%r151, %r151, %r117;
$L__BB0_12:
	bar.warp.sync 	-1;
	add.s32 	%r150, %r150, 8;
	setp.lt.s32 	%p11, %r150, %r6;
	@%p11 bra 	$L__BB0_3;
$L__BB0_13:
	mul.wide.s32 	%rd45, %r5, 4;
	add.s64 	%rd46, %rd1, %rd45;
	atom.global.add.f32 	%r118, [%rd46], %r154;
	atom.global.add.f32 	%r119, [%rd46+4], %r153;
	atom.global.add.f32 	%r120, [%rd46+8], %r152;
$L__BB0_14:
	shfl.sync.down.b32 	%r121|%p12, %r151, 16, 31, -1;
	add.ftz.f32 	%r122, %r151, %r121;
	shfl.sync.down.b32 	%r123|%p13, %r122, 8, 31, -1;
	add.ftz.f32 	%r124, %r122, %r123;
	shfl.sync.down.b32 	%r125|%p14, %r124, 4, 31, -1;
	add.ftz.f32 	%r126, %r124, %r125;
	shfl.sync.down.b32 	%r127|%p15, %r126, 2, 31, -1;
	add.ftz.f32 	%r128, %r126, %r127;
	shfl.sync.down.b32 	%r129|%p16, %r128, 1, 31, -1;
	add.ftz.f32 	%r167, %r128, %r129;
	setp.ne.s32 	%p17, %r3, 0;
	@%p17 bra 	$L__BB0_16;
	shl.b32 	%r130, %r2, 2;
	mov.b32 	%r131, _ZZ21tensor_core_nonbondedE8s_energy;
	add.s32 	%r132, %r131, %r130;
	st.shared.b32 	[%r132], %r167;
$L__BB0_16:
	bar.sync 	0;
	setp.gt.u32 	%p18, %r1, 31;
	@%p18 bra 	$L__BB0_20;
	mov.u32 	%r134, %ntid.x;
	add.s32 	%r135, %r134, 31;
	shr.u32 	%r136, %r135, 5;
	setp.ge.u32 	%p19, %r3, %r136;
	mov.b32 	%r166, 0f00000000;
	@%p19 bra 	$L__BB0_19;
	shl.b32 	%r137, %r3, 2;
	mov.b32 	%r138, _ZZ21tensor_core_nonbondedE8s_energy;
	add.s32 	%r139, %r138, %r137;
	ld.shared.b32 	%r166, [%r139];
$L__BB0_19:
	shfl.sync.down.b32 	%r140|%p20, %r166, 16, 31, -1;
	add.ftz.f32 	%r141, %r166, %r140;
	shfl.sync.down.b32 	%r142|%p21, %r141, 8, 31, -1;
	add.ftz.f32 	%r143, %r141, %r142;
	shfl.sync.down.b32 	%r144|%p22, %r143, 4, 31, -1;
	add.ftz.f32 	%r145, %r143, %r144;
	shfl.sync.down.b32 	%r146|%p23, %r145, 2, 31, -1;
	add.ftz.f32 	%r147, %r145, %r146;
	shfl.sync.down.b32 	%r148|%p24, %r147, 1, 31, -1;
	add.ftz.f32 	%r167, %r147, %r148;
$L__BB0_20:
	setp.ne.s32 	%p25, %r1, 0;
	setp.eq.ftz.f32 	%p26, %r167, 0f00000000;
	or.pred 	%p27, %p25, %p26;
	@%p27 bra 	$L__BB0_22;
	ld.param.b64 	%rd48, [tensor_core_nonbonded_param_3];
	cvta.to.global.u64 	%rd47, %rd48;
	atom.global.add.f32 	%r149, [%rd47], %r167;
$L__BB0_22:
	ret;

}
	// .globl	precompute_norm_squared
.visible .entry precompute_norm_squared(
	.param .u64 .ptr .align 1 precompute_norm_squared_param_0,
	.param .u64 .ptr .align 1 precompute_norm_squared_param_1,
	.param .u32 precompute_norm_squared_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd1, [precompute_norm_squared_param_0];
	ld.param.b64 	%rd2, [precompute_norm_squared_param_1];
	ld.param.b32 	%r2, [precompute_norm_squared_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB1_2;
	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.lo.s32 	%r6, %r1, 3;
	mul.wide.s32 	%rd5, %r6, 4;
	add.s64 	%rd6, %rd3, %rd5;
	ld.global.nc.b32 	%r7, [%rd6];
	ld.global.nc.b32 	%r8, [%rd6+4];
	ld.global.nc.b32 	%r9, [%rd6+8];
	mul.ftz.f32 	%r10, %r8, %r8;
	fma.rn.ftz.f32 	%r11, %r7, %r7, %r10;
	fma.rn.ftz.f32 	%r12, %r9, %r9, %r11;
	mul.wide.s32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd4, %rd7;
	st.global.b32 	[%rd8], %r12;
$L__BB1_2:
	ret;

}
	// .globl	convert_params_to_fp16
.visible .entry convert_params_to_fp16(
	.param .u64 .ptr .align 1 convert_params_to_fp16_param_0,
	.param .u64 .ptr .align 1 convert_params_to_fp16_param_1,
	.param .u64 .ptr .align 1 convert_params_to_fp16_param_2,
	.param .u64 .ptr .align 1 convert_params_to_fp16_param_3,
	.param .u64 .ptr .align 1 convert_params_to_fp16_param_4,
	.param .u64 .ptr .align 1 convert_params_to_fp16_param_5,
	.param .u32 convert_params_to_fp16_param_6
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<21>;

	ld.param.b64 	%rd1, [convert_params_to_fp16_param_0];
	ld.param.b64 	%rd2, [convert_params_to_fp16_param_1];
	ld.param.b64 	%rd3, [convert_params_to_fp16_param_2];
	ld.param.b64 	%rd4, [convert_params_to_fp16_param_3];
	ld.param.b64 	%rd5, [convert_params_to_fp16_param_4];
	ld.param.b64 	%rd6, [convert_params_to_fp16_param_5];
	ld.param.b32 	%r2, [convert_params_to_fp16_param_6];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB2_2;
	cvta.to.global.u64 	%rd7, %rd1;
	cvta.to.global.u64 	%rd8, %rd4;
	cvta.to.global.u64 	%rd9, %rd2;
	cvta.to.global.u64 	%rd10, %rd5;
	cvta.to.global.u64 	%rd11, %rd3;
	cvta.to.global.u64 	%rd12, %rd6;
	mul.wide.s32 	%rd13, %r1, 4;
	add.s64 	%rd14, %rd7, %rd13;
	ld.global.nc.b32 	%r6, [%rd14];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %r6;}

	// end inline asm
	mul.wide.s32 	%rd15, %r1, 2;
	add.s64 	%rd16, %rd8, %rd15;
	st.global.b16 	[%rd16], %rs1;
	add.s64 	%rd17, %rd9, %rd13;
	ld.global.nc.b32 	%r7, [%rd17];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %r7;}

	// end inline asm
	add.s64 	%rd18, %rd10, %rd15;
	st.global.b16 	[%rd18], %rs2;
	add.s64 	%rd19, %rd11, %rd13;
	ld.global.nc.b32 	%r8, [%rd19];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %r8;}

	// end inline asm
	add.s64 	%rd20, %rd12, %rd15;
	st.global.b16 	[%rd20], %rs3;
$L__BB2_2:
	ret;

}
	// .globl	convert_positions_to_fp16
.visible .entry convert_positions_to_fp16(
	.param .u64 .ptr .align 1 convert_positions_to_fp16_param_0,
	.param .u64 .ptr .align 1 convert_positions_to_fp16_param_1,
	.param .u32 convert_positions_to_fp16_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd1, [convert_positions_to_fp16_param_0];
	ld.param.b64 	%rd2, [convert_positions_to_fp16_param_1];
	ld.param.b32 	%r2, [convert_positions_to_fp16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB3_2;
	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.lo.s32 	%r9, %r1, 3;
	mul.wide.s32 	%rd5, %r9, 4;
	add.s64 	%rd6, %rd3, %rd5;
	ld.global.nc.b32 	%r6, [%rd6];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %r6;}

	// end inline asm
	mul.wide.s32 	%rd7, %r9, 2;
	add.s64 	%rd8, %rd4, %rd7;
	st.global.b16 	[%rd8], %rs1;
	ld.global.nc.b32 	%r7, [%rd6+4];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %r7;}

	// end inline asm
	st.global.b16 	[%rd8+2], %rs2;
	ld.global.nc.b32 	%r8, [%rd6+8];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %r8;}

	// end inline asm
	st.global.b16 	[%rd8+4], %rs3;
$L__BB3_2:
	ret;

}
	// .globl	tensor_core_nonbonded_batched
.visible .entry tensor_core_nonbonded_batched(
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_0,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_1,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_2,
	.param .u32 tensor_core_nonbonded_batched_param_3,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_4,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_5,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_6,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_7,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_8,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_9,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_10,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_11,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_12,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_13,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_14,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_15,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_16,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_17,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_18,
	.param .u64 .ptr .align 1 tensor_core_nonbonded_batched_param_19,
	.param .u32 tensor_core_nonbonded_batched_param_20
)
{
	.reg .pred 	%p<30>;
	.reg .b32 	%r<184>;
	.reg .b64 	%rd<59>;
	// demoted variable
	.shared .align 4 .b8 _ZZ29tensor_core_nonbonded_batchedE8s_energy[128];
	ld.param.b64 	%rd9, [tensor_core_nonbonded_batched_param_0];
	ld.param.b64 	%rd10, [tensor_core_nonbonded_batched_param_1];
	ld.param.b64 	%rd11, [tensor_core_nonbonded_batched_param_2];
	ld.param.b32 	%r64, [tensor_core_nonbonded_batched_param_3];
	ld.param.b64 	%rd18, [tensor_core_nonbonded_batched_param_4];
	ld.param.b64 	%rd19, [tensor_core_nonbonded_batched_param_6];
	ld.param.b64 	%rd20, [tensor_core_nonbonded_batched_param_8];
	ld.param.b64 	%rd21, [tensor_core_nonbonded_batched_param_9];
	ld.param.b64 	%rd22, [tensor_core_nonbonded_batched_param_10];
	ld.param.b64 	%rd13, [tensor_core_nonbonded_batched_param_15];
	ld.param.b64 	%rd14, [tensor_core_nonbonded_batched_param_16];
	ld.param.b64 	%rd15, [tensor_core_nonbonded_batched_param_17];
	ld.param.b64 	%rd16, [tensor_core_nonbonded_batched_param_18];
	ld.param.b64 	%rd17, [tensor_core_nonbonded_batched_param_19];
	ld.param.b32 	%r65, [tensor_core_nonbonded_batched_param_20];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd22;
	cvta.to.global.u64 	%rd3, %rd21;
	cvta.to.global.u64 	%rd4, %rd20;
	cvta.to.global.u64 	%rd5, %rd18;
	mov.u32 	%r67, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %tid.x;
	mad.lo.s32 	%r3, %r67, %r1, %r2;
	setp.lt.s32 	%p2, %r64, 1;
	mov.b32 	%r164, -1;
	mov.b32 	%r165, %r164;
	@%p2 bra 	$L__BB4_5;
	cvta.to.global.u64 	%rd6, %rd10;
	mov.b32 	%r162, 0;
	mov.b32 	%r163, %r162;
$L__BB4_2:
	mul.wide.u32 	%rd23, %r162, 4;
	add.s64 	%rd24, %rd6, %rd23;
	ld.global.nc.b32 	%r69, [%rd24];
	add.s32 	%r6, %r69, %r163;
	setp.ge.s32 	%p3, %r3, %r6;
	@%p3 bra 	$L__BB4_4;
	sub.s32 	%r165, %r3, %r163;
	mov.b32 	%r164, %r162;
	bra.uni 	$L__BB4_5;
$L__BB4_4:
	add.s32 	%r162, %r162, 1;
	setp.ne.s32 	%p4, %r162, %r64;
	mov.b32 	%r163, %r6;
	mov.b32 	%r165, %r164;
	@%p4 bra 	$L__BB4_2;
$L__BB4_5:
	or.b32 	%r72, %r164, %r165;
	mov.b32 	%r170, 0f00000000;
	setp.lt.s32 	%p5, %r72, 0;
	@%p5 bra 	$L__BB4_18;
	cvta.to.global.u64 	%rd25, %rd9;
	mul.wide.u32 	%rd26, %r164, 4;
	add.s64 	%rd27, %rd25, %rd26;
	ld.global.nc.b32 	%r11, [%rd27];
	add.s32 	%r12, %r11, %r165;
	mul.lo.s32 	%r13, %r12, 3;
	cvta.to.global.u64 	%rd28, %rd11;
	add.s64 	%rd29, %rd28, %rd26;
	ld.global.nc.b32 	%r74, [%rd29];
	add.s32 	%r75, %r74, %r165;
	cvta.to.global.u64 	%rd30, %rd13;
	mul.wide.s32 	%rd31, %r75, 4;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.nc.b32 	%r14, [%rd32];
	cvta.to.global.u64 	%rd33, %rd14;
	add.s64 	%rd34, %rd33, %rd31;
	ld.global.nc.b32 	%r15, [%rd34];
	setp.lt.s32 	%p6, %r15, 1;
	mov.b32 	%r170, 0f00000000;
	mov.b32 	%r169, %r170;
	mov.b32 	%r168, %r170;
	mov.b32 	%r167, %r170;
	@%p6 bra 	$L__BB4_17;
	mul.wide.s32 	%rd35, %r13, 4;
	add.s64 	%rd36, %rd5, %rd35;
	ld.global.nc.b32 	%r16, [%rd36];
	ld.global.nc.b32 	%r17, [%rd36+4];
	ld.global.nc.b32 	%r18, [%rd36+8];
	mul.wide.s32 	%rd37, %r12, 4;
	add.s64 	%rd38, %rd4, %rd37;
	ld.global.nc.b32 	%r19, [%rd38];
	add.s64 	%rd39, %rd3, %rd37;
	ld.global.nc.b32 	%r20, [%rd39];
	cvta.to.global.u64 	%rd40, %rd17;
	add.s64 	%rd41, %rd40, %rd37;
	ld.global.nc.b32 	%r78, [%rd41];
	add.s64 	%rd42, %rd2, %rd37;
	ld.global.nc.b32 	%r79, [%rd42];
	min.s32 	%r22, %r78, %r65;
	setp.lt.s32 	%p1, %r22, 1;
	mul.lo.s32 	%r21, %r12, %r65;
	mul.ftz.f32 	%r23, %r79, 0f43A60824;
	cvta.to.global.u64 	%rd7, %rd15;
	cvta.to.global.u64 	%rd8, %rd16;
	mov.b32 	%r167, 0f00000000;
	mov.b32 	%r166, 0;
	mov.b32 	%r168, %r167;
	mov.b32 	%r169, %r167;
	mov.b32 	%r170, %r167;
$L__BB4_8:
	add.s32 	%r80, %r166, %r14;
	mul.wide.s32 	%rd43, %r80, 4;
	add.s64 	%rd44, %rd7, %rd43;
	ld.global.nc.b32 	%r81, [%rd44];
	add.s32 	%r29, %r81, %r11;
	@%p1 bra 	$L__BB4_12;
	mov.b32 	%r171, 0;
	bra.uni 	$L__BB4_11;
$L__BB4_10:
	add.s32 	%r171, %r171, 1;
	setp.eq.s32 	%p8, %r171, %r22;
	@%p8 bra 	$L__BB4_12;
$L__BB4_11:
	add.s32 	%r83, %r171, %r21;
	mul.wide.s32 	%rd45, %r83, 4;
	add.s64 	%rd46, %rd8, %rd45;
	ld.global.nc.b32 	%r84, [%rd46];
	setp.eq.s32 	%p7, %r84, %r29;
	@%p7 bra 	$L__BB4_16;
	bra.uni 	$L__BB4_10;
$L__BB4_12:
	mul.lo.s32 	%r32, %r29, 3;
	mul.wide.s32 	%rd47, %r32, 4;
	add.s64 	%rd48, %rd5, %rd47;
	ld.global.nc.b32 	%r85, [%rd48];
	ld.global.nc.b32 	%r86, [%rd48+4];
	ld.global.nc.b32 	%r87, [%rd48+8];
	sub.ftz.f32 	%r33, %r85, %r16;
	sub.ftz.f32 	%r34, %r86, %r17;
	sub.ftz.f32 	%r35, %r87, %r18;
	mul.ftz.f32 	%r88, %r34, %r34;
	fma.rn.ftz.f32 	%r89, %r33, %r33, %r88;
	fma.rn.ftz.f32 	%r36, %r35, %r35, %r89;
	setp.gt.ftz.f32 	%p9, %r36, 0f42C80000;
	setp.lt.ftz.f32 	%p10, %r36, 0f2EDBE6FF;
	or.pred 	%p11, %p9, %p10;
	@%p11 bra 	$L__BB4_16;
	sqrt.approx.ftz.f32 	%r90, %r36;
	add.ftz.f32 	%r91, %r90, 0f3DCCCCCD;
	rcp.approx.ftz.f32 	%r92, %r91;
	mul.ftz.f32 	%r93, %r92, %r92;
	mul.ftz.f32 	%r94, %r93, %r93;
	mul.ftz.f32 	%r95, %r93, %r94;
	mul.wide.s32 	%rd49, %r29, 4;
	add.s64 	%rd50, %rd4, %rd49;
	ld.global.nc.b32 	%r96, [%rd50];
	add.s64 	%rd51, %rd3, %rd49;
	ld.global.nc.b32 	%r97, [%rd51];
	add.ftz.f32 	%r98, %r19, %r96;
	mul.ftz.f32 	%r99, %r98, 0f3F000000;
	mul.ftz.f32 	%r100, %r20, %r97;
	sqrt.approx.ftz.f32 	%r101, %r100;
	mul.ftz.f32 	%r102, %r99, %r99;
	mul.ftz.f32 	%r103, %r99, %r102;
	mul.ftz.f32 	%r104, %r103, %r103;
	mul.ftz.f32 	%r105, %r104, %r104;
	mul.ftz.f32 	%r106, %r101, 0f40800000;
	mul.ftz.f32 	%r107, %r95, %r105;
	mul.ftz.f32 	%r108, %r95, %r107;
	mul.ftz.f32 	%r109, %r95, %r104;
	sub.ftz.f32 	%r110, %r108, %r109;
	mul.ftz.f32 	%r37, %r106, %r110;
	mul.ftz.f32 	%r111, %r101, 0f41C00000;
	add.ftz.f32 	%r112, %r105, %r105;
	mul.ftz.f32 	%r113, %r95, %r112;
	mul.ftz.f32 	%r114, %r95, %r113;
	sub.ftz.f32 	%r115, %r114, %r109;
	mul.ftz.f32 	%r116, %r111, %r115;
	mul.ftz.f32 	%r117, %r93, %r116;
	add.s64 	%rd52, %rd2, %rd49;
	ld.global.nc.b32 	%r118, [%rd52];
	mul.ftz.f32 	%r119, %r23, %r118;
	mul.ftz.f32 	%r120, %r119, 0f3E800000;
	mul.ftz.f32 	%r38, %r93, %r120;
	add.ftz.f32 	%r121, %r38, %r38;
	fma.rn.ftz.f32 	%r172, %r92, %r121, %r117;
	abs.ftz.f32 	%r122, %r172;
	mul.ftz.f32 	%r40, %r90, %r122;
	setp.leu.ftz.f32 	%p12, %r40, 0f42A00000;
	@%p12 bra 	$L__BB4_15;
	mov.b32 	%r123, 0f42A00000;
	div.approx.ftz.f32 	%r124, %r123, %r40;
	mul.ftz.f32 	%r172, %r172, %r124;
$L__BB4_15:
	mul.ftz.f32 	%r125, %r33, %r172;
	sub.ftz.f32 	%r169, %r169, %r125;
	mul.ftz.f32 	%r126, %r34, %r172;
	sub.ftz.f32 	%r168, %r168, %r126;
	mul.ftz.f32 	%r127, %r35, %r172;
	sub.ftz.f32 	%r167, %r167, %r127;
	mul.wide.s32 	%rd53, %r32, 4;
	add.s64 	%rd54, %rd1, %rd53;
	atom.global.add.f32 	%r128, [%rd54], %r125;
	atom.global.add.f32 	%r129, [%rd54+4], %r126;
	atom.global.add.f32 	%r130, [%rd54+8], %r127;
	add.ftz.f32 	%r131, %r38, %r37;
	add.ftz.f32 	%r170, %r170, %r131;
$L__BB4_16:
	add.s32 	%r166, %r166, 1;
	setp.ne.s32 	%p13, %r166, %r15;
	@%p13 bra 	$L__BB4_8;
$L__BB4_17:
	mul.wide.s32 	%rd55, %r13, 4;
	add.s64 	%rd56, %rd1, %rd55;
	atom.global.add.f32 	%r132, [%rd56], %r169;
	atom.global.add.f32 	%r133, [%rd56+4], %r168;
	atom.global.add.f32 	%r134, [%rd56+8], %r167;
$L__BB4_18:
	and.b32 	%r57, %r2, 31;
	add.s32 	%r58, %r1, 31;
	shfl.sync.down.b32 	%r135|%p14, %r170, 16, 31, -1;
	add.ftz.f32 	%r136, %r170, %r135;
	shfl.sync.down.b32 	%r137|%p15, %r136, 8, 31, -1;
	add.ftz.f32 	%r138, %r136, %r137;
	shfl.sync.down.b32 	%r139|%p16, %r138, 4, 31, -1;
	add.ftz.f32 	%r140, %r138, %r139;
	shfl.sync.down.b32 	%r141|%p17, %r140, 2, 31, -1;
	add.ftz.f32 	%r142, %r140, %r141;
	shfl.sync.down.b32 	%r143|%p18, %r142, 1, 31, -1;
	add.ftz.f32 	%r183, %r142, %r143;
	setp.ne.s32 	%p19, %r57, 0;
	@%p19 bra 	$L__BB4_20;
	shr.u32 	%r144, %r2, 3;
	mov.b32 	%r145, _ZZ29tensor_core_nonbonded_batchedE8s_energy;
	add.s32 	%r146, %r145, %r144;
	st.shared.b32 	[%r146], %r183;
$L__BB4_20:
	bar.sync 	0;
	setp.gt.u32 	%p20, %r2, 31;
	@%p20 bra 	$L__BB4_24;
	shr.u32 	%r148, %r58, 5;
	setp.ge.u32 	%p21, %r57, %r148;
	mov.b32 	%r182, 0f00000000;
	@%p21 bra 	$L__BB4_23;
	shl.b32 	%r149, %r57, 2;
	mov.b32 	%r150, _ZZ29tensor_core_nonbonded_batchedE8s_energy;
	add.s32 	%r151, %r150, %r149;
	ld.shared.b32 	%r182, [%r151];
$L__BB4_23:
	shfl.sync.down.b32 	%r152|%p22, %r182, 16, 31, -1;
	add.ftz.f32 	%r153, %r182, %r152;
	shfl.sync.down.b32 	%r154|%p23, %r153, 8, 31, -1;
	add.ftz.f32 	%r155, %r153, %r154;
	shfl.sync.down.b32 	%r156|%p24, %r155, 4, 31, -1;
	add.ftz.f32 	%r157, %r155, %r156;
	shfl.sync.down.b32 	%r158|%p25, %r157, 2, 31, -1;
	add.ftz.f32 	%r159, %r157, %r158;
	shfl.sync.down.b32 	%r160|%p26, %r159, 1, 31, -1;
	add.ftz.f32 	%r183, %r159, %r160;
$L__BB4_24:
	setp.ne.s32 	%p27, %r2, 0;
	setp.eq.ftz.f32 	%p28, %r183, 0f00000000;
	or.pred 	%p29, %p27, %p28;
	@%p29 bra 	$L__BB4_26;
	ld.param.b64 	%rd58, [tensor_core_nonbonded_batched_param_7];
	cvta.to.global.u64 	%rd57, %rd58;
	atom.global.add.f32 	%r161, [%rd57], %r183;
$L__BB4_26:
	ret;

}
