//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-37061995
// Cuda compilation tools, release 13.1, V13.1.115
// Based on NVVM 21.0.0
//

.version 9.1
.target sm_120
.address_size 64

	// .globl	__raygen__count_neighbors
.const .align 8 .b8 params[88];

.visible .entry __raygen__count_neighbors()
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<127>;
	.reg .b64 	%rd<10>;

	// begin inline asm
	call (%r5), _optix_get_launch_index_x, ();
	// end inline asm
	// begin inline asm
	call (%r6), _optix_get_launch_index_y, ();
	// end inline asm
	ld.const.b32 	%r7, [params+16];
	setp.ge.u32 	%p1, %r5, %r7;
	ld.const.b32 	%r8, [params+28];
	setp.ge.u32 	%p2, %r6, %r8;
	or.pred 	%p3, %p1, %p2;
	@%p3 bra 	$L__BB0_3;
	ld.const.b64 	%rd2, [params+8];
	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.u32 	%rd4, %r5, 12;
	add.s64 	%rd5, %rd3, %rd4;
	cvt.rn.f32.u32 	%r89, %r6;
	mul.ftz.f32 	%r90, %r89, 0f40C90FDB;
	mov.b32 	%r91, 0f3FCF1BBD;
	div.approx.ftz.f32 	%r92, %r90, %r91;
	add.ftz.f32 	%r93, %r89, 0f3F000000;
	add.ftz.f32 	%r94, %r93, %r93;
	cvt.rn.f32.u32 	%r95, %r8;
	div.approx.ftz.f32 	%r96, %r94, %r95;
	mov.b32 	%r97, 0f3F800000;
	sub.ftz.f32 	%r98, %r97, %r96;
	abs.ftz.f32 	%r99, %r98;
	fma.rn.ftz.f32 	%r100, %r99, 0fBF000000, 0f3F000000;
	rsqrt.approx.ftz.f32 	%r101, %r100;
	mul.ftz.f32 	%r102, %r101, %r100;
	mul.ftz.f32 	%r103, %r101, 0fBF000000;
	fma.rn.ftz.f32 	%r104, %r102, %r103, 0f3F000000;
	fma.rn.ftz.f32 	%r105, %r102, %r104, %r102;
	setp.eq.ftz.f32 	%p4, %r99, 0f3F800000;
	selp.f32 	%r106, 0f00000000, %r105, %p4;
	setp.gt.ftz.f32 	%p5, %r99, 0f3F0F5C29;
	selp.f32 	%r107, %r106, %r99, %p5;
	copysign.f32 	%r108, %r98, %r107;
	mul.ftz.f32 	%r109, %r108, %r108;
	fma.rn.ftz.f32 	%r110, %r109, 0f3D10ECEF, 0f3C8B1ABB;
	fma.rn.ftz.f32 	%r111, %r110, %r109, 0f3CFC028C;
	fma.rn.ftz.f32 	%r112, %r111, %r109, 0f3D372139;
	fma.rn.ftz.f32 	%r113, %r112, %r109, 0f3D9993DB;
	fma.rn.ftz.f32 	%r114, %r113, %r109, 0f3E2AAAC6;
	mul.ftz.f32 	%r115, %r109, %r114;
	fma.rn.ftz.f32 	%r116, %r115, %r108, %r108;
	neg.ftz.f32 	%r117, %r116;
	selp.f32 	%r118, %r116, %r117, %p5;
	fma.rn.ftz.f32 	%r119, 0f3F6EE581, 0f3FD774EB, %r118;
	setp.gt.ftz.f32 	%p6, %r98, 0f3F0F5C29;
	selp.f32 	%r120, %r116, %r119, %p6;
	add.ftz.f32 	%r121, %r120, %r120;
	selp.f32 	%r122, %r121, %r120, %p5;
	sin.approx.ftz.f32 	%r123, %r122;
	cos.approx.ftz.f32 	%r124, %r92;
	mul.ftz.f32 	%r45, %r123, %r124;
	sin.approx.ftz.f32 	%r125, %r92;
	mul.ftz.f32 	%r46, %r123, %r125;
	cos.approx.ftz.f32 	%r47, %r122;
	ld.const.b64 	%rd1, [params];
	ld.global.b32 	%r44, [%rd5+8];
	ld.global.b32 	%r43, [%rd5+4];
	ld.global.b32 	%r42, [%rd5];
	ld.const.b32 	%r49, [params+20];
	mov.b32 	%r48, 981668463;
	mov.b32 	%r51, 255;
	mov.b32 	%r54, 1;
	mov.b32 	%r56, 2;
	mov.b32 	%r88, 0;
	// begin inline asm
	call(%r9,%r10,%r11,%r12,%r13,%r14,%r15,%r16,%r17,%r18,%r19,%r20,%r21,%r22,%r23,%r24,%r25,%r26,%r27,%r28,%r29,%r30,%r31,%r32,%r33,%r34,%r35,%r36,%r37,%r38,%r39,%r40),_optix_trace_typed_32,(%r88,%rd1,%r42,%r43,%r44,%r45,%r46,%r47,%r48,%r49,%r88,%r51,%r88,%r88,%r54,%r88,%r56,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88,%r88);
	// end inline asm
	setp.eq.s32 	%p7, %r10, 0;
	@%p7 bra 	$L__BB0_3;
	ld.const.b64 	%rd6, [params+32];
	cvta.to.global.u64 	%rd7, %rd6;
	mul.wide.u32 	%rd8, %r5, 4;
	add.s64 	%rd9, %rd7, %rd8;
	atom.global.add.u32 	%r126, [%rd9], %r10;
$L__BB0_3:
	ret;

}
	// .globl	__closesthit__count_neighbors
.visible .entry __closesthit__count_neighbors()
{
	.reg .b32 	%r<8>;

	// begin inline asm
	call (%r1), _optix_read_primitive_idx, ();
	// end inline asm
	mov.b32 	%r4, 1;
	// begin inline asm
	call (%r2), _optix_get_payload, (%r4);
	// end inline asm
	add.s32 	%r5, %r2, 1;
	// begin inline asm
	call _optix_set_payload, (%r4, %r5);
	// end inline asm
	mov.b32 	%r6, 0;
	// begin inline asm
	call _optix_set_payload, (%r6, %r1);
	// end inline asm
	ret;

}
	// .globl	__miss__count_neighbors
.visible .entry __miss__count_neighbors()
{


	ret;

}
	// .globl	compute_neighbor_offsets
.visible .entry compute_neighbor_offsets(
	.param .u64 .ptr .align 1 compute_neighbor_offsets_param_0,
	.param .u64 .ptr .align 1 compute_neighbor_offsets_param_1,
	.param .u32 compute_neighbor_offsets_param_2,
	.param .u64 .ptr .align 1 compute_neighbor_offsets_param_3
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<107>;
	.reg .b64 	%rd<32>;

	ld.param.b64 	%rd16, [compute_neighbor_offsets_param_0];
	ld.param.b64 	%rd17, [compute_neighbor_offsets_param_1];
	ld.param.b32 	%r29, [compute_neighbor_offsets_param_2];
	ld.param.b64 	%rd15, [compute_neighbor_offsets_param_3];
	cvta.to.global.u64 	%rd1, %rd16;
	cvta.to.global.u64 	%rd2, %rd17;
	mov.u32 	%r30, %tid.x;
	mov.u32 	%r31, %ctaid.x;
	or.b32 	%r32, %r30, %r31;
	setp.ne.s32 	%p1, %r32, 0;
	@%p1 bra 	$L__BB3_15;
	setp.eq.s32 	%p2, %r29, 0;
	mov.b32 	%r106, 0;
	@%p2 bra 	$L__BB3_14;
	and.b32 	%r1, %r29, 15;
	setp.lt.u32 	%p3, %r29, 16;
	mov.b32 	%r99, 0;
	mov.b32 	%r106, %r99;
	@%p3 bra 	$L__BB3_5;
	and.b32 	%r99, %r29, -16;
	neg.s32 	%r93, %r99;
	add.s64 	%rd29, %rd2, 32;
	add.s64 	%rd28, %rd1, 32;
	mov.b32 	%r106, 0;
$L__BB3_4:
	.pragma "nounroll";
	st.global.b32 	[%rd29+-32], %r106;
	ld.global.b32 	%r37, [%rd28+-32];
	add.s32 	%r38, %r37, %r106;
	st.global.b32 	[%rd29+-28], %r38;
	ld.global.b32 	%r39, [%rd28+-28];
	add.s32 	%r40, %r39, %r38;
	st.global.b32 	[%rd29+-24], %r40;
	ld.global.b32 	%r41, [%rd28+-24];
	add.s32 	%r42, %r41, %r40;
	st.global.b32 	[%rd29+-20], %r42;
	ld.global.b32 	%r43, [%rd28+-20];
	add.s32 	%r44, %r43, %r42;
	st.global.b32 	[%rd29+-16], %r44;
	ld.global.b32 	%r45, [%rd28+-16];
	add.s32 	%r46, %r45, %r44;
	st.global.b32 	[%rd29+-12], %r46;
	ld.global.b32 	%r47, [%rd28+-12];
	add.s32 	%r48, %r47, %r46;
	st.global.b32 	[%rd29+-8], %r48;
	ld.global.b32 	%r49, [%rd28+-8];
	add.s32 	%r50, %r49, %r48;
	st.global.b32 	[%rd29+-4], %r50;
	ld.global.b32 	%r51, [%rd28+-4];
	add.s32 	%r52, %r51, %r50;
	st.global.b32 	[%rd29], %r52;
	ld.global.b32 	%r53, [%rd28];
	add.s32 	%r54, %r53, %r52;
	st.global.b32 	[%rd29+4], %r54;
	ld.global.b32 	%r55, [%rd28+4];
	add.s32 	%r56, %r55, %r54;
	st.global.b32 	[%rd29+8], %r56;
	ld.global.b32 	%r57, [%rd28+8];
	add.s32 	%r58, %r57, %r56;
	st.global.b32 	[%rd29+12], %r58;
	ld.global.b32 	%r59, [%rd28+12];
	add.s32 	%r60, %r59, %r58;
	st.global.b32 	[%rd29+16], %r60;
	ld.global.b32 	%r61, [%rd28+16];
	add.s32 	%r62, %r61, %r60;
	st.global.b32 	[%rd29+20], %r62;
	ld.global.b32 	%r63, [%rd28+20];
	add.s32 	%r64, %r63, %r62;
	st.global.b32 	[%rd29+24], %r64;
	ld.global.b32 	%r65, [%rd28+24];
	add.s32 	%r66, %r65, %r64;
	st.global.b32 	[%rd29+28], %r66;
	ld.global.b32 	%r67, [%rd28+28];
	add.s32 	%r106, %r67, %r66;
	add.s32 	%r93, %r93, 16;
	add.s64 	%rd29, %rd29, 64;
	add.s64 	%rd28, %rd28, 64;
	setp.ne.s32 	%p4, %r93, 0;
	@%p4 bra 	$L__BB3_4;
$L__BB3_5:
	setp.eq.s32 	%p5, %r1, 0;
	@%p5 bra 	$L__BB3_14;
	and.b32 	%r11, %r29, 7;
	setp.lt.u32 	%p6, %r1, 8;
	@%p6 bra 	$L__BB3_8;
	mul.wide.u32 	%rd18, %r99, 4;
	add.s64 	%rd19, %rd2, %rd18;
	st.global.b32 	[%rd19], %r106;
	add.s64 	%rd20, %rd1, %rd18;
	ld.global.b32 	%r69, [%rd20];
	add.s32 	%r70, %r69, %r106;
	st.global.b32 	[%rd19+4], %r70;
	ld.global.b32 	%r71, [%rd20+4];
	add.s32 	%r72, %r71, %r70;
	st.global.b32 	[%rd19+8], %r72;
	ld.global.b32 	%r73, [%rd20+8];
	add.s32 	%r74, %r73, %r72;
	st.global.b32 	[%rd19+12], %r74;
	ld.global.b32 	%r75, [%rd20+12];
	add.s32 	%r76, %r75, %r74;
	st.global.b32 	[%rd19+16], %r76;
	ld.global.b32 	%r77, [%rd20+16];
	add.s32 	%r78, %r77, %r76;
	st.global.b32 	[%rd19+20], %r78;
	ld.global.b32 	%r79, [%rd20+20];
	add.s32 	%r80, %r79, %r78;
	st.global.b32 	[%rd19+24], %r80;
	ld.global.b32 	%r81, [%rd20+24];
	add.s32 	%r82, %r81, %r80;
	st.global.b32 	[%rd19+28], %r82;
	ld.global.b32 	%r83, [%rd20+28];
	add.s32 	%r106, %r83, %r82;
	add.s32 	%r99, %r99, 8;
$L__BB3_8:
	setp.eq.s32 	%p7, %r11, 0;
	@%p7 bra 	$L__BB3_14;
	and.b32 	%r17, %r29, 3;
	setp.lt.u32 	%p8, %r11, 4;
	@%p8 bra 	$L__BB3_11;
	mul.wide.u32 	%rd21, %r99, 4;
	add.s64 	%rd22, %rd2, %rd21;
	st.global.b32 	[%rd22], %r106;
	add.s64 	%rd23, %rd1, %rd21;
	ld.global.b32 	%r85, [%rd23];
	add.s32 	%r86, %r85, %r106;
	st.global.b32 	[%rd22+4], %r86;
	ld.global.b32 	%r87, [%rd23+4];
	add.s32 	%r88, %r87, %r86;
	st.global.b32 	[%rd22+8], %r88;
	ld.global.b32 	%r89, [%rd23+8];
	add.s32 	%r90, %r89, %r88;
	st.global.b32 	[%rd22+12], %r90;
	ld.global.b32 	%r91, [%rd23+12];
	add.s32 	%r106, %r91, %r90;
	add.s32 	%r99, %r99, 4;
$L__BB3_11:
	setp.eq.s32 	%p9, %r17, 0;
	@%p9 bra 	$L__BB3_14;
	mul.wide.u32 	%rd24, %r99, 4;
	add.s64 	%rd31, %rd1, %rd24;
	add.s64 	%rd30, %rd2, %rd24;
	neg.s32 	%r104, %r17;
$L__BB3_13:
	.pragma "nounroll";
	st.global.b32 	[%rd30], %r106;
	ld.global.b32 	%r92, [%rd31];
	add.s32 	%r106, %r92, %r106;
	add.s64 	%rd31, %rd31, 4;
	add.s64 	%rd30, %rd30, 4;
	add.s32 	%r104, %r104, 1;
	setp.ne.s32 	%p10, %r104, 0;
	@%p10 bra 	$L__BB3_13;
$L__BB3_14:
	mul.wide.u32 	%rd25, %r29, 4;
	add.s64 	%rd26, %rd2, %rd25;
	st.global.b32 	[%rd26], %r106;
	cvta.to.global.u64 	%rd27, %rd15;
	st.global.b32 	[%rd27], %r106;
$L__BB3_15:
	ret;

}
	// .globl	__raygen__build_neighbors
.visible .entry __raygen__build_neighbors()
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<125>;
	.reg .b64 	%rd<10>;

	// begin inline asm
	call (%r4), _optix_get_launch_index_x, ();
	// end inline asm
	// begin inline asm
	call (%r5), _optix_get_launch_index_y, ();
	// end inline asm
	ld.const.b32 	%r6, [params+16];
	setp.ge.u32 	%p1, %r4, %r6;
	ld.const.b32 	%r7, [params+28];
	setp.ge.u32 	%p2, %r5, %r7;
	or.pred 	%p3, %p1, %p2;
	@%p3 bra 	$L__BB4_2;
	ld.const.b64 	%rd2, [params+8];
	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.u32 	%rd4, %r4, 12;
	add.s64 	%rd5, %rd3, %rd4;
	cvt.rn.f32.u32 	%r88, %r5;
	mul.ftz.f32 	%r89, %r88, 0f40C90FDB;
	mov.b32 	%r90, 0f3FCF1BBD;
	div.approx.ftz.f32 	%r91, %r89, %r90;
	add.ftz.f32 	%r92, %r88, 0f3F000000;
	add.ftz.f32 	%r93, %r92, %r92;
	cvt.rn.f32.u32 	%r94, %r7;
	div.approx.ftz.f32 	%r95, %r93, %r94;
	mov.b32 	%r96, 0f3F800000;
	sub.ftz.f32 	%r97, %r96, %r95;
	abs.ftz.f32 	%r98, %r97;
	fma.rn.ftz.f32 	%r99, %r98, 0fBF000000, 0f3F000000;
	rsqrt.approx.ftz.f32 	%r100, %r99;
	mul.ftz.f32 	%r101, %r100, %r99;
	mul.ftz.f32 	%r102, %r100, 0fBF000000;
	fma.rn.ftz.f32 	%r103, %r101, %r102, 0f3F000000;
	fma.rn.ftz.f32 	%r104, %r101, %r103, %r101;
	setp.eq.ftz.f32 	%p4, %r98, 0f3F800000;
	selp.f32 	%r105, 0f00000000, %r104, %p4;
	setp.gt.ftz.f32 	%p5, %r98, 0f3F0F5C29;
	selp.f32 	%r106, %r105, %r98, %p5;
	copysign.f32 	%r107, %r97, %r106;
	mul.ftz.f32 	%r108, %r107, %r107;
	fma.rn.ftz.f32 	%r109, %r108, 0f3D10ECEF, 0f3C8B1ABB;
	fma.rn.ftz.f32 	%r110, %r109, %r108, 0f3CFC028C;
	fma.rn.ftz.f32 	%r111, %r110, %r108, 0f3D372139;
	fma.rn.ftz.f32 	%r112, %r111, %r108, 0f3D9993DB;
	fma.rn.ftz.f32 	%r113, %r112, %r108, 0f3E2AAAC6;
	mul.ftz.f32 	%r114, %r108, %r113;
	fma.rn.ftz.f32 	%r115, %r114, %r107, %r107;
	neg.ftz.f32 	%r116, %r115;
	selp.f32 	%r117, %r115, %r116, %p5;
	fma.rn.ftz.f32 	%r118, 0f3F6EE581, 0f3FD774EB, %r117;
	setp.gt.ftz.f32 	%p6, %r97, 0f3F0F5C29;
	selp.f32 	%r119, %r115, %r118, %p6;
	add.ftz.f32 	%r120, %r119, %r119;
	selp.f32 	%r121, %r120, %r119, %p5;
	sin.approx.ftz.f32 	%r122, %r121;
	cos.approx.ftz.f32 	%r123, %r91;
	mul.ftz.f32 	%r44, %r122, %r123;
	sin.approx.ftz.f32 	%r124, %r91;
	mul.ftz.f32 	%r45, %r122, %r124;
	cos.approx.ftz.f32 	%r46, %r121;
	ld.const.b64 	%rd6, [params+40];
	cvta.to.global.u64 	%rd7, %rd6;
	mul.wide.u32 	%rd8, %r4, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.b32 	%r57, [%rd9];
	ld.const.b64 	%rd1, [params];
	ld.global.b32 	%r43, [%rd5+8];
	ld.global.b32 	%r42, [%rd5+4];
	ld.global.b32 	%r41, [%rd5];
	ld.const.b32 	%r48, [params+20];
	mov.b32 	%r47, 981668463;
	mov.b32 	%r50, 255;
	mov.b32 	%r53, 1;
	mov.b32 	%r55, 2;
	mov.b32 	%r87, 0;
	// begin inline asm
	call(%r8,%r9,%r10,%r11,%r12,%r13,%r14,%r15,%r16,%r17,%r18,%r19,%r20,%r21,%r22,%r23,%r24,%r25,%r26,%r27,%r28,%r29,%r30,%r31,%r32,%r33,%r34,%r35,%r36,%r37,%r38,%r39),_optix_trace_typed_32,(%r87,%rd1,%r41,%r42,%r43,%r44,%r45,%r46,%r47,%r48,%r87,%r50,%r87,%r87,%r53,%r87,%r55,%r4,%r57,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87,%r87);
	// end inline asm
$L__BB4_2:
	ret;

}
	// .globl	__closesthit__build_neighbors
.visible .entry __closesthit__build_neighbors()
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<12>;

	// begin inline asm
	call (%r4), _optix_read_primitive_idx, ();
	// end inline asm
	mov.b32 	%r6, 0;
	// begin inline asm
	call (%r5), _optix_get_payload, (%r6);
	// end inline asm
	mov.b32 	%r8, 1;
	// begin inline asm
	call (%r7), _optix_get_payload, (%r8);
	// end inline asm
	setp.le.u32 	%p1, %r4, %r5;
	@%p1 bra 	$L__BB5_2;
	ld.const.b64 	%rd1, [params+32];
	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.u32 	%rd3, %r5, 4;
	add.s64 	%rd4, %rd2, %rd3;
	atom.global.add.u32 	%r9, [%rd4], 1;
	ld.const.b64 	%rd5, [params+40];
	cvta.to.global.u64 	%rd6, %rd5;
	add.s64 	%rd7, %rd6, %rd3;
	ld.global.b32 	%r10, [%rd7];
	add.s32 	%r11, %r10, %r9;
	ld.const.b64 	%rd8, [params+48];
	cvta.to.global.u64 	%rd9, %rd8;
	mul.wide.u32 	%rd10, %r11, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.b32 	[%rd11], %r4;
$L__BB5_2:
	add.s32 	%r13, %r7, 1;
	mov.b32 	%r12, 1;
	// begin inline asm
	call _optix_set_payload, (%r12, %r13);
	// end inline asm
	ret;

}
	// .globl	__miss__build_neighbors
.visible .entry __miss__build_neighbors()
{


	ret;

}
	// .globl	init_union_find
.visible .entry init_union_find(
	.param .u64 .ptr .align 1 init_union_find_param_0,
	.param .u32 init_union_find_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;

	ld.param.b64 	%rd1, [init_union_find_param_0];
	ld.param.b32 	%r2, [init_union_find_param_1];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.u32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB7_2;
	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.u32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	st.global.b32 	[%rd4], %r1;
$L__BB7_2:
	ret;

}
	// .globl	union_neighbors
.visible .entry union_neighbors(
	.param .u64 .ptr .align 1 union_neighbors_param_0,
	.param .u64 .ptr .align 1 union_neighbors_param_1,
	.param .u64 .ptr .align 1 union_neighbors_param_2,
	.param .u32 union_neighbors_param_3,
	.param .u64 .ptr .align 1 union_neighbors_param_4
)
{
	.reg .pred 	%p<23>;
	.reg .b32 	%r<51>;
	.reg .b64 	%rd<53>;

	ld.param.b64 	%rd20, [union_neighbors_param_0];
	ld.param.b64 	%rd21, [union_neighbors_param_1];
	ld.param.b64 	%rd23, [union_neighbors_param_2];
	ld.param.b32 	%r31, [union_neighbors_param_3];
	ld.param.b64 	%rd22, [union_neighbors_param_4];
	cvta.to.global.u64 	%rd1, %rd23;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %ntid.x;
	mov.u32 	%r34, %tid.x;
	mad.lo.s32 	%r1, %r32, %r33, %r34;
	setp.ge.u32 	%p1, %r1, %r31;
	@%p1 bra 	$L__BB8_20;
	cvta.to.global.u64 	%rd24, %rd20;
	mul.wide.u32 	%rd25, %r1, 4;
	add.s64 	%rd26, %rd24, %rd25;
	ld.global.b32 	%r41, [%rd26];
	ld.global.b32 	%r3, [%rd26+4];
	cvt.s64.s32 	%rd2, %r1;
	mul.wide.s32 	%rd27, %r1, 4;
	add.s64 	%rd3, %rd1, %rd27;
	ld.global.b32 	%r40, [%rd3];
	setp.eq.s32 	%p2, %r40, %r1;
	mov.b32 	%r38, %r1;
	mov.b32 	%r39, %r40;
	@%p2 bra 	$L__BB8_2;
	bra.uni 	$L__BB8_21;
$L__BB8_2:
	setp.eq.s32 	%p4, %r40, %r38;
	mov.b64 	%rd47, %rd3;
	@%p4 bra 	$L__BB8_3;
	bra.uni 	$L__BB8_22;
$L__BB8_3:
	setp.ge.u32 	%p6, %r41, %r3;
	@%p6 bra 	$L__BB8_20;
	cvta.to.global.u64 	%rd4, %rd21;
	cvta.to.global.u64 	%rd5, %rd22;
$L__BB8_5:
	mul.wide.u32 	%rd31, %r41, 4;
	add.s64 	%rd32, %rd4, %rd31;
	ld.global.b32 	%r25, [%rd32];
	cvt.s64.s32 	%rd51, %r25;
	mul.wide.s32 	%rd33, %r25, 4;
	add.s64 	%rd52, %rd1, %rd33;
	ld.global.b32 	%r44, [%rd52];
	setp.eq.s32 	%p7, %r44, %r25;
	mov.b32 	%r42, %r25;
	mov.b32 	%r43, %r44;
	@%p7 bra 	$L__BB8_6;
	bra.uni 	$L__BB8_23;
$L__BB8_6:
	setp.eq.s32 	%p9, %r44, %r42;
	mov.b64 	%rd48, %rd52;
	@%p9 bra 	$L__BB8_8;
$L__BB8_7:
	st.global.b32 	[%rd48], %r42;
	mul.wide.s32 	%rd36, %r44, 4;
	add.s64 	%rd48, %rd1, %rd36;
	ld.global.b32 	%r44, [%rd48];
	setp.ne.s32 	%p10, %r44, %r42;
	@%p10 bra 	$L__BB8_7;
$L__BB8_8:
	setp.eq.s32 	%p11, %r38, %r42;
	@%p11 bra 	$L__BB8_19;
	ld.global.b32 	%r47, [%rd3];
	setp.eq.s32 	%p12, %r47, %r1;
	mov.b32 	%r45, %r1;
	mov.b64 	%rd12, %rd2;
	mov.b32 	%r46, %r47;
	@%p12 bra 	$L__BB8_10;
	bra.uni 	$L__BB8_24;
$L__BB8_10:
	setp.eq.s32 	%p14, %r47, %r45;
	mov.b64 	%rd50, %rd3;
	@%p14 bra 	$L__BB8_11;
	bra.uni 	$L__BB8_25;
$L__BB8_11:
	ld.global.b32 	%r50, [%rd52];
	setp.eq.s32 	%p16, %r50, %r25;
	mov.b32 	%r49, %r50;
	@%p16 bra 	$L__BB8_12;
	bra.uni 	$L__BB8_26;
$L__BB8_12:
	setp.eq.s32 	%p18, %r50, %r25;
	@%p18 bra 	$L__BB8_14;
$L__BB8_13:
	st.global.b32 	[%rd52], %r25;
	mul.wide.s32 	%rd42, %r50, 4;
	add.s64 	%rd52, %rd1, %rd42;
	ld.global.b32 	%r50, [%rd52];
	setp.ne.s32 	%p19, %r50, %r25;
	@%p19 bra 	$L__BB8_13;
$L__BB8_14:
	setp.eq.s32 	%p20, %r45, %r25;
	@%p20 bra 	$L__BB8_18;
	setp.ge.s32 	%p21, %r45, %r25;
	@%p21 bra 	$L__BB8_17;
	shl.b64 	%rd45, %rd51, 2;
	add.s64 	%rd46, %rd1, %rd45;
	atom.global.min.s32 	%r36, [%rd46], %r45;
	bra.uni 	$L__BB8_18;
$L__BB8_17:
	shl.b64 	%rd43, %rd12, 2;
	add.s64 	%rd44, %rd1, %rd43;
	atom.global.min.s32 	%r35, [%rd44], %r25;
$L__BB8_18:
	atom.global.or.b32 	%r37, [%rd5], 1;
$L__BB8_19:
	add.s32 	%r41, %r41, 1;
	setp.ne.s32 	%p22, %r41, %r3;
	@%p22 bra 	$L__BB8_5;
$L__BB8_20:
	ret;
$L__BB8_21:
	mul.wide.s32 	%rd28, %r39, 4;
	add.s64 	%rd29, %rd1, %rd28;
	ld.global.b32 	%r7, [%rd29];
	setp.eq.s32 	%p3, %r7, %r39;
	mov.b32 	%r38, %r39;
	mov.b32 	%r39, %r7;
	@%p3 bra 	$L__BB8_2;
	bra.uni 	$L__BB8_21;
$L__BB8_22:
	st.global.b32 	[%rd47], %r38;
	mul.wide.s32 	%rd30, %r40, 4;
	add.s64 	%rd47, %rd1, %rd30;
	ld.global.b32 	%r40, [%rd47];
	setp.eq.s32 	%p5, %r40, %r38;
	@%p5 bra 	$L__BB8_3;
	bra.uni 	$L__BB8_22;
$L__BB8_23:
	mul.wide.s32 	%rd34, %r43, 4;
	add.s64 	%rd35, %rd1, %rd34;
	ld.global.b32 	%r15, [%rd35];
	setp.eq.s32 	%p8, %r15, %r43;
	mov.b32 	%r42, %r43;
	mov.b32 	%r43, %r15;
	@%p8 bra 	$L__BB8_6;
	bra.uni 	$L__BB8_23;
$L__BB8_24:
	cvt.s64.s32 	%rd12, %r46;
	mul.wide.s32 	%rd37, %r46, 4;
	add.s64 	%rd38, %rd1, %rd37;
	ld.global.b32 	%r21, [%rd38];
	setp.eq.s32 	%p13, %r21, %r46;
	mov.b32 	%r45, %r46;
	mov.b32 	%r46, %r21;
	@%p13 bra 	$L__BB8_10;
	bra.uni 	$L__BB8_24;
$L__BB8_25:
	st.global.b32 	[%rd50], %r45;
	mul.wide.s32 	%rd39, %r47, 4;
	add.s64 	%rd50, %rd1, %rd39;
	ld.global.b32 	%r47, [%rd50];
	setp.eq.s32 	%p15, %r47, %r45;
	@%p15 bra 	$L__BB8_11;
	bra.uni 	$L__BB8_25;
$L__BB8_26:
	cvt.s64.s32 	%rd51, %r49;
	mul.wide.s32 	%rd40, %r49, 4;
	add.s64 	%rd41, %rd1, %rd40;
	ld.global.b32 	%r27, [%rd41];
	setp.eq.s32 	%p17, %r27, %r49;
	mov.b32 	%r25, %r49;
	mov.b32 	%r49, %r27;
	@%p17 bra 	$L__BB8_12;
	bra.uni 	$L__BB8_26;

}
	// .globl	flatten_clusters
.visible .entry flatten_clusters(
	.param .u64 .ptr .align 1 flatten_clusters_param_0,
	.param .u64 .ptr .align 1 flatten_clusters_param_1,
	.param .u64 .ptr .align 1 flatten_clusters_param_2,
	.param .u32 flatten_clusters_param_3,
	.param .u32 flatten_clusters_param_4,
	.param .u64 .ptr .align 1 flatten_clusters_param_5,
	.param .u64 .ptr .align 1 flatten_clusters_param_6
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<27>;

	ld.param.b64 	%rd9, [flatten_clusters_param_0];
	ld.param.b64 	%rd10, [flatten_clusters_param_1];
	ld.param.b64 	%rd6, [flatten_clusters_param_2];
	ld.param.b32 	%r9, [flatten_clusters_param_3];
	ld.param.b32 	%r8, [flatten_clusters_param_4];
	ld.param.b64 	%rd7, [flatten_clusters_param_5];
	ld.param.b64 	%rd8, [flatten_clusters_param_6];
	cvta.to.global.u64 	%rd1, %rd10;
	cvta.to.global.u64 	%rd2, %rd9;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r11, %ntid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r1, %r10, %r11, %r12;
	setp.ge.u32 	%p1, %r1, %r9;
	@%p1 bra 	$L__BB9_8;
	mul.wide.s32 	%rd11, %r1, 4;
	add.s64 	%rd26, %rd2, %rd11;
	ld.global.b32 	%r17, [%rd26];
	setp.eq.s32 	%p2, %r17, %r1;
	mov.b32 	%r3, %r1;
	mov.b32 	%r16, %r17;
	@%p2 bra 	$L__BB9_2;
	bra.uni 	$L__BB9_9;
$L__BB9_2:
	setp.eq.s32 	%p4, %r17, %r3;
	@%p4 bra 	$L__BB9_4;
$L__BB9_3:
	st.global.b32 	[%rd26], %r3;
	mul.wide.s32 	%rd14, %r17, 4;
	add.s64 	%rd26, %rd2, %rd14;
	ld.global.b32 	%r17, [%rd26];
	setp.ne.s32 	%p5, %r17, %r3;
	@%p5 bra 	$L__BB9_3;
$L__BB9_4:
	mul.wide.u32 	%rd15, %r1, 4;
	add.s64 	%rd16, %rd2, %rd15;
	st.global.b32 	[%rd16], %r3;
	cvta.to.global.u64 	%rd17, %rd6;
	add.s64 	%rd18, %rd17, %rd15;
	ld.global.b32 	%r13, [%rd18];
	setp.ge.u32 	%p6, %r13, %r8;
	@%p6 bra 	$L__BB9_6;
	add.s64 	%rd25, %rd1, %rd15;
	st.global.b32 	[%rd25], -1;
	bra.uni 	$L__BB9_8;
$L__BB9_6:
	setp.ne.s32 	%p7, %r1, %r3;
	@%p7 bra 	$L__BB9_8;
	cvta.to.global.u64 	%rd19, %rd8;
	atom.global.add.u32 	%r14, [%rd19], 1;
	cvta.to.global.u64 	%rd20, %rd7;
	add.s64 	%rd22, %rd20, %rd15;
	st.global.b32 	[%rd22], %r14;
	add.s64 	%rd23, %rd1, %rd15;
	st.global.b32 	[%rd23], %r14;
$L__BB9_8:
	ret;
$L__BB9_9:
	mul.wide.s32 	%rd12, %r16, 4;
	add.s64 	%rd13, %rd2, %rd12;
	ld.global.b32 	%r5, [%rd13];
	setp.eq.s32 	%p3, %r5, %r16;
	mov.b32 	%r3, %r16;
	mov.b32 	%r16, %r5;
	@%p3 bra 	$L__BB9_2;
	bra.uni 	$L__BB9_9;

}
	// .globl	propagate_cluster_ids
.visible .entry propagate_cluster_ids(
	.param .u64 .ptr .align 1 propagate_cluster_ids_param_0,
	.param .u64 .ptr .align 1 propagate_cluster_ids_param_1,
	.param .u64 .ptr .align 1 propagate_cluster_ids_param_2,
	.param .u32 propagate_cluster_ids_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<13>;

	ld.param.b64 	%rd2, [propagate_cluster_ids_param_0];
	ld.param.b64 	%rd3, [propagate_cluster_ids_param_1];
	ld.param.b64 	%rd4, [propagate_cluster_ids_param_2];
	ld.param.b32 	%r2, [propagate_cluster_ids_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.u32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB10_3;
	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.u32 	%rd6, %r1, 4;
	add.s64 	%rd1, %rd5, %rd6;
	ld.global.b32 	%r6, [%rd1];
	setp.eq.s32 	%p2, %r6, -1;
	@%p2 bra 	$L__BB10_3;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd9, %rd7, %rd6;
	ld.global.b32 	%r7, [%rd9];
	cvta.to.global.u64 	%rd10, %rd4;
	mul.wide.s32 	%rd11, %r7, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.b32 	%r8, [%rd12];
	st.global.b32 	[%rd1], %r8;
$L__BB10_3:
	ret;

}
	// .globl	count_cluster_sizes
.visible .entry count_cluster_sizes(
	.param .u64 .ptr .align 1 count_cluster_sizes_param_0,
	.param .u64 .ptr .align 1 count_cluster_sizes_param_1,
	.param .u32 count_cluster_sizes_param_2,
	.param .u32 count_cluster_sizes_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd1, [count_cluster_sizes_param_0];
	ld.param.b64 	%rd2, [count_cluster_sizes_param_1];
	ld.param.b32 	%r4, [count_cluster_sizes_param_2];
	ld.param.b32 	%r3, [count_cluster_sizes_param_3];
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	setp.ge.u32 	%p1, %r1, %r4;
	@%p1 bra 	$L__BB11_3;
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.b32 	%r2, [%rd5];
	setp.lt.s32 	%p2, %r2, 0;
	setp.ge.u32 	%p3, %r2, %r3;
	or.pred 	%p4, %p2, %p3;
	@%p4 bra 	$L__BB11_3;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd6, %rd7;
	atom.global.add.u32 	%r8, [%rd8], 1;
$L__BB11_3:
	ret;

}
	// .globl	filter_small_clusters
.visible .entry filter_small_clusters(
	.param .u64 .ptr .align 1 filter_small_clusters_param_0,
	.param .u64 .ptr .align 1 filter_small_clusters_param_1,
	.param .u32 filter_small_clusters_param_2,
	.param .u32 filter_small_clusters_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd2, [filter_small_clusters_param_0];
	ld.param.b64 	%rd3, [filter_small_clusters_param_1];
	ld.param.b32 	%r4, [filter_small_clusters_param_2];
	ld.param.b32 	%r3, [filter_small_clusters_param_3];
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	setp.ge.u32 	%p1, %r1, %r4;
	@%p1 bra 	$L__BB12_4;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.u32 	%rd5, %r1, 4;
	add.s64 	%rd1, %rd4, %rd5;
	ld.global.b32 	%r2, [%rd1];
	setp.lt.s32 	%p2, %r2, 0;
	@%p2 bra 	$L__BB12_4;
	cvta.to.global.u64 	%rd6, %rd3;
	mul.wide.u32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.b32 	%r8, [%rd8];
	setp.ge.u32 	%p3, %r8, %r3;
	@%p3 bra 	$L__BB12_4;
	st.global.b32 	[%rd1], -1;
$L__BB12_4:
	ret;

}
