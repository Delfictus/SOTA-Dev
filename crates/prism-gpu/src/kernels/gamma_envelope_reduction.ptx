//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-37061995
// Cuda compilation tools, release 13.1, V13.1.115
// Based on NVVM 21.0.0
//

.version 9.1
.target sm_120
.address_size 64

	// .globl	compute_gamma_envelopes_batch

.visible .entry compute_gamma_envelopes_batch(
	.param .u64 .ptr .align 1 compute_gamma_envelopes_batch_param_0,
	.param .u64 .ptr .align 1 compute_gamma_envelopes_batch_param_1,
	.param .u64 .ptr .align 1 compute_gamma_envelopes_batch_param_2,
	.param .u64 .ptr .align 1 compute_gamma_envelopes_batch_param_3,
	.param .u64 .ptr .align 1 compute_gamma_envelopes_batch_param_4,
	.param .f64 compute_gamma_envelopes_batch_param_5,
	.param .u32 compute_gamma_envelopes_batch_param_6
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<180>;

	ld.param.b64 	%rd69, [compute_gamma_envelopes_batch_param_1];
	ld.param.b64 	%rd71, [compute_gamma_envelopes_batch_param_2];
	ld.param.b64 	%rd72, [compute_gamma_envelopes_batch_param_3];
	ld.param.b64 	%rd73, [compute_gamma_envelopes_batch_param_4];
	ld.param.b64 	%rd70, [compute_gamma_envelopes_batch_param_5];
	ld.param.b32 	%r21, [compute_gamma_envelopes_batch_param_6];
	cvta.to.global.u64 	%rd1, %rd73;
	cvta.to.global.u64 	%rd2, %rd72;
	cvta.to.global.u64 	%rd3, %rd71;
	mov.u32 	%r22, %ctaid.x;
	mov.u32 	%r23, %ntid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r1, %r22, %r23, %r24;
	setp.ge.s32 	%p1, %r1, %r21;
	@%p1 bra 	$L__BB0_23;
	mul.lo.s32 	%r26, %r1, 75;
	mul.wide.s32 	%rd152, %r26, 8;
	mov.b64 	%rd168, 0d7FEFFFFFFFFFFFFF;
	mov.b64 	%rd169, 0dFFEFFFFFFFFFFFFF;
	mov.b64 	%rd170, 0d0000000000000000;
	mov.b32 	%r27, 0;
	mov.b32 	%r33, %r27;
$L__BB0_2:
	.pragma "nounroll";
	ld.param.b64 	%rd151, [compute_gamma_envelopes_batch_param_0];
	add.s64 	%rd78, %rd69, %rd152;
	// begin inline asm
	ld.global.nc.f64 %rd77, [%rd78];
	// end inline asm
	setp.lt.f64 	%p2, %rd77, 0d3E112E0BE826D695;
	add.s64 	%rd11, %rd151, %rd152;
	@%p2 bra 	$L__BB0_4;
	// begin inline asm
	ld.global.nc.f64 %rd79, [%rd11];
	// end inline asm
	sub.f64 	%rd81, %rd70, %rd79;
	max.f64 	%rd82, %rd81, 0d0000000000000000;
	div.rn.f64 	%rd83, %rd82, %rd77;
	add.f64 	%rd84, %rd83, 0dBFF0000000000000;
	min.f64 	%rd168, %rd168, %rd84;
	max.f64 	%rd169, %rd169, %rd84;
	add.f64 	%rd170, %rd170, %rd84;
	add.s32 	%r33, %r33, 1;
$L__BB0_4:
	add.s64 	%rd86, %rd78, 8;
	// begin inline asm
	ld.global.nc.f64 %rd85, [%rd86];
	// end inline asm
	setp.lt.f64 	%p3, %rd85, 0d3E112E0BE826D695;
	@%p3 bra 	$L__BB0_6;
	add.s64 	%rd88, %rd11, 8;
	// begin inline asm
	ld.global.nc.f64 %rd87, [%rd88];
	// end inline asm
	sub.f64 	%rd89, %rd70, %rd87;
	max.f64 	%rd90, %rd89, 0d0000000000000000;
	div.rn.f64 	%rd91, %rd90, %rd85;
	add.f64 	%rd92, %rd91, 0dBFF0000000000000;
	min.f64 	%rd168, %rd168, %rd92;
	max.f64 	%rd169, %rd169, %rd92;
	add.f64 	%rd170, %rd170, %rd92;
	add.s32 	%r33, %r33, 1;
$L__BB0_6:
	add.s64 	%rd94, %rd78, 16;
	// begin inline asm
	ld.global.nc.f64 %rd93, [%rd94];
	// end inline asm
	setp.lt.f64 	%p4, %rd93, 0d3E112E0BE826D695;
	@%p4 bra 	$L__BB0_8;
	add.s64 	%rd96, %rd11, 16;
	// begin inline asm
	ld.global.nc.f64 %rd95, [%rd96];
	// end inline asm
	sub.f64 	%rd97, %rd70, %rd95;
	max.f64 	%rd98, %rd97, 0d0000000000000000;
	div.rn.f64 	%rd99, %rd98, %rd93;
	add.f64 	%rd100, %rd99, 0dBFF0000000000000;
	min.f64 	%rd168, %rd168, %rd100;
	max.f64 	%rd169, %rd169, %rd100;
	add.f64 	%rd170, %rd170, %rd100;
	add.s32 	%r33, %r33, 1;
$L__BB0_8:
	setp.eq.s32 	%p5, %r27, 72;
	@%p5 bra 	$L__BB0_20;
	add.s64 	%rd102, %rd78, 24;
	// begin inline asm
	ld.global.nc.f64 %rd101, [%rd102];
	// end inline asm
	setp.lt.f64 	%p6, %rd101, 0d3E112E0BE826D695;
	@%p6 bra 	$L__BB0_11;
	add.s64 	%rd104, %rd11, 24;
	// begin inline asm
	ld.global.nc.f64 %rd103, [%rd104];
	// end inline asm
	sub.f64 	%rd105, %rd70, %rd103;
	max.f64 	%rd106, %rd105, 0d0000000000000000;
	div.rn.f64 	%rd107, %rd106, %rd101;
	add.f64 	%rd108, %rd107, 0dBFF0000000000000;
	min.f64 	%rd168, %rd168, %rd108;
	max.f64 	%rd169, %rd169, %rd108;
	add.f64 	%rd170, %rd170, %rd108;
	add.s32 	%r33, %r33, 1;
$L__BB0_11:
	add.s64 	%rd110, %rd78, 32;
	// begin inline asm
	ld.global.nc.f64 %rd109, [%rd110];
	// end inline asm
	setp.lt.f64 	%p7, %rd109, 0d3E112E0BE826D695;
	@%p7 bra 	$L__BB0_13;
	add.s64 	%rd112, %rd11, 32;
	// begin inline asm
	ld.global.nc.f64 %rd111, [%rd112];
	// end inline asm
	sub.f64 	%rd113, %rd70, %rd111;
	max.f64 	%rd114, %rd113, 0d0000000000000000;
	div.rn.f64 	%rd115, %rd114, %rd109;
	add.f64 	%rd116, %rd115, 0dBFF0000000000000;
	min.f64 	%rd168, %rd168, %rd116;
	max.f64 	%rd169, %rd169, %rd116;
	add.f64 	%rd170, %rd170, %rd116;
	add.s32 	%r33, %r33, 1;
$L__BB0_13:
	add.s64 	%rd118, %rd78, 40;
	// begin inline asm
	ld.global.nc.f64 %rd117, [%rd118];
	// end inline asm
	setp.lt.f64 	%p8, %rd117, 0d3E112E0BE826D695;
	@%p8 bra 	$L__BB0_15;
	add.s64 	%rd120, %rd11, 40;
	// begin inline asm
	ld.global.nc.f64 %rd119, [%rd120];
	// end inline asm
	sub.f64 	%rd121, %rd70, %rd119;
	max.f64 	%rd122, %rd121, 0d0000000000000000;
	div.rn.f64 	%rd123, %rd122, %rd117;
	add.f64 	%rd124, %rd123, 0dBFF0000000000000;
	min.f64 	%rd168, %rd168, %rd124;
	max.f64 	%rd169, %rd169, %rd124;
	add.f64 	%rd170, %rd170, %rd124;
	add.s32 	%r33, %r33, 1;
$L__BB0_15:
	add.s64 	%rd126, %rd78, 48;
	// begin inline asm
	ld.global.nc.f64 %rd125, [%rd126];
	// end inline asm
	setp.lt.f64 	%p9, %rd125, 0d3E112E0BE826D695;
	@%p9 bra 	$L__BB0_17;
	add.s64 	%rd128, %rd11, 48;
	// begin inline asm
	ld.global.nc.f64 %rd127, [%rd128];
	// end inline asm
	sub.f64 	%rd129, %rd70, %rd127;
	max.f64 	%rd130, %rd129, 0d0000000000000000;
	div.rn.f64 	%rd131, %rd130, %rd125;
	add.f64 	%rd132, %rd131, 0dBFF0000000000000;
	min.f64 	%rd168, %rd168, %rd132;
	max.f64 	%rd169, %rd169, %rd132;
	add.f64 	%rd170, %rd170, %rd132;
	add.s32 	%r33, %r33, 1;
$L__BB0_17:
	add.s64 	%rd134, %rd78, 56;
	// begin inline asm
	ld.global.nc.f64 %rd133, [%rd134];
	// end inline asm
	setp.lt.f64 	%p10, %rd133, 0d3E112E0BE826D695;
	@%p10 bra 	$L__BB0_19;
	add.s64 	%rd136, %rd11, 56;
	// begin inline asm
	ld.global.nc.f64 %rd135, [%rd136];
	// end inline asm
	sub.f64 	%rd137, %rd70, %rd135;
	max.f64 	%rd138, %rd137, 0d0000000000000000;
	div.rn.f64 	%rd139, %rd138, %rd133;
	add.f64 	%rd140, %rd139, 0dBFF0000000000000;
	min.f64 	%rd168, %rd168, %rd140;
	max.f64 	%rd169, %rd169, %rd140;
	add.f64 	%rd170, %rd170, %rd140;
	add.s32 	%r33, %r33, 1;
$L__BB0_19:
	add.s32 	%r27, %r27, 8;
	add.s64 	%rd152, %rd152, 64;
	bra.uni 	$L__BB0_2;
$L__BB0_20:
	setp.ne.s32 	%p11, %r33, 0;
	@%p11 bra 	$L__BB0_22;
	mul.wide.s32 	%rd147, %r1, 8;
	add.s64 	%rd148, %rd3, %rd147;
	st.global.b64 	[%rd148], 0;
	add.s64 	%rd149, %rd2, %rd147;
	st.global.b64 	[%rd149], 0;
	add.s64 	%rd150, %rd1, %rd147;
	st.global.b64 	[%rd150], 0;
	bra.uni 	$L__BB0_23;
$L__BB0_22:
	cvt.rn.f64.s32 	%rd141, %r33;
	div.rn.f64 	%rd142, %rd170, %rd141;
	mul.wide.s32 	%rd143, %r1, 8;
	add.s64 	%rd144, %rd3, %rd143;
	st.global.b64 	[%rd144], %rd168;
	add.s64 	%rd145, %rd2, %rd143;
	st.global.b64 	[%rd145], %rd169;
	add.s64 	%rd146, %rd1, %rd143;
	st.global.b64 	[%rd146], %rd142;
$L__BB0_23:
	ret;

}
	// .globl	classify_gamma_envelopes_batch
.visible .entry classify_gamma_envelopes_batch(
	.param .u64 .ptr .align 1 classify_gamma_envelopes_batch_param_0,
	.param .u64 .ptr .align 1 classify_gamma_envelopes_batch_param_1,
	.param .u64 .ptr .align 1 classify_gamma_envelopes_batch_param_2,
	.param .u32 classify_gamma_envelopes_batch_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<12>;

	ld.param.b64 	%rd1, [classify_gamma_envelopes_batch_param_0];
	ld.param.b64 	%rd2, [classify_gamma_envelopes_batch_param_1];
	ld.param.b64 	%rd3, [classify_gamma_envelopes_batch_param_2];
	ld.param.b32 	%r2, [classify_gamma_envelopes_batch_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB1_2;
	cvta.to.global.u64 	%rd8, %rd3;
	mul.wide.s32 	%rd9, %r1, 8;
	add.s64 	%rd5, %rd1, %rd9;
	// begin inline asm
	ld.global.nc.f64 %rd4, [%rd5];
	// end inline asm
	add.s64 	%rd7, %rd2, %rd9;
	// begin inline asm
	ld.global.nc.f64 %rd6, [%rd7];
	// end inline asm
	setp.lt.f64 	%p2, %rd6, 0d0000000000000000;
	setp.gt.f64 	%p3, %rd4, 0d0000000000000000;
	selp.b32 	%r6, 1, 0, %p3;
	selp.b32 	%r7, -1, %r6, %p2;
	mul.wide.s32 	%rd10, %r1, 4;
	add.s64 	%rd11, %rd8, %rd10;
	st.global.b32 	[%rd11], %r7;
$L__BB1_2:
	ret;

}
	// .globl	compute_weighted_avg_susceptibility
.visible .entry compute_weighted_avg_susceptibility(
	.param .u64 .ptr .align 1 compute_weighted_avg_susceptibility_param_0,
	.param .u64 .ptr .align 1 compute_weighted_avg_susceptibility_param_1,
	.param .u64 .ptr .align 1 compute_weighted_avg_susceptibility_param_2,
	.param .f64 compute_weighted_avg_susceptibility_param_3,
	.param .u32 compute_weighted_avg_susceptibility_param_4,
	.param .u32 compute_weighted_avg_susceptibility_param_5,
	.param .u32 compute_weighted_avg_susceptibility_param_6,
	.param .u32 compute_weighted_avg_susceptibility_param_7
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<77>;
	.reg .b64 	%rd<114>;

	ld.param.b64 	%rd44, [compute_weighted_avg_susceptibility_param_0];
	ld.param.b64 	%rd46, [compute_weighted_avg_susceptibility_param_2];
	ld.param.b64 	%rd47, [compute_weighted_avg_susceptibility_param_3];
	ld.param.b32 	%r35, [compute_weighted_avg_susceptibility_param_4];
	ld.param.b32 	%r36, [compute_weighted_avg_susceptibility_param_5];
	ld.param.b32 	%r37, [compute_weighted_avg_susceptibility_param_6];
	ld.param.b32 	%r38, [compute_weighted_avg_susceptibility_param_7];
	cvta.to.global.u64 	%rd1, %rd46;
	mov.u32 	%r39, %ctaid.x;
	mov.u32 	%r40, %ntid.x;
	mov.u32 	%r41, %tid.x;
	mad.lo.s32 	%r1, %r39, %r40, %r41;
	mul.lo.s32 	%r42, %r35, %r36;
	mul.lo.s32 	%r43, %r42, 75;
	setp.ge.s32 	%p1, %r1, %r43;
	@%p1 bra 	$L__BB2_23;
	mul.hi.s32 	%r44, %r1, 458129845;
	shr.u32 	%r45, %r44, 31;
	shr.s32 	%r46, %r44, 3;
	add.s32 	%r47, %r46, %r45;
	mul.lo.s32 	%r48, %r47, 75;
	sub.s32 	%r2, %r1, %r48;
	rem.s32 	%r3, %r47, %r36;
	add.s32 	%r4, %r3, %r38;
	setp.ge.s32 	%p2, %r4, %r37;
	@%p2 bra 	$L__BB2_24;
	ld.param.b32 	%r63, [compute_weighted_avg_susceptibility_param_4];
	setp.lt.s32 	%p3, %r63, 1;
	mov.b64 	%rd95, 0d0000000000000000;
	mov.b64 	%rd96, %rd95;
	@%p3 bra 	$L__BB2_19;
	ld.param.b32 	%r64, [compute_weighted_avg_susceptibility_param_4];
	mul.lo.s32 	%r5, %r64, %r2;
	and.b32 	%r6, %r64, 3;
	setp.lt.u32 	%p4, %r64, 4;
	mov.b64 	%rd96, 0d0000000000000000;
	mov.b32 	%r73, 0;
	mov.b64 	%rd95, %rd96;
	@%p4 bra 	$L__BB2_14;
	ld.param.b32 	%r65, [compute_weighted_avg_susceptibility_param_4];
	add.s32 	%r50, %r5, 2;
	mad.lo.s32 	%r72, %r36, %r50, %r3;
	add.s32 	%r51, %r5, 3;
	mad.lo.s32 	%r71, %r36, %r51, %r3;
	and.b32 	%r52, %r65, 2147483644;
	neg.s32 	%r70, %r52;
	mad.lo.s32 	%r53, %r36, %r5, %r36;
	add.s32 	%r69, %r3, %r53;
	mul.lo.s32 	%r54, %r2, %r36;
	mad.lo.s32 	%r67, %r54, %r65, %r3;
	mov.b64 	%rd96, 0d0000000000000000;
	mov.b32 	%r68, %r4;
$L__BB2_5:
	ld.param.b64 	%rd90, [compute_weighted_avg_susceptibility_param_1];
	mul.wide.s32 	%rd56, %r68, 4;
	add.s64 	%rd55, %rd90, %rd56;
	// begin inline asm
	ld.global.nc.f32 %r55, [%rd55];
	// end inline asm
	cvt.ftz.f64.f32 	%rd5, %r55;
	setp.lt.f64 	%p5, %rd5, 0d3E112E0BE826D695;
	@%p5 bra 	$L__BB2_7;
	mul.wide.s32 	%rd59, %r67, 8;
	add.s64 	%rd58, %rd44, %rd59;
	// begin inline asm
	ld.global.nc.f64 %rd57, [%rd58];
	// end inline asm
	sub.f64 	%rd60, %rd47, %rd57;
	max.f64 	%rd61, %rd60, 0d0000000000000000;
	fma.rn.f64 	%rd95, %rd61, %rd5, %rd95;
	add.f64 	%rd96, %rd96, %rd5;
$L__BB2_7:
	mul.wide.s32 	%rd10, %r37, 4;
	add.s64 	%rd62, %rd55, %rd10;
	// begin inline asm
	ld.global.nc.f32 %r56, [%rd62];
	// end inline asm
	cvt.ftz.f64.f32 	%rd12, %r56;
	setp.lt.f64 	%p6, %rd12, 0d3E112E0BE826D695;
	@%p6 bra 	$L__BB2_9;
	mul.wide.s32 	%rd65, %r69, 8;
	add.s64 	%rd64, %rd44, %rd65;
	// begin inline asm
	ld.global.nc.f64 %rd63, [%rd64];
	// end inline asm
	sub.f64 	%rd66, %rd47, %rd63;
	max.f64 	%rd67, %rd66, 0d0000000000000000;
	fma.rn.f64 	%rd95, %rd67, %rd12, %rd95;
	add.f64 	%rd96, %rd96, %rd12;
$L__BB2_9:
	add.s64 	%rd68, %rd62, %rd10;
	// begin inline asm
	ld.global.nc.f32 %r57, [%rd68];
	// end inline asm
	cvt.ftz.f64.f32 	%rd18, %r57;
	setp.lt.f64 	%p7, %rd18, 0d3E112E0BE826D695;
	@%p7 bra 	$L__BB2_11;
	mul.wide.s32 	%rd71, %r72, 8;
	add.s64 	%rd70, %rd44, %rd71;
	// begin inline asm
	ld.global.nc.f64 %rd69, [%rd70];
	// end inline asm
	sub.f64 	%rd72, %rd47, %rd69;
	max.f64 	%rd73, %rd72, 0d0000000000000000;
	fma.rn.f64 	%rd95, %rd73, %rd18, %rd95;
	add.f64 	%rd96, %rd96, %rd18;
$L__BB2_11:
	add.s64 	%rd74, %rd68, %rd10;
	// begin inline asm
	ld.global.nc.f32 %r58, [%rd74];
	// end inline asm
	cvt.ftz.f64.f32 	%rd23, %r58;
	setp.lt.f64 	%p8, %rd23, 0d3E112E0BE826D695;
	@%p8 bra 	$L__BB2_13;
	mul.wide.s32 	%rd77, %r71, 8;
	add.s64 	%rd76, %rd44, %rd77;
	// begin inline asm
	ld.global.nc.f64 %rd75, [%rd76];
	// end inline asm
	sub.f64 	%rd78, %rd47, %rd75;
	max.f64 	%rd79, %rd78, 0d0000000000000000;
	fma.rn.f64 	%rd95, %rd79, %rd23, %rd95;
	add.f64 	%rd96, %rd96, %rd23;
$L__BB2_13:
	ld.param.b32 	%r66, [compute_weighted_avg_susceptibility_param_4];
	and.b32 	%r73, %r66, 2147483644;
	shl.b32 	%r59, %r36, 2;
	add.s32 	%r72, %r72, %r59;
	add.s32 	%r71, %r71, %r59;
	add.s32 	%r70, %r70, 4;
	add.s32 	%r69, %r69, %r59;
	shl.b32 	%r60, %r37, 2;
	add.s32 	%r68, %r68, %r60;
	add.s32 	%r67, %r67, %r59;
	setp.ne.s32 	%p9, %r70, 0;
	@%p9 bra 	$L__BB2_5;
$L__BB2_14:
	setp.eq.s32 	%p10, %r6, 0;
	@%p10 bra 	$L__BB2_19;
	add.s32 	%r61, %r73, %r5;
	mad.lo.s32 	%r76, %r36, %r61, %r3;
	mad.lo.s32 	%r75, %r73, %r37, %r4;
	neg.s32 	%r74, %r6;
$L__BB2_16:
	.pragma "nounroll";
	ld.param.b64 	%rd91, [compute_weighted_avg_susceptibility_param_1];
	mul.wide.s32 	%rd81, %r75, 4;
	add.s64 	%rd80, %rd91, %rd81;
	// begin inline asm
	ld.global.nc.f32 %r62, [%rd80];
	// end inline asm
	cvt.ftz.f64.f32 	%rd34, %r62;
	setp.lt.f64 	%p11, %rd34, 0d3E112E0BE826D695;
	@%p11 bra 	$L__BB2_18;
	mul.wide.s32 	%rd84, %r76, 8;
	add.s64 	%rd83, %rd44, %rd84;
	// begin inline asm
	ld.global.nc.f64 %rd82, [%rd83];
	// end inline asm
	sub.f64 	%rd85, %rd47, %rd82;
	max.f64 	%rd86, %rd85, 0d0000000000000000;
	fma.rn.f64 	%rd95, %rd86, %rd34, %rd95;
	add.f64 	%rd96, %rd96, %rd34;
$L__BB2_18:
	add.s32 	%r76, %r76, %r36;
	add.s32 	%r75, %r75, %r37;
	add.s32 	%r74, %r74, 1;
	setp.ne.s32 	%p12, %r74, 0;
	@%p12 bra 	$L__BB2_16;
$L__BB2_19:
	setp.leu.f64 	%p13, %rd96, 0d3E112E0BE826D695;
	setp.leu.f64 	%p14, %rd95, 0d3E112E0BE826D695;
	or.pred 	%p15, %p13, %p14;
	@%p15 bra 	$L__BB2_21;
	div.rn.f64 	%rd113, %rd95, %rd96;
	bra.uni 	$L__BB2_22;
$L__BB2_21:
	mul.f64 	%rd113, %rd47, 0d3FE0000000000000;
$L__BB2_22:
	ld.param.b64 	%rd92, [compute_weighted_avg_susceptibility_param_2];
	cvta.to.global.u64 	%rd87, %rd92;
	mul.wide.s32 	%rd88, %r1, 8;
	add.s64 	%rd89, %rd87, %rd88;
	st.global.b64 	[%rd89], %rd113;
$L__BB2_23:
	ret;
$L__BB2_24:
	mul.f64 	%rd48, %rd47, 0d3FE0000000000000;
	mul.wide.s32 	%rd49, %r1, 8;
	add.s64 	%rd50, %rd1, %rd49;
	st.global.b64 	[%rd50], %rd48;
	bra.uni 	$L__BB2_23;

}
